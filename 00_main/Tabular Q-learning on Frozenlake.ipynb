{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tabular Q-learning on Frozenlake\n",
    "\n",
    "The final version of the algorithm is here:\n",
    "1. Start with an empty table for Q(s, a).\n",
    "2. Obtain (s, a, r, s') from the environment.\n",
    "3. Make a Bellman update: ğ‘„(ğ‘ , ğ‘) â† (1 âˆ’ ğ›¼)ğ‘„(ğ‘ , ğ‘) + ğ›¼ (ğ‘Ÿ + ğ›¾ max_a' ğ‘„(ğ‘ ', ğ‘â€²)\n",
    "4. Check convergence conditions. If not met, repeat from step 2.\n",
    "\n",
    "As mentioned earlier, this method is called tabular Q-learning, as we keep a table of\n",
    "states with their Q-values. Let's try it on our FrozenLake environment.\n",
    "\n",
    "In the beginning, we import packages and define constants. The new thing here\n",
    "is the value of ğ›¼, which will be used as the learning rate in the value update. The\n",
    "initialization of our Agent class is simpler now, as we don't need to track the history\n",
    "of rewards and transition counters, just our value table. This will make our memory\n",
    "footprint smaller, which is not a big issue for FrozenLake, but can be critical for\n",
    "larger environments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import collections\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "ENV_NAME = \"FrozenLake-v0\"\n",
    "GAMMA = 0.9\n",
    "ALPHA = 0.2\n",
    "TEST_EPISODES = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self):\n",
    "        self.env = gym.make(ENV_NAME)\n",
    "        self.state = self.env.reset()\n",
    "        self.values = collections.defaultdict(float)\n",
    "\n",
    "    def sample_env(self):\n",
    "        action = self.env.action_space.sample()\n",
    "        old_state = self.state\n",
    "        new_state, reward, is_done, _ = self.env.step(action)\n",
    "        self.state = self.env.reset() if is_done else new_state\n",
    "        return old_state, action, reward, new_state\n",
    "\n",
    "    def best_value_and_action(self, state):\n",
    "        best_value, best_action = None, None\n",
    "        for action in range(self.env.action_space.n):\n",
    "            action_value = self.values[(state, action)]\n",
    "            if best_value is None or best_value < action_value:\n",
    "                best_value = action_value\n",
    "                best_action = action\n",
    "        return best_value, best_action\n",
    "\n",
    "    def value_update(self, s, a, r, next_s):\n",
    "        best_v, _ = self.best_value_and_action(next_s)\n",
    "        new_v = r + GAMMA * best_v\n",
    "        old_v = self.values[(s, a)]\n",
    "        self.values[(s, a)] = old_v * (1-ALPHA) + new_v * ALPHA\n",
    "\n",
    "    def play_episode(self, env):\n",
    "        total_reward = 0.0\n",
    "        state = env.reset()\n",
    "        while True:\n",
    "            _, action = self.best_value_and_action(state)\n",
    "            new_state, reward, is_done, _ = env.step(action)\n",
    "            total_reward += reward\n",
    "            if is_done:\n",
    "                break\n",
    "            state = new_state\n",
    "        return total_reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sample_env method is used to obtain the next transition from the environment.\n",
    "We sample a random action from the action space and return the tuple of the old\n",
    "state, action taken, reward obtained, and the new state. The tuple will be used in\n",
    "the training loop later.\n",
    "\n",
    "The best_value_and_action method receives the state of the environment and finds the best action to\n",
    "take from this state by taking the action with the largest value that we have in the\n",
    "table. If we don't have the value associated with the state and action pair, then we\n",
    "take it as zero. This method will be used two times: first, in the test method that\n",
    "plays one episode using our current values table (to evaluate our policy's quality),\n",
    "and second, in the method that performs the value update to get the value of the\n",
    "next state.\n",
    "\n",
    "In the value_update method, we update our values table using one step from the environment. To do this,\n",
    "we calculate the Bellman approximation for our state, s, and action, a, by summing\n",
    "the immediate reward with the discounted value of the next state. Then we obtain\n",
    "the previous value of the state and action pair, and blend these values together\n",
    "using the learning rate. The result is the new approximation for the value of state s\n",
    "and action a, which is stored in our table.\n",
    "\n",
    "The last method (play_episode) in our Agent class plays one full episode using the provided test\n",
    "environment. The action on every step is taken using our current value table of\n",
    "Q-values. This method is used to evaluate our current policy to check the progress\n",
    "of learning. Note that this method doesn't alter our value table: it only uses it to\n",
    "find the best action to take.\n",
    "\n",
    "The rest of the example is the training loop, which is very similar to examples from\n",
    "Chapter 5, Tabular Learning and the Bellman Equation: we create a test environment,\n",
    "agent, and summary writer, and then, in the loop, we do one step in the environment\n",
    "and perform a value update using the obtained data. Next, we test our current policy\n",
    "by playing several test episodes. If a good reward is obtained, then we stop training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best reward updated 0.000 -> 0.050\n",
      "Best reward updated 0.050 -> 0.150\n",
      "Best reward updated 0.150 -> 0.200\n",
      "Best reward updated 0.200 -> 0.350\n",
      "Best reward updated 0.350 -> 0.400\n",
      "Best reward updated 0.400 -> 0.450\n",
      "Best reward updated 0.450 -> 0.550\n",
      "Best reward updated 0.550 -> 0.600\n",
      "Best reward updated 0.600 -> 0.750\n",
      "Best reward updated 0.750 -> 0.850\n",
      "Solved in 3978 iterations!\n"
     ]
    }
   ],
   "source": [
    "test_env = gym.make(ENV_NAME)\n",
    "agent = Agent()\n",
    "writer = SummaryWriter(comment=\"-q-learning\")\n",
    "\n",
    "iter_no = 0\n",
    "best_reward = 0.0\n",
    "while True:\n",
    "    iter_no += 1\n",
    "    s, a, r, next_s = agent.sample_env()\n",
    "    agent.value_update(s, a, r, next_s)\n",
    "\n",
    "    reward = 0.0\n",
    "    for _ in range(TEST_EPISODES):\n",
    "        reward += agent.play_episode(test_env)\n",
    "    reward /= TEST_EPISODES\n",
    "    writer.add_scalar(\"reward\", reward, iter_no)\n",
    "    if reward > best_reward:\n",
    "        print(\"Best reward updated %.3f -> %.3f\" % (\n",
    "            best_reward, reward))\n",
    "        best_reward = reward\n",
    "    if reward > 0.80:\n",
    "        print(\"Solved in %d iterations!\" % iter_no)\n",
    "        break\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may have noticed that this version used more iterations to solve the problem\n",
    "compared to the value iteration method from the previous chapter. The reason\n",
    "for that is that we are no longer using the experience obtained during testing. (In\n",
    "example Chapter05/02_frozenlake_q_iteration.py, periodical tests caused an\n",
    "update of Q-table statistics. Here, we don't touch Q-values during the test, which\n",
    "causes more iterations before the environment gets solved.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
