{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-learning for FrozenLake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import collections\n",
    "from tensorboardX import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENV_NAME = \"FrozenLake-v0\"\n",
    "#ENV_NAME = \"FrozenLake8x8-v0\"      # uncomment for larger version\n",
    "GAMMA = 0.9\n",
    "TEST_EPISODES = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The differences with the value iteration are really minors. \n",
    "\n",
    "- The most obvious change is to our value table. In the previous example, we kept the value of the state, so the key in the dictionary was just a state. Now we need to store values of the Q-function, which has two parameters: state and action, so the key in the value table is now a composite.\n",
    "\n",
    "- The second difference is in our calc_action_value() function. We just don't need\n",
    "it anymore, as our action values are stored in the value table.\n",
    "\n",
    "- Finally, the most important change in the code is in the agent's value_iteration()\n",
    "method. Before, it was just a wrapper around the calc_action_value() call,\n",
    "which did the job of Bellman approximation. Now, as this function has gone and\n",
    "been replaced by a value table, we need to do this approximation in the value_\n",
    "iteration() method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self):\n",
    "        self.env = gym.make(ENV_NAME)\n",
    "        self.state = self.env.reset()\n",
    "        self.rewards = collections.defaultdict(float)\n",
    "        self.transits = collections.defaultdict(collections.Counter)\n",
    "        self.values = collections.defaultdict(float)\n",
    "\n",
    "    def play_n_random_steps(self, count):\n",
    "        for _ in range(count):\n",
    "            action = self.env.action_space.sample()\n",
    "            new_state, reward, is_done, _ = self.env.step(action)\n",
    "            self.rewards[(self.state, action, new_state)] = reward\n",
    "            self.transits[(self.state, action)][new_state] += 1\n",
    "            self.state = self.env.reset() if is_done else new_state\n",
    "\n",
    "    def select_action(self, state):\n",
    "        best_action, best_value = None, None\n",
    "        for action in range(self.env.action_space.n):\n",
    "            action_value = self.values[(state, action)]\n",
    "            if best_value is None or best_value < action_value:\n",
    "                best_value = action_value\n",
    "                best_action = action\n",
    "        return best_action\n",
    "\n",
    "    def play_episode(self, env):\n",
    "        total_reward = 0.0\n",
    "        state = env.reset()\n",
    "        while True:\n",
    "            action = self.select_action(state)\n",
    "            new_state, reward, is_done, _ = env.step(action)\n",
    "            self.rewards[(state, action, new_state)] = reward\n",
    "            self.transits[(state, action)][new_state] += 1\n",
    "            total_reward += reward\n",
    "            if is_done:\n",
    "                break\n",
    "            state = new_state\n",
    "        return total_reward\n",
    "\n",
    "    def value_iteration(self):\n",
    "        for state in range(self.env.observation_space.n):\n",
    "            for action in range(self.env.action_space.n):\n",
    "                action_value = 0.0\n",
    "                target_counts = self.transits[(state, action)]\n",
    "                total = sum(target_counts.values())\n",
    "                for tgt_state, count in target_counts.items():\n",
    "                    key = (state, action, tgt_state)\n",
    "                    reward = self.rewards[key]\n",
    "                    best_action = self.select_action(tgt_state)\n",
    "                    val = reward + GAMMA * \\\n",
    "                          self.values[(tgt_state, best_action)]\n",
    "                    action_value += (count / total) * val\n",
    "                self.values[(state, action)] = action_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The code is very similar to calc_action_value() in the previous example and,\n",
    "in fact, it does almost the same thing. For the given state and action, it needs to\n",
    "calculate the value of this action using statistics about target states that we have\n",
    "reached with the action. To calculate this value, we use the Bellman equation and\n",
    "our counters, which allow us to approximate the probability of the target state.\n",
    "However, in Bellman's equation, we have the value of the state; now, we need to\n",
    "calculate it differently.**\n",
    "\n",
    "**Before, we had it stored in the value table (as we approximated the value of the\n",
    "states), so we just took it from this table. We can't do this anymore, so we have to\n",
    "call the select_action method, which will choose for us the action with the largest\n",
    "Q-value, and then we take this Q-value as the value of the target state. Of course,\n",
    "we can implement another function that can calculate for us this value of the state,\n",
    "but select_action does almost everything we need, so we will reuse it here.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**As I said, we don't have the calc_action_value method anymore, so, to select\n",
    "an action, we just iterate over the actions and look up their values in our values\n",
    "table. It could look like a minor improvement, but if you think about the data that\n",
    "we used in calc_action_value, it may become obvious why the learning of the\n",
    "Q-function is much more popular in RL than the learning of the V-function.\n",
    "Our calc_action_value function uses both information about the reward and\n",
    "probabilities. It's not a huge problem for the value iteration method, which relies\n",
    "on this information during training. However, in the next chapter, you will learn\n",
    "about the value iteration method extension, which doesn't require probability\n",
    "approximation, but just takes it from the environment samples. For such methods,\n",
    "this dependency on probability adds an extra burden for the agent. In the case of\n",
    "Q-learning, what the agent needs to make the decision is just Q-values.\n",
    "I don't want to say that V-functions are completely useless, because they are an\n",
    "essential part of the actor-critic method, which we will talk about in part three\n",
    "of this book. However, in the area of value learning, Q-functions is the definite\n",
    "favorite. With regards to convergence speed, both our versions are almost identical\n",
    "(but the Q-learning version requires four times more memory for the value table).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best reward updated 0.000 -> 0.100\n",
      "Best reward updated 0.100 -> 0.150\n",
      "Best reward updated 0.150 -> 0.300\n",
      "Best reward updated 0.300 -> 0.350\n",
      "Best reward updated 0.350 -> 0.400\n",
      "Best reward updated 0.400 -> 0.450\n",
      "Best reward updated 0.450 -> 0.800\n",
      "Best reward updated 0.800 -> 0.900\n",
      "Solved in 24 iterations!\n"
     ]
    }
   ],
   "source": [
    "test_env = gym.make(ENV_NAME)\n",
    "agent = Agent()\n",
    "writer = SummaryWriter(comment=\"-q-iteration\")\n",
    "\n",
    "iter_no = 0\n",
    "best_reward = 0.0\n",
    "while True:\n",
    "    iter_no += 1\n",
    "    agent.play_n_random_steps(100)\n",
    "    agent.value_iteration()\n",
    "\n",
    "    reward = 0.0\n",
    "    for _ in range(TEST_EPISODES):\n",
    "        reward += agent.play_episode(test_env)\n",
    "    reward /= TEST_EPISODES\n",
    "    writer.add_scalar(\"reward\", reward, iter_no)\n",
    "    if reward > best_reward:\n",
    "        print(\"Best reward updated %.3f -> %.3f\" % (best_reward, reward))\n",
    "        best_reward = reward\n",
    "    if reward > 0.80:\n",
    "        print(\"Solved in %d iterations!\" % iter_no)\n",
    "        break\n",
    "writer.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
