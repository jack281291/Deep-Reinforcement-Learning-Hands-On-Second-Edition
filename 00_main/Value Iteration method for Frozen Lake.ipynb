{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Value Iteration method for Frozen Lake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import collections\n",
    "from tensorboardX import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENV_NAME = \"FrozenLake-v0\"\n",
    "#ENV_NAME = \"FrozenLake8x8-v0\"      # uncomment for larger version\n",
    "GAMMA = 0.9\n",
    "TEST_EPISODES = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The\n",
    "central data structures in this example are as follows:\n",
    "- Reward table: A dictionary with the composite key \"source state\" + \"action\" +\n",
    "\"target state\". The value is obtained from the immediate reward.\n",
    "- Transitions table: A dictionary keeping counters of the experienced\n",
    "transitions. The key is the composite \"state\" + \"action\", and the value is\n",
    "another dictionary that maps the target state into a count of times that we\n",
    "have seen it. For example, if in state 0 we execute action 1 ten times, after\n",
    "three times it will lead us to state 4 and after seven times to state 5.\n",
    "The entry with the key (0, 1) in this table will be a dict with contents\n",
    "{4: 3, 5: 7}. We can use this table to estimate the probabilities of our\n",
    "transitions.\n",
    "- Value table: A dictionary that maps a state into the calculated value of this\n",
    "state.\n",
    "\n",
    "The overall logic of our code is simple: in the loop, we play 100 random steps from\n",
    "the environment, populating the reward and transition tables. After those 100\n",
    "steps, we perform a value iteration loop over all states, updating our value table.\n",
    "Then we play several full episodes to check our improvements using the updated\n",
    "value table. If the average reward for those test episodes is above the 0.8 boundary,\n",
    "then we stop training. During the test episodes, we also update our reward and\n",
    "transition tables to use all data from the environment.\n",
    "\n",
    "We define the Agent class, which will keep our tables and contain functions\n",
    "that we will be using in the training loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self):\n",
    "        self.env = gym.make(ENV_NAME)\n",
    "        self.state = self.env.reset()\n",
    "        self.rewards = collections.defaultdict(float)\n",
    "        self.transits = collections.defaultdict(\n",
    "            collections.Counter)\n",
    "        self.values = collections.defaultdict(float)\n",
    "\n",
    "    def play_n_random_steps(self, count):\n",
    "        for _ in range(count):\n",
    "            action = self.env.action_space.sample()\n",
    "            new_state, reward, is_done, _ = self.env.step(action)\n",
    "            self.rewards[(self.state, action, new_state)] = reward\n",
    "            self.transits[(self.state, action)][new_state] += 1\n",
    "            self.state = self.env.reset() \\\n",
    "                if is_done else new_state\n",
    "\n",
    "    def calc_action_value(self, state, action):\n",
    "        target_counts = self.transits[(state, action)]\n",
    "        total = sum(target_counts.values())\n",
    "        action_value = 0.0\n",
    "        for tgt_state, count in target_counts.items():\n",
    "            reward = self.rewards[(state, action, tgt_state)]\n",
    "            val = reward + GAMMA * self.values[tgt_state]\n",
    "            action_value += (count / total) * val\n",
    "        return action_value\n",
    "\n",
    "    def select_action(self, state):\n",
    "        best_action, best_value = None, None\n",
    "        for action in range(self.env.action_space.n):\n",
    "            action_value = self.calc_action_value(state, action)\n",
    "            if best_value is None or best_value < action_value:\n",
    "                best_value = action_value\n",
    "                best_action = action\n",
    "        return best_action\n",
    "\n",
    "    def play_episode(self, env):\n",
    "        total_reward = 0.0\n",
    "        state = env.reset()\n",
    "        while True:\n",
    "            action = self.select_action(state)\n",
    "            new_state, reward, is_done, _ = env.step(action)\n",
    "            self.rewards[(state, action, new_state)] = reward\n",
    "            self.transits[(state, action)][new_state] += 1\n",
    "            total_reward += reward\n",
    "            if is_done:\n",
    "                break\n",
    "            state = new_state\n",
    "        return total_reward\n",
    "\n",
    "    def value_iteration(self):\n",
    "        for state in range(self.env.observation_space.n):\n",
    "            state_values = [\n",
    "                self.calc_action_value(state, action)\n",
    "                for action in range(self.env.action_space.n)\n",
    "            ]\n",
    "            self.values[state] = max(state_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the class constructor, we create the environment that we will be using for data\n",
    "samples, obtain our first observation, and define tables for rewards, transitions,\n",
    "and values. We use the play_n_random_steps function is used to gather random experience from the environment and update\n",
    "the reward and transition tables. Note that we don't need to wait for the end of the\n",
    "episode to start learning; we just perform N steps and remember their outcomes.\n",
    "This is one of the differences between value iteration and the cross-entropy method,\n",
    "which can learn only on full episodes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function **calc_action_value** calculates the value of the action from the state using our\n",
    "transition, reward, and values tables. We will use it for two purposes: to select the\n",
    "best action to perform from the state and to calculate the new value of the state on\n",
    "value iteration. We do the following:\n",
    "1. We extract transition counters for the given state and action from the\n",
    "transition table. Counters in this table have a form of dict, with target states\n",
    "as the key and a count of experienced transitions as the value. We sum all\n",
    "counters to obtain the total count of times we have executed the action from\n",
    "the state. We will use this total value later to go from an individual counter\n",
    "to probability.\n",
    "2. Then we iterate every target state that our action has landed on and calculate\n",
    "its contribution to the total action value using the Bellman equation. This\n",
    "contribution is equal to immediate reward plus discounted value for the\n",
    "target state. We multiply this sum to the probability of this transition and\n",
    "add the result to the final action value.\n",
    "\n",
    "Then, the approximate value for the state and action, Q(s, a), will be equal to the\n",
    "probability of every state, multiplied to the value of the state. From the Bellman\n",
    "equation, this equals the sum of the immediate reward and the discounted long-term\n",
    "state value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The select_action function uses the function that I just described to make a decision about\n",
    "the best action to take from the given state. It iterates over all possible actions in\n",
    "the environment and calculates the value for every action. The action with the\n",
    "largest value wins and is returned as the action to take. This action selection process\n",
    "is deterministic, as the play_n_random_steps() function introduces enough\n",
    "exploration. So, our agent will behave greedily in regard to our value approximation.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The play_episode() function uses select_action() to find the best action to take\n",
    "and plays one full episode using the provided environment. This function is used to\n",
    "play test episodes, during which we don't want to mess with the current state of the\n",
    "main environment used to gather random data. So, we use the second environment\n",
    "passed as an argument. The logic is very simple and should be already familiar to\n",
    "you: we just loop over states accumulating reward for one episode.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The final method of the Agent class is our value iteration implementation and it\n",
    "is surprisingly simple, thanks to the preceding functions. What we do is just loop\n",
    "over all states in the environment, then for every state, we calculate the values for\n",
    "the states reachable from it, obtaining candidates for the value of the state. Then we\n",
    "update the value of our current state with the maximum value of the action available\n",
    "from the state.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best reward updated 0.000 -> 0.250\n",
      "Best reward updated 0.250 -> 0.350\n",
      "Best reward updated 0.350 -> 0.400\n",
      "Best reward updated 0.400 -> 0.500\n",
      "Best reward updated 0.500 -> 0.550\n",
      "Best reward updated 0.550 -> 0.600\n",
      "Best reward updated 0.600 -> 0.650\n",
      "Best reward updated 0.650 -> 0.700\n",
      "Best reward updated 0.700 -> 0.750\n",
      "Best reward updated 0.750 -> 0.800\n",
      "Best reward updated 0.800 -> 0.850\n",
      "Solved in 88 iterations!\n"
     ]
    }
   ],
   "source": [
    "test_env = gym.make(ENV_NAME)\n",
    "agent = Agent()\n",
    "writer = SummaryWriter(comment=\"-v-iteration\")\n",
    "\n",
    "iter_no = 0\n",
    "best_reward = 0.0\n",
    "while True:\n",
    "    iter_no += 1\n",
    "    agent.play_n_random_steps(100)\n",
    "    agent.value_iteration()\n",
    "\n",
    "    reward = 0.0\n",
    "    for _ in range(TEST_EPISODES):\n",
    "        reward += agent.play_episode(test_env)\n",
    "    reward /= TEST_EPISODES\n",
    "    writer.add_scalar(\"reward\", reward, iter_no)\n",
    "    if reward > best_reward:\n",
    "        print(\"Best reward updated %.3f -> %.3f\" % (\n",
    "            best_reward, reward))\n",
    "        best_reward = reward\n",
    "    if reward > 0.80:\n",
    "        print(\"Solved in %d iterations!\" % iter_no)\n",
    "        break\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we perform 100 random steps to fill our reward and transition tables with\n",
    "fresh data, and then we run value iteration over all states. The rest of the code plays\n",
    "test episodes using the value table as our policy, then writes data into TensorBoard,\n",
    "tracks the best average reward, and checks for the training loop stop condition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our solution is stochastic, and my experiments usually required from 12 to 100\n",
    "iterations to reach a solution, but in all cases, it took less than a second to find a good\n",
    "policy that could solve the environment in 80% of runs. If you remember how many\n",
    "hours were required to achieve a 60% success ratio using the cross-entropy method,\n",
    "then you can understand that this is a major improvement. There are several reasons\n",
    "for that.\n",
    "\n",
    "First of all, the stochastic outcome of our actions, plus the length of the episodes\n",
    "(six to 10 steps on average), makes it hard for the cross-entropy method to\n",
    "understand what was done right in the episode and which step was a mistake. Value\n",
    "iteration works with individual values of the state (or action) and incorporates the\n",
    "probabilistic outcome of actions naturally by estimating probability and calculating\n",
    "the expected value. So, it's much simpler for value iteration and requires much less\n",
    "data from the environment (which is called sample efficiency in RL).\n",
    "\n",
    "The second reason is the fact that value iteration doesn't need full episodes to\n",
    "start learning. In an extreme case, we can start updating our values just from a\n",
    "single example. However, for FrozenLake, due to the reward structure (we get\n",
    "1 only after successfully reaching the target state), we still need to have at least\n",
    "one successful episode to start learning from a useful value table, which may\n",
    "be challenging to achieve in more complex environments. For example, you can\n",
    "try switching the existing code to a larger version of FrozenLake, which has the\n",
    "name FrozenLake8x8-v0. The larger version of FrozenLake can take from 150 to\n",
    "1,000 iterations to solve, and, according to TensorBoard charts, most of the time\n",
    "it waits for the first successful episode, then it very quickly reaches convergence."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
