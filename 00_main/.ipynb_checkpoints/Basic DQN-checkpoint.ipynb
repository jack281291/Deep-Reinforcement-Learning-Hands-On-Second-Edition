{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic DQN\n",
    "\n",
    "To get started, we will implement the same DQN method as in Chapter 6, Deep\n",
    "Q-Networks, but leveraging the high-level libraries described in Chapter 7, Higher-\n",
    "Level RL Libraries. This will make our code much more compact, which is good,\n",
    "as non-relevant details won't distract us from the method's logic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, input_shape, n_actions):\n",
    "        super(DQN, self).__init__()\n",
    "\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(input_shape[0], 32, kernel_size=8, stride=4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        conv_out_size = self._get_conv_out(input_shape)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(conv_out_size, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, n_actions)\n",
    "        )\n",
    "\n",
    "    def _get_conv_out(self, shape):\n",
    "        o = self.conv(torch.zeros(1, *shape))\n",
    "        return int(np.prod(o.size()))\n",
    "\n",
    "    def forward(self, x):\n",
    "        fx = x.float() / 256\n",
    "        conv_out = self.conv(fx).view(fx.size()[0], -1)\n",
    "        return self.fc(conv_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Common library\n",
    "\n",
    "First of all, we have hyperparameters\n",
    "for our Pong environment from the previous chapter. The hyperparameters are\n",
    "stored in the SimpleNamespace object, which is a class from the Python standard\n",
    "library that provides simple access to a variable set of keys and values. This makes\n",
    "it easy to add another configuration set for different, more complicated Atari games\n",
    "and allows us to experiment with hyperparameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from datetime import timedelta, datetime\n",
    "from types import SimpleNamespace\n",
    "from typing import Iterable, Tuple, List\n",
    "\n",
    "import ptan\n",
    "import ptan.ignite as ptan_ignite\n",
    "from ignite.engine import Engine\n",
    "from ignite.metrics import RunningAverage\n",
    "from ignite.contrib.handlers import tensorboard_logger as tb_logger\n",
    "\n",
    "\n",
    "SEED = 123"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "HYPERPARAMS = {\n",
    "    'pong': SimpleNamespace(**{\n",
    "        'env_name':         \"PongNoFrameskip-v4\",\n",
    "        'stop_reward':      18.0,\n",
    "        'run_name':         'pong',\n",
    "        'replay_size':      100000,\n",
    "        'replay_initial':   10000,\n",
    "        'target_net_sync':  1000,\n",
    "        'epsilon_frames':   10**5,\n",
    "        'epsilon_start':    1.0,\n",
    "        'epsilon_final':    0.02,\n",
    "        'learning_rate':    0.0001,\n",
    "        'gamma':            0.99,\n",
    "        'batch_size':       32\n",
    "    }),\n",
    "    'breakout-small': SimpleNamespace(**{\n",
    "        'env_name':         \"BreakoutNoFrameskip-v4\",\n",
    "        'stop_reward':      500.0,\n",
    "        'run_name':         'breakout-small',\n",
    "        'replay_size':      3*10 ** 5,\n",
    "        'replay_initial':   20000,\n",
    "        'target_net_sync':  1000,\n",
    "        'epsilon_frames':   10 ** 6,\n",
    "        'epsilon_start':    1.0,\n",
    "        'epsilon_final':    0.1,\n",
    "        'learning_rate':    0.0001,\n",
    "        'gamma':            0.99,\n",
    "        'batch_size':       64\n",
    "    }),\n",
    "    'breakout': SimpleNamespace(**{\n",
    "        'env_name':         \"BreakoutNoFrameskip-v4\",\n",
    "        'stop_reward':      500.0,\n",
    "        'run_name':         'breakout',\n",
    "        'replay_size':      10 ** 6,\n",
    "        'replay_initial':   50000,\n",
    "        'target_net_sync':  10000,\n",
    "        'epsilon_frames':   10 ** 6,\n",
    "        'epsilon_start':    1.0,\n",
    "        'epsilon_final':    0.1,\n",
    "        'learning_rate':    0.00025,\n",
    "        'gamma':            0.99,\n",
    "        'batch_size':       32\n",
    "    }),\n",
    "    'invaders': SimpleNamespace(**{\n",
    "        'env_name': \"SpaceInvadersNoFrameskip-v4\",\n",
    "        'stop_reward': 500.0,\n",
    "        'run_name': 'breakout',\n",
    "        'replay_size': 10 ** 6,\n",
    "        'replay_initial': 50000,\n",
    "        'target_net_sync': 10000,\n",
    "        'epsilon_frames': 10 ** 6,\n",
    "        'epsilon_start': 1.0,\n",
    "        'epsilon_final': 0.1,\n",
    "        'learning_rate': 0.00025,\n",
    "        'gamma': 0.99,\n",
    "        'batch_size': 32\n",
    "    }),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next function from lib/common.py has the name unpack_batch and it takes\n",
    "the batch of transitions and converts it into the set of NumPy arrays suitable\n",
    "for training. Every transition from ExperienceSourceFirstLast has a type\n",
    "of ExperienceFirstLast, which is a namedtuple with the following fields:\n",
    "- state: observation from the environment.\n",
    "- action: integer action taken by the agent.\n",
    "- reward: if we have created ExperienceSourceFirstLast with the\n",
    "attribute steps_count=1, it's just the immediate reward. For larger step\n",
    "counts, it contains the discounted sum of rewards for this number of steps.\n",
    "- last_state: if the transition corresponds to the final step in the\n",
    "environment, then this field is None; otherwise, it contains the last\n",
    "observation in the experience chain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unpack_batch(batch: List[ptan.experience.ExperienceFirstLast]):\n",
    "    states, actions, rewards, dones, last_states = [],[],[],[],[]\n",
    "    for exp in batch:\n",
    "        state = np.array(exp.state)\n",
    "        states.append(state)\n",
    "        actions.append(exp.action)\n",
    "        rewards.append(exp.reward)\n",
    "        dones.append(exp.last_state is None)\n",
    "        if exp.last_state is None:\n",
    "            lstate = state  # the result will be masked anyway\n",
    "        else:\n",
    "            lstate = np.array(exp.last_state)\n",
    "        last_states.append(lstate)\n",
    "    return np.array(states, copy=False), np.array(actions), \\\n",
    "           np.array(rewards, dtype=np.float32), \\\n",
    "           np.array(dones, dtype=np.uint8), \\\n",
    "           np.array(last_states, copy=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note how we handle the final transitions in the batch. To avoid the special handling\n",
    "of such cases, for terminal transitions, we store the initial state in the last_states\n",
    "array. To make our calculations of the Bellman update correct, we can mask such\n",
    "batch entries during the loss calculation using the dones array. Another solution\n",
    "would be to calculate the value of the last states only for non-terminal transitions,\n",
    "but it would make our loss function logic a bit more complicated.\n",
    "\n",
    "Calculation of the DQN loss function is provided by the function calc_loss_dqn,\n",
    "and the code is almost the same as in Chapter 6, Deep Q-Networks. One small addition\n",
    "is torch.no_grad(), which stops the PyTorch calculation graph from being recorded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_loss_dqn(batch, net, tgt_net, gamma, device=\"cpu\"):\n",
    "    states, actions, rewards, dones, next_states = \\\n",
    "        unpack_batch(batch)\n",
    "\n",
    "    states_v = torch.tensor(states).to(device)\n",
    "    next_states_v = torch.tensor(next_states).to(device)\n",
    "    actions_v = torch.tensor(actions).to(device)\n",
    "    rewards_v = torch.tensor(rewards).to(device)\n",
    "    done_mask = torch.BoolTensor(dones).to(device)\n",
    "\n",
    "    actions_v = actions_v.unsqueeze(-1)\n",
    "    state_action_vals = net(states_v).gather(1, actions_v)\n",
    "    state_action_vals = state_action_vals.squeeze(-1)\n",
    "    with torch.no_grad():\n",
    "        next_state_vals = tgt_net(next_states_v).max(1)[0]\n",
    "        next_state_vals[done_mask] = 0.0\n",
    "\n",
    "    bellman_vals = next_state_vals.detach() * gamma + rewards_v\n",
    "    return nn.MSELoss()(state_action_vals, bellman_vals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Besides those core DQN functions, common.py provides several utilities related to our\n",
    "training loop, data generation, and TensorBoard tracking. The first such utility is a\n",
    "small class that implements epsilon decay during the training. Epsilon defines the\n",
    "probability of taking the random action by the agent. It should be decayed from 1.0 in\n",
    "the beginning (fully random agent) to some small number, like 0.02 or 0.01. The code\n",
    "is trivial but needed in almost any DQN, so it is provided by the following little class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EpsilonTracker:\n",
    "    def __init__(self, selector: ptan.actions.EpsilonGreedyActionSelector,\n",
    "                 params: SimpleNamespace):\n",
    "        self.selector = selector\n",
    "        self.params = params\n",
    "        self.frame(0)\n",
    "\n",
    "    def frame(self, frame_idx: int):\n",
    "        eps = self.params.epsilon_start - \\\n",
    "              frame_idx / self.params.epsilon_frames\n",
    "        self.selector.epsilon = max(self.params.epsilon_final, eps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another small function is batch_generator, which takes ExperienceReplayBuffer\n",
    "(the PTAN class described in Chapter 7, Higher-Level RL Libraries) and infinitely\n",
    "generates training batches sampled from the buffer. In the beginning, the function\n",
    "ensures that the buffer contains the required amount of samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_generator(buffer: ptan.experience.ExperienceReplayBuffer,\n",
    "                    initial: int, batch_size: int):\n",
    "    buffer.populate(initial)\n",
    "    while True:\n",
    "        buffer.populate(1)\n",
    "        yield buffer.sample(batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, a lengthy, but nevertheless very useful, function called setup_ignite\n",
    "attaches the needed Ignite handlers, showing the training progress and writing\n",
    "metrics to TensorBoard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_ignite(engine: Engine, params: SimpleNamespace,\n",
    "                 exp_source, run_name: str,\n",
    "                 extra_metrics: Iterable[str] = ()):\n",
    "    # get rid of missing metrics warning\n",
    "    warnings.simplefilter(\"ignore\", category=UserWarning)\n",
    "\n",
    "    handler = ptan_ignite.EndOfEpisodeHandler(\n",
    "        exp_source, bound_avg_reward=params.stop_reward)\n",
    "    handler.attach(engine)\n",
    "    ptan_ignite.EpisodeFPSHandler().attach(engine)\n",
    "\n",
    "    @engine.on(ptan_ignite.EpisodeEvents.EPISODE_COMPLETED)\n",
    "    def episode_completed(trainer: Engine):\n",
    "        passed = trainer.state.metrics.get('time_passed', 0)\n",
    "        print(\"Episode %d: reward=%.0f, steps=%s, \"\n",
    "              \"speed=%.1f f/s, elapsed=%s\" % (\n",
    "            trainer.state.episode, trainer.state.episode_reward,\n",
    "            trainer.state.episode_steps,\n",
    "            trainer.state.metrics.get('avg_fps', 0),\n",
    "            timedelta(seconds=int(passed))))\n",
    "\n",
    "    @engine.on(ptan_ignite.EpisodeEvents.BOUND_REWARD_REACHED)\n",
    "    def game_solved(trainer: Engine):\n",
    "        passed = trainer.state.metrics['time_passed']\n",
    "        print(\"Game solved in %s, after %d episodes \"\n",
    "              \"and %d iterations!\" % (\n",
    "            timedelta(seconds=int(passed)),\n",
    "            trainer.state.episode, trainer.state.iteration))\n",
    "        trainer.should_terminate = True\n",
    "\n",
    "    now = datetime.now().isoformat(timespec='minutes')\n",
    "    logdir = f\"runs/{now}-{params.run_name}-{run_name}\"\n",
    "    tb = tb_logger.TensorboardLogger(log_dir=logdir)\n",
    "    run_avg = RunningAverage(output_transform=lambda v: v['loss'])\n",
    "    run_avg.attach(engine, \"avg_loss\")\n",
    "\n",
    "    metrics = ['reward', 'steps', 'avg_reward']\n",
    "    handler = tb_logger.OutputHandler(\n",
    "        tag=\"episodes\", metric_names=metrics)\n",
    "    event = ptan_ignite.EpisodeEvents.EPISODE_COMPLETED\n",
    "    tb.attach(engine, log_handler=handler, event_name=event)\n",
    "\n",
    "    # write to tensorboard every 100 iterations\n",
    "    ptan_ignite.PeriodicEvents().attach(engine)\n",
    "    metrics = ['avg_loss', 'avg_fps']\n",
    "    metrics.extend(extra_metrics)\n",
    "    handler = tb_logger.OutputHandler(\n",
    "        tag=\"train\", metric_names=metrics,\n",
    "        output_transform=lambda a: a)\n",
    "    event = ptan_ignite.PeriodEvents.ITERS_100_COMPLETED\n",
    "    tb.attach(engine, log_handler=handler, event_name=event)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initially, setup_ignite attaches two Ignite handlers provided by PTAN:\n",
    "- EndOfEpisodeHandler, which emits the Ignite event every time a game\n",
    "episode ends. It can also fire an event when the averaged reward for\n",
    "episodes crosses some boundary. We use this to detect when the game\n",
    "is finally solved.\n",
    "- EpisodeFPSHandler, a small class that tracks the time the episode has taken\n",
    "and the amount of interactions that we have had with the environment.\n",
    "From this, we calculate frames per second (FPS), which is an important\n",
    "performance metric to track.\n",
    "\n",
    "Then we install two event handlers, with one being called at the end of an episode.\n",
    "It will show information about the completed episode on the console. Another\n",
    "function will be called when the average reward grows above the boundary defined\n",
    "in the hyperparameters (18.0 in the case of Pong). This function shows a message\n",
    "about the solved game and stops the training.\n",
    "\n",
    "The rest of the function is related to the TensorBoard data that we want to track, first, we create a TensorboardLogger, a special class provided by Ignite to write\n",
    "into TensorBoard. Our processing function will return the loss value, so we attach\n",
    "the RunningAverage transformation (also provided by Ignite) to get a smoothed\n",
    "version of the loss over time.\n",
    "\n",
    "TensorboardLogger can track two groups of values from Ignite: outputs (values\n",
    "returned by the transformation function) and metrics (calculated during the training\n",
    "and kept in the engine state). EndOfEpisodeHandler and EpisodeFPSHandler\n",
    "provide metrics, which are updated at the end of every game episode. So, we attach\n",
    "OutputHandler, which will write into TensorBoard information about the episode\n",
    "every time it is completed.\n",
    "\n",
    "Another group of values that we want to track are metrics from the training process:\n",
    "loss, FPS, and, possibly, some custom metrics. Those values are updated every\n",
    "training iteration, but we are going to do millions of iterations, so we will store\n",
    "values in TensorBoard every 100 training iterations; otherwise, the data files will be\n",
    "huge. All this functionality might look too complicated, but it provides us with the\n",
    "unified set of metrics gathered from the training process. In fact, Ignite is not very\n",
    "complicated and provides a very flexible framework."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import ptan\n",
    "import argparse\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "\n",
    "from ignite.engine import Engine\n",
    "\n",
    "NAME = \"01_baseline\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we create the environment and apply a set of standard wrappers. We have\n",
    "already discussed them in Chapter 6, Deep Q-Networks and will also touch upon them\n",
    "in the next chapter, when we optimize the performance of the Pong solver. Then, we\n",
    "create the DQN model and the target network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "params = HYPERPARAMS['pong']\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "env = gym.make(params.env_name)\n",
    "env = ptan.common.wrappers.wrap_dqn(env)\n",
    "env.seed(SEED)\n",
    "\n",
    "net = DQN(env.observation_space.shape,\n",
    "                    env.action_space.n).to(device)\n",
    "\n",
    "tgt_net = ptan.agent.TargetNet(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we create the agent, passing it an epsilon-greedy action selector. During\n",
    "the training, epsilon will be decreased by the EpsilonTracker class that we have\n",
    "already discussed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "selector = ptan.actions.EpsilonGreedyActionSelector(\n",
    "        epsilon=params.epsilon_start)\n",
    "epsilon_tracker = EpsilonTracker(selector, params)\n",
    "agent = ptan.agent.DQNAgent(net, selector, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next two very important objects are ExperienceSource and\n",
    "ExperienceReplayBuffer. The first one takes the agent and environment and\n",
    "provides transitions over game episodes. Those transitions will be kept in the\n",
    "experience replay buffer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_source = ptan.experience.ExperienceSourceFirstLast(\n",
    "        env, agent, gamma=params.gamma)\n",
    "buffer = ptan.experience.ExperienceReplayBuffer(\n",
    "        exp_source, buffer_size=params.replay_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we create an optimizer and define the processing function, which will\n",
    "be called for every batch of transitions to train the model. To do this, we call\n",
    "function calc_loss_dqn and then backpropagate on the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(net.parameters(),\n",
    "                           lr=params.learning_rate)\n",
    "\n",
    "def process_batch(engine, batch):\n",
    "    optimizer.zero_grad()\n",
    "    loss_v = calc_loss_dqn(\n",
    "        batch, net, tgt_net.target_model,\n",
    "        gamma=params.gamma, device=device)\n",
    "    loss_v.backward()\n",
    "    optimizer.step()\n",
    "    epsilon_tracker.frame(engine.state.iteration)\n",
    "    if engine.state.iteration % params.target_net_sync == 0:\n",
    "        tgt_net.sync()\n",
    "    return {\n",
    "        \"loss\": loss_v.item(),\n",
    "        \"epsilon\": selector.epsilon,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function also asks EpsilonTracker to decrease the epsilon and does periodical\n",
    "target network synchronization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1: reward=-19, steps=1083, speed=0.0 f/s, elapsed=0:00:27\n",
      "Episode 2: reward=-19, steps=1037, speed=0.0 f/s, elapsed=0:00:27\n",
      "Episode 3: reward=-20, steps=898, speed=0.0 f/s, elapsed=0:00:27\n",
      "Episode 4: reward=-21, steps=1000, speed=0.0 f/s, elapsed=0:00:27\n",
      "Episode 5: reward=-21, steps=819, speed=0.0 f/s, elapsed=0:00:27\n",
      "Episode 6: reward=-21, steps=814, speed=0.0 f/s, elapsed=0:00:27\n",
      "Episode 7: reward=-21, steps=903, speed=0.0 f/s, elapsed=0:00:27\n",
      "Episode 8: reward=-21, steps=844, speed=0.0 f/s, elapsed=0:00:27\n",
      "Episode 9: reward=-21, steps=881, speed=0.0 f/s, elapsed=0:00:27\n",
      "Episode 10: reward=-18, steps=1141, speed=0.0 f/s, elapsed=0:00:27\n",
      "Episode 11: reward=-21, steps=816, speed=0.0 f/s, elapsed=0:00:27\n",
      "Episode 12: reward=-20, steps=991, speed=0.0 f/s, elapsed=0:00:27\n",
      "Episode 13: reward=-21, steps=867, speed=0.0 f/s, elapsed=0:00:27\n",
      "Episode 14: reward=-19, steps=1150, speed=0.0 f/s, elapsed=0:00:27\n",
      "Episode 15: reward=-21, steps=847, speed=0.0 f/s, elapsed=0:00:27\n",
      "Episode 16: reward=-21, steps=823, speed=0.0 f/s, elapsed=0:00:27\n",
      "Episode 17: reward=-21, steps=954, speed=0.0 f/s, elapsed=0:00:27\n",
      "Episode 18: reward=-21, steps=799, speed=0.0 f/s, elapsed=0:00:27\n",
      "Episode 19: reward=-21, steps=865, speed=0.0 f/s, elapsed=0:00:27\n",
      "Episode 20: reward=-21, steps=784, speed=0.0 f/s, elapsed=0:00:27\n",
      "Episode 21: reward=-21, steps=979, speed=0.0 f/s, elapsed=0:00:27\n",
      "Episode 22: reward=-20, steps=1077, speed=61.6 f/s, elapsed=0:00:33\n",
      "Episode 23: reward=-21, steps=849, speed=61.6 f/s, elapsed=0:00:47\n",
      "Episode 24: reward=-20, steps=911, speed=61.6 f/s, elapsed=0:01:02\n",
      "Episode 25: reward=-21, steps=853, speed=61.6 f/s, elapsed=0:01:15\n",
      "Episode 26: reward=-21, steps=930, speed=61.6 f/s, elapsed=0:01:30\n",
      "Episode 27: reward=-21, steps=759, speed=61.6 f/s, elapsed=0:01:43\n",
      "Episode 28: reward=-21, steps=934, speed=61.6 f/s, elapsed=0:01:58\n",
      "Episode 29: reward=-21, steps=896, speed=61.6 f/s, elapsed=0:02:12\n",
      "Episode 30: reward=-16, steps=1346, speed=61.6 f/s, elapsed=0:02:34\n",
      "Episode 31: reward=-20, steps=913, speed=61.6 f/s, elapsed=0:02:49\n",
      "Episode 32: reward=-21, steps=862, speed=61.6 f/s, elapsed=0:03:03\n",
      "Episode 33: reward=-21, steps=849, speed=61.6 f/s, elapsed=0:03:16\n",
      "Episode 34: reward=-18, steps=1197, speed=61.7 f/s, elapsed=0:03:35\n",
      "Episode 35: reward=-20, steps=913, speed=61.7 f/s, elapsed=0:03:50\n",
      "Episode 36: reward=-20, steps=933, speed=61.7 f/s, elapsed=0:04:05\n",
      "Episode 37: reward=-20, steps=884, speed=61.7 f/s, elapsed=0:04:20\n",
      "Episode 38: reward=-20, steps=943, speed=61.7 f/s, elapsed=0:04:35\n",
      "Episode 39: reward=-21, steps=877, speed=61.7 f/s, elapsed=0:04:49\n",
      "Episode 40: reward=-20, steps=923, speed=61.7 f/s, elapsed=0:05:04\n",
      "Episode 41: reward=-20, steps=910, speed=61.7 f/s, elapsed=0:05:19\n",
      "Episode 42: reward=-19, steps=1049, speed=61.7 f/s, elapsed=0:05:35\n",
      "Episode 43: reward=-21, steps=921, speed=61.7 f/s, elapsed=0:05:50\n",
      "Episode 44: reward=-20, steps=945, speed=61.7 f/s, elapsed=0:06:06\n",
      "Episode 45: reward=-21, steps=788, speed=61.7 f/s, elapsed=0:06:18\n",
      "Episode 46: reward=-18, steps=1178, speed=61.7 f/s, elapsed=0:06:37\n",
      "Episode 47: reward=-21, steps=896, speed=61.7 f/s, elapsed=0:06:52\n",
      "Episode 48: reward=-20, steps=871, speed=61.7 f/s, elapsed=0:07:06\n",
      "Episode 49: reward=-21, steps=787, speed=61.7 f/s, elapsed=0:07:18\n",
      "Episode 50: reward=-20, steps=1005, speed=61.7 f/s, elapsed=0:07:35\n",
      "Episode 51: reward=-21, steps=874, speed=61.8 f/s, elapsed=0:07:49\n",
      "Episode 52: reward=-20, steps=883, speed=61.8 f/s, elapsed=0:08:03\n",
      "Episode 53: reward=-21, steps=881, speed=61.8 f/s, elapsed=0:08:17\n",
      "Episode 54: reward=-21, steps=836, speed=61.8 f/s, elapsed=0:08:31\n",
      "Episode 55: reward=-20, steps=977, speed=61.8 f/s, elapsed=0:08:46\n",
      "Episode 56: reward=-21, steps=789, speed=61.8 f/s, elapsed=0:08:59\n",
      "Episode 57: reward=-20, steps=1011, speed=61.8 f/s, elapsed=0:09:15\n",
      "Episode 58: reward=-21, steps=940, speed=61.8 f/s, elapsed=0:09:31\n",
      "Episode 59: reward=-21, steps=820, speed=61.8 f/s, elapsed=0:09:44\n",
      "Episode 60: reward=-21, steps=785, speed=61.8 f/s, elapsed=0:09:56\n",
      "Episode 61: reward=-21, steps=960, speed=61.8 f/s, elapsed=0:10:12\n",
      "Episode 62: reward=-21, steps=823, speed=61.8 f/s, elapsed=0:10:25\n",
      "Episode 63: reward=-21, steps=785, speed=61.8 f/s, elapsed=0:10:38\n",
      "Episode 64: reward=-21, steps=943, speed=61.8 f/s, elapsed=0:10:53\n",
      "Episode 65: reward=-21, steps=840, speed=61.8 f/s, elapsed=0:11:07\n",
      "Episode 66: reward=-21, steps=1067, speed=61.8 f/s, elapsed=0:11:24\n",
      "Episode 67: reward=-20, steps=1002, speed=61.8 f/s, elapsed=0:11:40\n",
      "Episode 68: reward=-19, steps=989, speed=61.8 f/s, elapsed=0:11:56\n",
      "Episode 69: reward=-20, steps=927, speed=61.8 f/s, elapsed=0:12:11\n",
      "Episode 70: reward=-21, steps=756, speed=61.8 f/s, elapsed=0:12:23\n",
      "Episode 71: reward=-21, steps=762, speed=61.8 f/s, elapsed=0:12:35\n",
      "Episode 72: reward=-21, steps=893, speed=61.8 f/s, elapsed=0:12:50\n",
      "Episode 73: reward=-18, steps=1008, speed=61.8 f/s, elapsed=0:13:06\n",
      "Episode 74: reward=-21, steps=880, speed=61.9 f/s, elapsed=0:13:20\n",
      "Episode 75: reward=-21, steps=878, speed=61.9 f/s, elapsed=0:13:34\n",
      "Episode 76: reward=-21, steps=760, speed=61.9 f/s, elapsed=0:13:46\n",
      "Episode 77: reward=-21, steps=964, speed=61.9 f/s, elapsed=0:14:02\n",
      "Episode 78: reward=-19, steps=937, speed=61.9 f/s, elapsed=0:14:17\n",
      "Episode 79: reward=-20, steps=994, speed=61.9 f/s, elapsed=0:14:33\n",
      "Episode 80: reward=-21, steps=761, speed=61.9 f/s, elapsed=0:14:45\n",
      "Episode 81: reward=-21, steps=884, speed=61.9 f/s, elapsed=0:15:00\n",
      "Episode 82: reward=-20, steps=891, speed=61.9 f/s, elapsed=0:15:14\n",
      "Episode 83: reward=-20, steps=988, speed=61.9 f/s, elapsed=0:15:30\n",
      "Episode 84: reward=-21, steps=879, speed=61.9 f/s, elapsed=0:15:44\n",
      "Episode 85: reward=-21, steps=917, speed=61.9 f/s, elapsed=0:15:59\n",
      "Episode 86: reward=-20, steps=1000, speed=61.9 f/s, elapsed=0:16:15\n",
      "Episode 87: reward=-21, steps=868, speed=61.9 f/s, elapsed=0:16:29\n",
      "Episode 88: reward=-20, steps=901, speed=61.9 f/s, elapsed=0:16:43\n",
      "Episode 89: reward=-19, steps=1020, speed=61.9 f/s, elapsed=0:17:00\n",
      "Episode 90: reward=-21, steps=816, speed=61.9 f/s, elapsed=0:17:13\n",
      "Episode 91: reward=-21, steps=819, speed=61.9 f/s, elapsed=0:17:26\n",
      "Episode 92: reward=-21, steps=836, speed=61.9 f/s, elapsed=0:17:40\n",
      "Episode 93: reward=-19, steps=1001, speed=61.9 f/s, elapsed=0:17:56\n",
      "Episode 94: reward=-20, steps=895, speed=61.9 f/s, elapsed=0:18:10\n",
      "Episode 95: reward=-21, steps=816, speed=61.9 f/s, elapsed=0:18:23\n",
      "Episode 96: reward=-21, steps=792, speed=61.9 f/s, elapsed=0:18:36\n",
      "Episode 97: reward=-21, steps=882, speed=61.9 f/s, elapsed=0:18:50\n",
      "Episode 98: reward=-20, steps=891, speed=61.9 f/s, elapsed=0:19:05\n",
      "Episode 99: reward=-21, steps=817, speed=61.9 f/s, elapsed=0:19:18\n",
      "Episode 100: reward=-21, steps=950, speed=61.9 f/s, elapsed=0:19:33\n",
      "Episode 101: reward=-21, steps=881, speed=61.9 f/s, elapsed=0:19:48\n",
      "Episode 102: reward=-21, steps=817, speed=61.9 f/s, elapsed=0:20:01\n",
      "Episode 103: reward=-21, steps=840, speed=61.9 f/s, elapsed=0:20:14\n",
      "Episode 104: reward=-21, steps=881, speed=61.9 f/s, elapsed=0:20:28\n",
      "Episode 105: reward=-20, steps=945, speed=61.9 f/s, elapsed=0:20:44\n",
      "Episode 106: reward=-20, steps=861, speed=61.9 f/s, elapsed=0:20:58\n",
      "Episode 107: reward=-21, steps=818, speed=61.9 f/s, elapsed=0:21:11\n",
      "Episode 108: reward=-21, steps=823, speed=61.9 f/s, elapsed=0:21:24\n",
      "Episode 109: reward=-21, steps=1000, speed=61.9 f/s, elapsed=0:21:40\n",
      "Episode 110: reward=-21, steps=901, speed=62.0 f/s, elapsed=0:21:55\n",
      "Episode 111: reward=-21, steps=896, speed=62.0 f/s, elapsed=0:22:09\n",
      "Episode 112: reward=-20, steps=900, speed=62.0 f/s, elapsed=0:22:23\n",
      "Episode 113: reward=-21, steps=880, speed=62.0 f/s, elapsed=0:22:38\n",
      "Episode 114: reward=-20, steps=975, speed=62.0 f/s, elapsed=0:22:53\n",
      "Episode 115: reward=-21, steps=823, speed=62.0 f/s, elapsed=0:23:07\n",
      "Episode 116: reward=-20, steps=914, speed=62.0 f/s, elapsed=0:23:21\n",
      "Episode 117: reward=-21, steps=1004, speed=62.0 f/s, elapsed=0:23:37\n",
      "Episode 118: reward=-21, steps=960, speed=62.0 f/s, elapsed=0:23:53\n",
      "Episode 119: reward=-21, steps=1063, speed=62.0 f/s, elapsed=0:24:10\n",
      "Episode 120: reward=-20, steps=1106, speed=62.0 f/s, elapsed=0:24:28\n",
      "Episode 121: reward=-21, steps=1118, speed=62.0 f/s, elapsed=0:24:46\n",
      "Episode 122: reward=-21, steps=911, speed=62.0 f/s, elapsed=0:25:01\n",
      "Episode 123: reward=-21, steps=863, speed=62.0 f/s, elapsed=0:25:14\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 124: reward=-21, steps=896, speed=62.0 f/s, elapsed=0:25:29\n",
      "Episode 125: reward=-21, steps=897, speed=62.0 f/s, elapsed=0:25:43\n",
      "Episode 126: reward=-21, steps=843, speed=62.0 f/s, elapsed=0:25:57\n",
      "Episode 127: reward=-21, steps=846, speed=62.0 f/s, elapsed=0:26:10\n",
      "Episode 128: reward=-21, steps=1034, speed=62.0 f/s, elapsed=0:26:27\n",
      "Episode 129: reward=-21, steps=784, speed=62.0 f/s, elapsed=0:26:40\n",
      "Episode 130: reward=-20, steps=1430, speed=62.0 f/s, elapsed=0:27:03\n",
      "Episode 131: reward=-21, steps=820, speed=62.0 f/s, elapsed=0:27:16\n",
      "Episode 132: reward=-21, steps=898, speed=62.0 f/s, elapsed=0:27:30\n",
      "Episode 133: reward=-21, steps=943, speed=62.0 f/s, elapsed=0:27:45\n",
      "Episode 134: reward=-19, steps=1138, speed=62.0 f/s, elapsed=0:28:04\n",
      "Episode 135: reward=-20, steps=1275, speed=62.0 f/s, elapsed=0:28:24\n",
      "Episode 136: reward=-20, steps=1160, speed=62.0 f/s, elapsed=0:28:43\n",
      "Episode 137: reward=-21, steps=1182, speed=62.0 f/s, elapsed=0:29:02\n",
      "Episode 138: reward=-21, steps=879, speed=62.0 f/s, elapsed=0:29:16\n",
      "Episode 139: reward=-21, steps=1305, speed=62.0 f/s, elapsed=0:29:37\n",
      "Episode 140: reward=-21, steps=844, speed=62.0 f/s, elapsed=0:29:51\n",
      "Episode 141: reward=-21, steps=1057, speed=62.0 f/s, elapsed=0:30:08\n",
      "Episode 142: reward=-21, steps=1019, speed=62.0 f/s, elapsed=0:30:24\n",
      "Episode 143: reward=-21, steps=848, speed=62.0 f/s, elapsed=0:30:38\n",
      "Episode 144: reward=-21, steps=935, speed=62.0 f/s, elapsed=0:30:53\n",
      "Episode 145: reward=-21, steps=970, speed=62.1 f/s, elapsed=0:31:08\n",
      "Episode 146: reward=-21, steps=1092, speed=62.1 f/s, elapsed=0:31:26\n",
      "Episode 147: reward=-20, steps=1149, speed=62.1 f/s, elapsed=0:31:44\n",
      "Episode 148: reward=-21, steps=883, speed=62.1 f/s, elapsed=0:31:59\n",
      "Episode 149: reward=-21, steps=877, speed=62.1 f/s, elapsed=0:32:13\n",
      "Episode 150: reward=-21, steps=1123, speed=62.1 f/s, elapsed=0:32:31\n",
      "Episode 151: reward=-20, steps=1086, speed=62.1 f/s, elapsed=0:32:48\n",
      "Episode 152: reward=-20, steps=1158, speed=62.1 f/s, elapsed=0:33:07\n",
      "Episode 153: reward=-21, steps=1120, speed=62.1 f/s, elapsed=0:33:25\n",
      "Episode 154: reward=-21, steps=1299, speed=62.1 f/s, elapsed=0:33:46\n",
      "Episode 155: reward=-20, steps=1220, speed=62.1 f/s, elapsed=0:34:05\n",
      "Episode 156: reward=-20, steps=1172, speed=62.1 f/s, elapsed=0:34:24\n",
      "Episode 157: reward=-20, steps=1922, speed=62.1 f/s, elapsed=0:34:55\n",
      "Episode 158: reward=-20, steps=1511, speed=62.1 f/s, elapsed=0:35:19\n",
      "Episode 159: reward=-21, steps=1175, speed=62.1 f/s, elapsed=0:35:38\n",
      "Episode 160: reward=-18, steps=1767, speed=62.1 f/s, elapsed=0:36:07\n",
      "Episode 161: reward=-19, steps=1617, speed=62.1 f/s, elapsed=0:36:33\n",
      "Episode 162: reward=-21, steps=2102, speed=62.1 f/s, elapsed=0:37:06\n",
      "Episode 163: reward=-19, steps=1985, speed=62.1 f/s, elapsed=0:37:38\n",
      "Episode 164: reward=-20, steps=1743, speed=62.1 f/s, elapsed=0:38:06\n",
      "Episode 165: reward=-21, steps=1309, speed=62.1 f/s, elapsed=0:38:27\n",
      "Episode 166: reward=-20, steps=1582, speed=62.1 f/s, elapsed=0:38:53\n",
      "Episode 167: reward=-18, steps=1907, speed=62.1 f/s, elapsed=0:39:23\n",
      "Episode 168: reward=-21, steps=1899, speed=62.1 f/s, elapsed=0:39:54\n",
      "Episode 169: reward=-20, steps=1824, speed=62.1 f/s, elapsed=0:40:23\n",
      "Episode 170: reward=-20, steps=1022, speed=62.1 f/s, elapsed=0:40:39\n",
      "Episode 171: reward=-17, steps=1674, speed=62.1 f/s, elapsed=0:41:06\n",
      "Episode 172: reward=-20, steps=1361, speed=62.1 f/s, elapsed=0:41:28\n",
      "Episode 173: reward=-20, steps=1561, speed=62.1 f/s, elapsed=0:41:53\n",
      "Episode 174: reward=-20, steps=1230, speed=62.1 f/s, elapsed=0:42:13\n",
      "Episode 175: reward=-21, steps=1105, speed=62.1 f/s, elapsed=0:42:31\n",
      "Episode 176: reward=-21, steps=1549, speed=62.1 f/s, elapsed=0:42:55\n",
      "Episode 177: reward=-20, steps=1959, speed=62.1 f/s, elapsed=0:43:27\n",
      "Episode 178: reward=-20, steps=1290, speed=62.2 f/s, elapsed=0:43:48\n",
      "Episode 179: reward=-20, steps=1676, speed=62.2 f/s, elapsed=0:44:15\n",
      "Episode 180: reward=-21, steps=1517, speed=62.2 f/s, elapsed=0:44:39\n",
      "Episode 181: reward=-19, steps=2073, speed=62.2 f/s, elapsed=0:45:12\n",
      "Episode 182: reward=-19, steps=1493, speed=62.2 f/s, elapsed=0:45:36\n",
      "Episode 183: reward=-21, steps=1637, speed=62.2 f/s, elapsed=0:46:03\n",
      "Episode 184: reward=-20, steps=1520, speed=62.2 f/s, elapsed=0:46:27\n",
      "Episode 185: reward=-19, steps=1967, speed=62.2 f/s, elapsed=0:46:59\n",
      "Episode 186: reward=-19, steps=1529, speed=62.2 f/s, elapsed=0:47:23\n",
      "Episode 187: reward=-18, steps=1935, speed=62.2 f/s, elapsed=0:47:54\n",
      "Episode 188: reward=-21, steps=1878, speed=62.2 f/s, elapsed=0:48:24\n",
      "Episode 189: reward=-20, steps=1758, speed=62.2 f/s, elapsed=0:48:53\n",
      "Episode 190: reward=-21, steps=1208, speed=62.2 f/s, elapsed=0:49:12\n",
      "Episode 191: reward=-19, steps=2046, speed=62.2 f/s, elapsed=0:49:45\n",
      "Episode 192: reward=-16, steps=2438, speed=62.2 f/s, elapsed=0:50:24\n",
      "Episode 193: reward=-19, steps=1779, speed=62.2 f/s, elapsed=0:50:53\n",
      "Episode 194: reward=-15, steps=2152, speed=62.2 f/s, elapsed=0:51:27\n",
      "Episode 195: reward=-21, steps=1603, speed=62.2 f/s, elapsed=0:51:53\n",
      "Episode 196: reward=-20, steps=1867, speed=62.2 f/s, elapsed=0:52:23\n",
      "Episode 197: reward=-19, steps=2260, speed=62.2 f/s, elapsed=0:52:59\n",
      "Episode 198: reward=-18, steps=1791, speed=62.2 f/s, elapsed=0:53:28\n",
      "Episode 199: reward=-14, steps=2055, speed=62.2 f/s, elapsed=0:54:01\n",
      "Episode 200: reward=-15, steps=2247, speed=62.2 f/s, elapsed=0:54:37\n",
      "Episode 201: reward=-15, steps=2192, speed=62.2 f/s, elapsed=0:55:12\n",
      "Episode 202: reward=-20, steps=2048, speed=62.2 f/s, elapsed=0:55:45\n",
      "Episode 203: reward=-18, steps=1910, speed=62.2 f/s, elapsed=0:56:16\n",
      "Episode 204: reward=-17, steps=2334, speed=62.2 f/s, elapsed=0:56:53\n",
      "Episode 205: reward=-20, steps=1655, speed=62.2 f/s, elapsed=0:57:20\n",
      "Episode 206: reward=-13, steps=2556, speed=62.2 f/s, elapsed=0:58:01\n",
      "Episode 207: reward=-18, steps=1721, speed=62.2 f/s, elapsed=0:58:28\n",
      "Episode 208: reward=-17, steps=2363, speed=62.2 f/s, elapsed=0:59:06\n",
      "Episode 209: reward=-17, steps=1824, speed=62.2 f/s, elapsed=0:59:35\n",
      "Episode 210: reward=-15, steps=2289, speed=62.2 f/s, elapsed=1:00:12\n",
      "Episode 211: reward=-21, steps=1674, speed=62.2 f/s, elapsed=1:00:39\n",
      "Episode 212: reward=-16, steps=2080, speed=62.2 f/s, elapsed=1:01:13\n",
      "Episode 213: reward=-20, steps=1870, speed=62.2 f/s, elapsed=1:01:43\n",
      "Episode 214: reward=-18, steps=1738, speed=62.2 f/s, elapsed=1:02:11\n",
      "Episode 215: reward=-17, steps=1887, speed=62.2 f/s, elapsed=1:02:41\n",
      "Episode 216: reward=-19, steps=2100, speed=62.2 f/s, elapsed=1:03:15\n",
      "Episode 217: reward=-15, steps=2104, speed=62.2 f/s, elapsed=1:03:49\n",
      "Episode 218: reward=-20, steps=1937, speed=62.2 f/s, elapsed=1:04:20\n",
      "Episode 219: reward=-19, steps=1874, speed=62.2 f/s, elapsed=1:04:51\n",
      "Episode 220: reward=-17, steps=1884, speed=62.2 f/s, elapsed=1:05:21\n",
      "Episode 221: reward=-16, steps=2228, speed=62.2 f/s, elapsed=1:05:57\n",
      "Episode 222: reward=-19, steps=2355, speed=62.2 f/s, elapsed=1:06:35\n",
      "Episode 223: reward=-19, steps=1647, speed=62.1 f/s, elapsed=1:07:02\n",
      "Episode 224: reward=-15, steps=2657, speed=62.1 f/s, elapsed=1:07:45\n",
      "Episode 225: reward=-21, steps=2061, speed=62.1 f/s, elapsed=1:08:18\n",
      "Episode 226: reward=-14, steps=2213, speed=62.1 f/s, elapsed=1:08:54\n",
      "Episode 227: reward=-19, steps=1781, speed=62.1 f/s, elapsed=1:09:23\n",
      "Episode 228: reward=-17, steps=1753, speed=62.1 f/s, elapsed=1:09:52\n",
      "Episode 229: reward=-17, steps=1900, speed=62.1 f/s, elapsed=1:10:22\n",
      "Episode 230: reward=-13, steps=1942, speed=62.1 f/s, elapsed=1:10:54\n",
      "Episode 231: reward=-12, steps=1973, speed=62.1 f/s, elapsed=1:11:26\n",
      "Episode 232: reward=-12, steps=2059, speed=62.1 f/s, elapsed=1:11:59\n",
      "Episode 233: reward=-13, steps=2277, speed=62.1 f/s, elapsed=1:12:36\n",
      "Episode 234: reward=-16, steps=1811, speed=62.0 f/s, elapsed=1:13:06\n",
      "Episode 235: reward=-19, steps=1576, speed=62.0 f/s, elapsed=1:13:31\n",
      "Episode 236: reward=-19, steps=1415, speed=62.0 f/s, elapsed=1:13:54\n",
      "Episode 237: reward=-19, steps=1499, speed=62.0 f/s, elapsed=1:14:18\n",
      "Episode 238: reward=-10, steps=2035, speed=62.0 f/s, elapsed=1:14:52\n",
      "Episode 239: reward=-11, steps=1944, speed=62.0 f/s, elapsed=1:15:23\n",
      "Episode 240: reward=-20, steps=1340, speed=62.0 f/s, elapsed=1:15:45\n",
      "Episode 241: reward=-20, steps=1410, speed=62.0 f/s, elapsed=1:16:08\n",
      "Episode 242: reward=-19, steps=1526, speed=62.0 f/s, elapsed=1:16:32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 243: reward=-11, steps=2145, speed=62.0 f/s, elapsed=1:17:07\n",
      "Episode 244: reward=-10, steps=2600, speed=62.0 f/s, elapsed=1:17:50\n",
      "Episode 245: reward=-13, steps=2275, speed=62.0 f/s, elapsed=1:18:27\n",
      "Episode 246: reward=-9, steps=2374, speed=61.9 f/s, elapsed=1:19:05\n",
      "Episode 247: reward=-17, steps=1774, speed=61.9 f/s, elapsed=1:19:34\n",
      "Episode 248: reward=-10, steps=2101, speed=61.9 f/s, elapsed=1:20:09\n",
      "Episode 249: reward=-13, steps=1990, speed=61.9 f/s, elapsed=1:20:41\n",
      "Episode 250: reward=-11, steps=1981, speed=61.9 f/s, elapsed=1:21:13\n",
      "Episode 251: reward=-3, steps=3589, speed=61.9 f/s, elapsed=1:22:11\n",
      "Episode 252: reward=-10, steps=2282, speed=61.9 f/s, elapsed=1:22:49\n",
      "Episode 253: reward=-20, steps=1518, speed=61.9 f/s, elapsed=1:23:13\n",
      "Episode 254: reward=-20, steps=1458, speed=61.9 f/s, elapsed=1:23:37\n",
      "Episode 255: reward=-12, steps=1842, speed=61.9 f/s, elapsed=1:24:07\n",
      "Episode 256: reward=-6, steps=2755, speed=61.9 f/s, elapsed=1:24:52\n",
      "Episode 257: reward=-5, steps=2513, speed=61.9 f/s, elapsed=1:25:32\n",
      "Episode 258: reward=-18, steps=1705, speed=61.9 f/s, elapsed=1:26:00\n",
      "Episode 259: reward=-13, steps=2196, speed=61.8 f/s, elapsed=1:26:36\n",
      "Episode 260: reward=-11, steps=2465, speed=61.8 f/s, elapsed=1:27:16\n",
      "Episode 261: reward=-16, steps=2065, speed=61.8 f/s, elapsed=1:27:49\n",
      "Episode 262: reward=-13, steps=2059, speed=61.8 f/s, elapsed=1:28:23\n",
      "Episode 263: reward=-12, steps=2353, speed=61.8 f/s, elapsed=1:29:01\n",
      "Episode 264: reward=-8, steps=2848, speed=61.8 f/s, elapsed=1:29:47\n",
      "Episode 265: reward=-9, steps=2547, speed=61.8 f/s, elapsed=1:30:29\n",
      "Episode 266: reward=-19, steps=2229, speed=61.8 f/s, elapsed=1:31:05\n",
      "Episode 267: reward=-3, steps=3576, speed=61.8 f/s, elapsed=1:32:03\n",
      "Episode 268: reward=-1, steps=3592, speed=61.8 f/s, elapsed=1:33:01\n",
      "Episode 269: reward=-6, steps=2985, speed=61.8 f/s, elapsed=1:33:50\n",
      "Episode 270: reward=-17, steps=1913, speed=61.8 f/s, elapsed=1:34:21\n",
      "Episode 271: reward=-12, steps=2750, speed=61.8 f/s, elapsed=1:35:05\n",
      "Episode 272: reward=-3, steps=3333, speed=61.8 f/s, elapsed=1:36:00\n",
      "Episode 273: reward=-7, steps=2745, speed=61.8 f/s, elapsed=1:36:44\n",
      "Episode 274: reward=-8, steps=2699, speed=61.8 f/s, elapsed=1:37:28\n",
      "Episode 275: reward=-10, steps=2791, speed=61.8 f/s, elapsed=1:38:13\n",
      "Episode 276: reward=-5, steps=3139, speed=61.8 f/s, elapsed=1:39:04\n",
      "Episode 277: reward=-4, steps=3538, speed=61.8 f/s, elapsed=1:40:02\n",
      "Episode 278: reward=-10, steps=2267, speed=61.8 f/s, elapsed=1:40:38\n",
      "Episode 279: reward=-1, steps=3417, speed=61.8 f/s, elapsed=1:41:34\n",
      "Episode 280: reward=-14, steps=2111, speed=61.8 f/s, elapsed=1:42:08\n",
      "Episode 281: reward=-8, steps=2605, speed=61.7 f/s, elapsed=1:42:51\n",
      "Episode 282: reward=-9, steps=2662, speed=61.8 f/s, elapsed=1:43:34\n",
      "Episode 283: reward=-9, steps=2616, speed=61.8 f/s, elapsed=1:44:16\n",
      "Episode 284: reward=-2, steps=3215, speed=61.8 f/s, elapsed=1:45:08\n",
      "Episode 285: reward=-4, steps=3089, speed=61.8 f/s, elapsed=1:45:58\n",
      "Episode 286: reward=3, steps=3598, speed=61.8 f/s, elapsed=1:46:56\n",
      "Episode 287: reward=-7, steps=2787, speed=61.8 f/s, elapsed=1:47:41\n",
      "Episode 288: reward=-5, steps=2917, speed=61.8 f/s, elapsed=1:48:28\n",
      "Episode 289: reward=-7, steps=2947, speed=61.8 f/s, elapsed=1:49:15\n",
      "Episode 290: reward=-6, steps=2817, speed=61.8 f/s, elapsed=1:50:00\n",
      "Episode 291: reward=-6, steps=2492, speed=61.8 f/s, elapsed=1:50:41\n",
      "Episode 292: reward=-8, steps=2832, speed=61.8 f/s, elapsed=1:51:26\n",
      "Episode 293: reward=-3, steps=2979, speed=61.8 f/s, elapsed=1:52:14\n",
      "Episode 294: reward=-4, steps=3317, speed=61.8 f/s, elapsed=1:53:08\n",
      "Episode 295: reward=3, steps=3571, speed=61.8 f/s, elapsed=1:54:06\n",
      "Episode 296: reward=-4, steps=3059, speed=61.8 f/s, elapsed=1:54:55\n",
      "Episode 297: reward=-7, steps=2833, speed=61.8 f/s, elapsed=1:55:41\n",
      "Episode 298: reward=-8, steps=2719, speed=61.8 f/s, elapsed=1:56:24\n",
      "Episode 299: reward=-5, steps=3131, speed=61.8 f/s, elapsed=1:57:15\n",
      "Episode 300: reward=-11, steps=2498, speed=61.8 f/s, elapsed=1:57:55\n",
      "Episode 301: reward=-4, steps=2981, speed=61.8 f/s, elapsed=1:58:44\n",
      "Episode 302: reward=-6, steps=2882, speed=61.8 f/s, elapsed=1:59:30\n",
      "Episode 303: reward=7, steps=2985, speed=61.8 f/s, elapsed=2:00:18\n",
      "Episode 304: reward=-5, steps=2996, speed=61.8 f/s, elapsed=2:01:07\n",
      "Episode 305: reward=-3, steps=3323, speed=61.8 f/s, elapsed=2:02:00\n",
      "Episode 306: reward=-4, steps=3154, speed=61.8 f/s, elapsed=2:02:51\n",
      "Episode 307: reward=-2, steps=3475, speed=61.8 f/s, elapsed=2:03:47\n",
      "Episode 308: reward=-3, steps=3102, speed=61.8 f/s, elapsed=2:04:37\n",
      "Episode 309: reward=-7, steps=2810, speed=61.8 f/s, elapsed=2:05:23\n",
      "Episode 310: reward=-4, steps=3248, speed=61.8 f/s, elapsed=2:06:15\n",
      "Episode 311: reward=-1, steps=3530, speed=61.8 f/s, elapsed=2:07:12\n",
      "Episode 312: reward=7, steps=2816, speed=61.8 f/s, elapsed=2:07:58\n",
      "Episode 313: reward=3, steps=3261, speed=61.8 f/s, elapsed=2:08:50\n",
      "Episode 314: reward=2, steps=3451, speed=61.8 f/s, elapsed=2:09:46\n",
      "Episode 315: reward=-9, steps=2537, speed=61.9 f/s, elapsed=2:10:27\n",
      "Episode 316: reward=2, steps=3302, speed=61.9 f/s, elapsed=2:11:20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/anton/envs/reinforcement_learning/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 3343, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-18-c80b81bfcb1c>\", line 4, in <module>\n",
      "    params.batch_size))\n",
      "  File \"/home/anton/envs/reinforcement_learning/lib/python3.6/site-packages/ignite/engine/engine.py\", line 446, in run\n",
      "    self._handle_exception(e)\n",
      "  File \"/home/anton/envs/reinforcement_learning/lib/python3.6/site-packages/ignite/engine/engine.py\", line 410, in _handle_exception\n",
      "    raise e\n",
      "  File \"/home/anton/envs/reinforcement_learning/lib/python3.6/site-packages/ignite/engine/engine.py\", line 433, in run\n",
      "    hours, mins, secs = self._run_once_on_dataset()\n",
      "  File \"/home/anton/envs/reinforcement_learning/lib/python3.6/site-packages/ignite/engine/engine.py\", line 399, in _run_once_on_dataset\n",
      "    self._handle_exception(e)\n",
      "  File \"/home/anton/envs/reinforcement_learning/lib/python3.6/site-packages/ignite/engine/engine.py\", line 410, in _handle_exception\n",
      "    raise e\n",
      "  File \"/home/anton/envs/reinforcement_learning/lib/python3.6/site-packages/ignite/engine/engine.py\", line 387, in _run_once_on_dataset\n",
      "    for batch in self.state.dataloader:\n",
      "  File \"<ipython-input-7-4f6f4f280fbe>\", line 5, in batch_generator\n",
      "    buffer.populate(1)\n",
      "  File \"/home/anton/envs/reinforcement_learning/lib/python3.6/site-packages/ptan/experience.py\", line 368, in populate\n",
      "    entry = next(self.experience_source_iter)\n",
      "  File \"/home/anton/envs/reinforcement_learning/lib/python3.6/site-packages/ptan/experience.py\", line 176, in __iter__\n",
      "    for exp in super(ExperienceSourceFirstLast, self).__iter__():\n",
      "  File \"/home/anton/envs/reinforcement_learning/lib/python3.6/site-packages/ptan/experience.py\", line 94, in __iter__\n",
      "    next_state, r, is_done, _ = env.step(action_n[0])\n",
      "  File \"/home/anton/envs/reinforcement_learning/lib/python3.6/site-packages/gym/core.py\", line 273, in step\n",
      "    observation, reward, done, info = self.env.step(action)\n",
      "  File \"/home/anton/envs/reinforcement_learning/lib/python3.6/site-packages/ptan/common/wrappers.py\", line 196, in step\n",
      "    ob, reward, done, info = self.env.step(action)\n",
      "  File \"/home/anton/envs/reinforcement_learning/lib/python3.6/site-packages/gym/core.py\", line 261, in step\n",
      "    observation, reward, done, info = self.env.step(action)\n",
      "  File \"/home/anton/envs/reinforcement_learning/lib/python3.6/site-packages/gym/core.py\", line 262, in step\n",
      "    return self.observation(observation), reward, done, info\n",
      "  File \"/home/anton/envs/reinforcement_learning/lib/python3.6/site-packages/ptan/common/wrappers.py\", line 136, in observation\n",
      "    return ProcessFrame84.process(obs)\n",
      "  File \"/home/anton/envs/reinforcement_learning/lib/python3.6/site-packages/ptan/common/wrappers.py\", line 147, in process\n",
      "    resized_screen = cv2.resize(img, (84, 110), interpolation=cv2.INTER_AREA)\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/anton/envs/reinforcement_learning/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2044, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/anton/envs/reinforcement_learning/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 1169, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"/home/anton/envs/reinforcement_learning/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 316, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/anton/envs/reinforcement_learning/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 350, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"/usr/lib/python3.6/inspect.py\", line 1490, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"/usr/lib/python3.6/inspect.py\", line 1448, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"/usr/lib/python3.6/inspect.py\", line 696, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"/usr/lib/python3.6/inspect.py\", line 732, in getmodule\n",
      "    for modname, module in list(sys.modules.items()):\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "object of type 'NoneType' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "\u001b[0;32m<ipython-input-18-c80b81bfcb1c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m engine.run(batch_generator(buffer, params.replay_initial,\n\u001b[0;32m----> 4\u001b[0;31m                                   params.batch_size))\n\u001b[0m",
      "\u001b[0;32m~/envs/reinforcement_learning/lib/python3.6/site-packages/ignite/engine/engine.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, data, max_epochs)\u001b[0m\n\u001b[1;32m    445\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_logger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Engine run is terminating due to exception: %s.\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 446\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    447\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/envs/reinforcement_learning/lib/python3.6/site-packages/ignite/engine/engine.py\u001b[0m in \u001b[0;36m_handle_exception\u001b[0;34m(self, e)\u001b[0m\n\u001b[1;32m    409\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 410\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    411\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/envs/reinforcement_learning/lib/python3.6/site-packages/ignite/engine/engine.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, data, max_epochs)\u001b[0m\n\u001b[1;32m    432\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fire_event\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEvents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEPOCH_STARTED\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 433\u001b[0;31m                 \u001b[0mhours\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmins\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msecs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_once_on_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    434\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_logger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Epoch[%s] Complete. Time taken: %02d:%02d:%02d\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhours\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmins\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msecs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/envs/reinforcement_learning/lib/python3.6/site-packages/ignite/engine/engine.py\u001b[0m in \u001b[0;36m_run_once_on_dataset\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    398\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_logger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Current run is terminating due to exception: %s.\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 399\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    400\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/envs/reinforcement_learning/lib/python3.6/site-packages/ignite/engine/engine.py\u001b[0m in \u001b[0;36m_handle_exception\u001b[0;34m(self, e)\u001b[0m\n\u001b[1;32m    409\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 410\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    411\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/envs/reinforcement_learning/lib/python3.6/site-packages/ignite/engine/engine.py\u001b[0m in \u001b[0;36m_run_once_on_dataset\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    386\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 387\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    388\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-4f6f4f280fbe>\u001b[0m in \u001b[0;36mbatch_generator\u001b[0;34m(buffer, initial, batch_size)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0mbuffer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpopulate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m         \u001b[0;32myield\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/envs/reinforcement_learning/lib/python3.6/site-packages/ptan/experience.py\u001b[0m in \u001b[0;36mpopulate\u001b[0;34m(self, samples)\u001b[0m\n\u001b[1;32m    367\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 368\u001b[0;31m             \u001b[0mentry\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperience_source_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    369\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_add\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mentry\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/envs/reinforcement_learning/lib/python3.6/site-packages/ptan/experience.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    175\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 176\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mexp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mExperienceSourceFirstLast\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    177\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mexp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/envs/reinforcement_learning/lib/python3.6/site-packages/ptan/experience.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     93\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m                     \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_done\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction_n\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m                     \u001b[0mnext_state_n\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr_n\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_done_n\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnext_state\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mis_done\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/envs/reinforcement_learning/lib/python3.6/site-packages/gym/core.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    272\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 273\u001b[0;31m         \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    274\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreward\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/envs/reinforcement_learning/lib/python3.6/site-packages/ptan/common/wrappers.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    195\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m         \u001b[0mob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/envs/reinforcement_learning/lib/python3.6/site-packages/gym/core.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    260\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 261\u001b[0;31m         \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    262\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/envs/reinforcement_learning/lib/python3.6/site-packages/gym/core.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    261\u001b[0m         \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 262\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    263\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/envs/reinforcement_learning/lib/python3.6/site-packages/ptan/common/wrappers.py\u001b[0m in \u001b[0;36mobservation\u001b[0;34m(self, obs)\u001b[0m\n\u001b[1;32m    135\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mobservation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 136\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mProcessFrame84\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    137\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/envs/reinforcement_learning/lib/python3.6/site-packages/ptan/common/wrappers.py\u001b[0m in \u001b[0;36mprocess\u001b[0;34m(frame)\u001b[0m\n\u001b[1;32m    146\u001b[0m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m0.299\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m0.587\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m0.114\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m         \u001b[0mresized_screen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m84\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m110\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minterpolation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mINTER_AREA\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    148\u001b[0m         \u001b[0mx_t\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresized_screen\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m18\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m102\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m~/envs/reinforcement_learning/lib/python3.6/site-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mshowtraceback\u001b[0;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[1;32m   2043\u001b[0m                         \u001b[0;31m# in the engines. This should return a list of strings.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2044\u001b[0;31m                         \u001b[0mstb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_render_traceback_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2045\u001b[0m                     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'KeyboardInterrupt' object has no attribute '_render_traceback_'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "\u001b[0;32m~/envs/reinforcement_learning/lib/python3.6/site-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mshowtraceback\u001b[0;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[1;32m   2045\u001b[0m                     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2046\u001b[0m                         stb = self.InteractiveTB.structured_traceback(etype,\n\u001b[0;32m-> 2047\u001b[0;31m                                             value, tb, tb_offset=tb_offset)\n\u001b[0m\u001b[1;32m   2048\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2049\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_showtraceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0metype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/envs/reinforcement_learning/lib/python3.6/site-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1434\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1435\u001b[0m         return FormattedTB.structured_traceback(\n\u001b[0;32m-> 1436\u001b[0;31m             self, etype, value, tb, tb_offset, number_of_lines_of_context)\n\u001b[0m\u001b[1;32m   1437\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1438\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/envs/reinforcement_learning/lib/python3.6/site-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1334\u001b[0m             \u001b[0;31m# Verbose modes need a full traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1335\u001b[0m             return VerboseTB.structured_traceback(\n\u001b[0;32m-> 1336\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0metype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtb_offset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumber_of_lines_of_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1337\u001b[0m             )\n\u001b[1;32m   1338\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'Minimal'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/envs/reinforcement_learning/lib/python3.6/site-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, evalue, etb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1191\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m         formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n\u001b[0;32m-> 1193\u001b[0;31m                                                                tb_offset)\n\u001b[0m\u001b[1;32m   1194\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1195\u001b[0m         \u001b[0mcolors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mColors\u001b[0m  \u001b[0;31m# just a shorthand + quicker name lookup\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/envs/reinforcement_learning/lib/python3.6/site-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mformat_exception_as_a_whole\u001b[0;34m(self, etype, evalue, etb, number_of_lines_of_context, tb_offset)\u001b[0m\n\u001b[1;32m   1148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1150\u001b[0;31m         \u001b[0mlast_unique\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecursion_repeat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind_recursion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morig_etype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1152\u001b[0m         \u001b[0mframes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat_records\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlast_unique\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecursion_repeat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/envs/reinforcement_learning/lib/python3.6/site-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mfind_recursion\u001b[0;34m(etype, value, records)\u001b[0m\n\u001b[1;32m    449\u001b[0m     \u001b[0;31m# first frame (from in to out) that looks different.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    450\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_recursion_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0metype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 451\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    452\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m     \u001b[0;31m# Select filename, lineno, func_name to track frames with\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: object of type 'NoneType' has no len()"
     ]
    }
   ],
   "source": [
    "engine = Engine(process_batch)\n",
    "setup_ignite(engine, params, exp_source, NAME)\n",
    "engine.run(batch_generator(buffer, params.replay_initial,\n",
    "                                  params.batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
