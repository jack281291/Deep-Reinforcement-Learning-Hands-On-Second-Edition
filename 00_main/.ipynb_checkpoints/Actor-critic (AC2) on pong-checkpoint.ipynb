{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Actor-critic (AC2) on pong"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../Chapter11/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import ptan\n",
    "import numpy as np\n",
    "import argparse\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.utils as nn_utils\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from lib import common"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start, as usual, by defining hyperparameters. These values\n",
    "are not tuned, as we will do this in the next section of this chapter. \n",
    "\n",
    "We have one new\n",
    "value here: CLIP_GRAD. This hyperparameter specifies the threshold for gradient\n",
    "clipping, which basically prevents our gradients from becoming too large at the\n",
    "optimization stage and pushing our policy too far. Clipping is implemented using\n",
    "the PyTorch functionality, but the idea is very simple: if the L2 norm of the gradient\n",
    "is larger than this hyperparameter, then the gradient vector is clipped to this value.\n",
    "\n",
    "The REWARD_STEPS hyperparameter determines how many steps ahead we will take\n",
    "to approximate the total discounted reward for every action.\n",
    "\n",
    "In the policy gradient methods, we used about 10 steps, but in A2C, we will use\n",
    "our value approximation to get a state value for further steps, so it will be fine to\n",
    "decrease the number of steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "GAMMA = 0.99\n",
    "LEARNING_RATE = 0.003\n",
    "ENTROPY_BETA = 0.03\n",
    "BATCH_SIZE = 32\n",
    "NUM_ENVS = 50\n",
    "\n",
    "REWARD_STEPS = 4\n",
    "CLIP_GRAD = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our network architecture has a shared convolution body and two heads: the first\n",
    "returns the policy with the probability distribution over our actions and the second\n",
    "head returns one single number, which will approximate the state's value. It might\n",
    "look similar to our dueling DQN architecture from Chapter 8, DQN Extensions, but\n",
    "our training procedure is different.\n",
    "\n",
    "The forward pass through the network returns a tuple of two tensors: policy\n",
    "and value. Now we have a large and important function, which takes the\n",
    "batch of environment transitions and returns three tensors: the batch of states,\n",
    "batch of actions taken, and batch of Q-values calculated using the formula ùëÑ(ùë†, ùëé) = Œ£ ùõæ^ùëñ * ùëü_ùëñ + ùõæ^ùëÅ * ùëâ(ùë†_ùëÅ). This Q-value will be used in two places: to calculate mean squared error (MSE) loss to improve the value approximation in the same way as DQN, and to calculate the advantage of the action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AtariA2C(nn.Module):\n",
    "    def __init__(self, input_shape, n_actions):\n",
    "        super(AtariA2C, self).__init__()\n",
    "\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(input_shape[0], 32, kernel_size=8, stride=4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        conv_out_size = self._get_conv_out(input_shape)\n",
    "        self.policy = nn.Sequential(\n",
    "            nn.Linear(conv_out_size, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, n_actions)\n",
    "        )\n",
    "\n",
    "        self.value = nn.Sequential(\n",
    "            nn.Linear(conv_out_size, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 1)\n",
    "        )\n",
    "\n",
    "    def _get_conv_out(self, shape):\n",
    "        o = self.conv(torch.zeros(1, *shape))\n",
    "        return int(np.prod(o.size()))\n",
    "\n",
    "    def forward(self, x):\n",
    "        fx = x.float() / 256\n",
    "        conv_out = self.conv(fx).view(fx.size()[0], -1)\n",
    "        return self.policy(conv_out), self.value(conv_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the first loop, we just walk through our batch of transitions and copy their fields\n",
    "into the lists. Note that the reward value already contains the discounted reward for\n",
    "REWARD_STEPS, as we use the ptan.ExperienceSourceFirstLast class. We also\n",
    "need to handle episode-ending situations and remember indices of batch entries for\n",
    "non-terminal episodes.\n",
    "\n",
    "In the preceding code, we convert the gathered state and actions into a PyTorch\n",
    "tensor and copy them into the graphics processing unit (GPU) if needed. The extra\n",
    "call to np.array() might look redundant, but without it, the performance of tensor\n",
    "creation degrades 5-10x. This issue in PyTorch (https://github.com/pytorch/\n",
    "pytorch/issues/13918) hasn't been solved yet, so one solution is to pass a single\n",
    "NumPy array instead of a list of arrays.\n",
    "\n",
    "The rest of the function calculates Q-values, taking into account the terminal\n",
    "episodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unpack_batch(batch, net, device='cpu'):\n",
    "    \"\"\"\n",
    "    Convert batch into training tensors\n",
    "    :param batch:\n",
    "    :param net:\n",
    "    :return: states variable, actions tensor, reference values variable\n",
    "    \"\"\"\n",
    "    states = []\n",
    "    actions = []\n",
    "    rewards = []\n",
    "    not_done_idx = []\n",
    "    last_states = []\n",
    "    for idx, exp in enumerate(batch):\n",
    "        states.append(np.array(exp.state, copy=False))\n",
    "        actions.append(int(exp.action))\n",
    "        rewards.append(exp.reward)\n",
    "        if exp.last_state is not None:\n",
    "            not_done_idx.append(idx)\n",
    "            last_states.append(np.array(exp.last_state, copy=False))\n",
    "\n",
    "    states_v = torch.FloatTensor(\n",
    "        np.array(states, copy=False)).to(device)\n",
    "    actions_t = torch.LongTensor(actions).to(device)\n",
    "\n",
    "    # handle rewards\n",
    "    rewards_np = np.array(rewards, dtype=np.float32)\n",
    "    if not_done_idx:\n",
    "        last_states_v = torch.FloatTensor(np.array(last_states, copy=False)).to(device)\n",
    "        last_vals_v = net(last_states_v)[1]\n",
    "        last_vals_np = last_vals_v.data.cpu().numpy()[:, 0]\n",
    "        last_vals_np *= GAMMA ** REWARD_STEPS\n",
    "        rewards_np[not_done_idx] += last_vals_np\n",
    "\n",
    "    ref_vals_v = torch.FloatTensor(rewards_np).to(device)\n",
    "\n",
    "    return states_v, actions_t, ref_vals_v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The preparation code for the training loop is the same as usual, except that we now\n",
    "use the array of environments to gather experience, instead of one environment.\n",
    "\n",
    "One very important detail here is passing the eps parameter to the optimizer. If\n",
    "you're familiar with the Adam algorithm, you may know that epsilon is a small\n",
    "number added to the denominator to prevent zero division situations. Normally,\n",
    "this value is set to some small number such as 1e-8 or 1e-10, but in our case, these\n",
    "values turned out to be too small. I have no mathematically strict explanation for\n",
    "this, but with the default value of epsilon, the method does not converge at all.\n",
    "Very likely, the division to a small value of 1e-8 makes the gradients too large,\n",
    "which turns out to be fatal for training stability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AtariA2C(\n",
      "  (conv): Sequential(\n",
      "    (0): Conv2d(4, 32, kernel_size=(8, 8), stride=(4, 4))\n",
      "    (1): ReLU()\n",
      "    (2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2))\n",
      "    (3): ReLU()\n",
      "    (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (5): ReLU()\n",
      "  )\n",
      "  (policy): Sequential(\n",
      "    (0): Linear(in_features=3136, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=6, bias=True)\n",
      "  )\n",
      "  (value): Sequential(\n",
      "    (0): Linear(in_features=3136, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "37517: done 1 games, mean reward -21.000, speed 367.91 f/s\n",
      "37570: done 2 games, mean reward -21.000, speed 333.56 f/s\n",
      "37651: done 3 games, mean reward -21.000, speed 386.47 f/s\n",
      "37682: done 4 games, mean reward -21.000, speed 318.37 f/s\n",
      "38555: done 5 games, mean reward -21.000, speed 372.31 f/s\n",
      "38566: done 6 games, mean reward -21.000, speed 174.43 f/s\n",
      "39030: done 7 games, mean reward -21.000, speed 371.48 f/s\n",
      "39225: done 8 games, mean reward -21.000, speed 366.53 f/s\n",
      "39235: done 9 games, mean reward -21.000, speed 166.43 f/s\n",
      "39288: done 10 games, mean reward -21.000, speed 385.49 f/s\n",
      "39923: done 11 games, mean reward -21.000, speed 362.27 f/s\n",
      "40249: done 12 games, mean reward -21.000, speed 362.34 f/s\n",
      "40585: done 13 games, mean reward -21.000, speed 350.48 f/s\n",
      "40758: done 14 games, mean reward -21.000, speed 349.44 f/s\n",
      "41448: done 15 games, mean reward -21.000, speed 346.31 f/s\n",
      "41499: done 16 games, mean reward -21.000, speed 398.90 f/s\n",
      "41535: done 17 games, mean reward -21.000, speed 346.13 f/s\n",
      "41591: done 18 games, mean reward -21.000, speed 326.80 f/s\n",
      "43109: done 19 games, mean reward -21.000, speed 370.14 f/s\n",
      "43743: done 20 games, mean reward -20.950, speed 374.22 f/s\n",
      "43916: done 21 games, mean reward -20.952, speed 344.61 f/s\n",
      "43982: done 22 games, mean reward -20.909, speed 346.81 f/s\n",
      "43991: done 23 games, mean reward -20.913, speed 354.07 f/s\n",
      "44554: done 24 games, mean reward -20.875, speed 364.11 f/s\n",
      "44630: done 25 games, mean reward -20.840, speed 380.14 f/s\n",
      "44724: done 26 games, mean reward -20.808, speed 344.50 f/s\n",
      "45108: done 27 games, mean reward -20.815, speed 365.21 f/s\n",
      "45156: done 28 games, mean reward -20.821, speed 324.21 f/s\n",
      "45414: done 29 games, mean reward -20.759, speed 363.44 f/s\n",
      "46121: done 30 games, mean reward -20.767, speed 362.71 f/s\n",
      "46141: done 31 games, mean reward -20.742, speed 435.82 f/s\n",
      "46447: done 32 games, mean reward -20.719, speed 345.01 f/s\n",
      "46545: done 33 games, mean reward -20.727, speed 364.06 f/s\n",
      "46582: done 34 games, mean reward -20.706, speed 347.81 f/s\n",
      "46643: done 35 games, mean reward -20.714, speed 337.05 f/s\n",
      "46812: done 36 games, mean reward -20.722, speed 352.35 f/s\n",
      "47269: done 37 games, mean reward -20.703, speed 357.51 f/s\n",
      "47461: done 38 games, mean reward -20.684, speed 371.28 f/s\n",
      "48113: done 39 games, mean reward -20.692, speed 370.88 f/s\n",
      "48439: done 40 games, mean reward -20.675, speed 353.01 f/s\n",
      "49551: done 41 games, mean reward -20.659, speed 365.93 f/s\n",
      "49641: done 42 games, mean reward -20.619, speed 344.36 f/s\n",
      "49868: done 43 games, mean reward -20.605, speed 358.87 f/s\n",
      "50042: done 44 games, mean reward -20.591, speed 371.01 f/s\n",
      "50403: done 45 games, mean reward -20.556, speed 351.37 f/s\n",
      "50872: done 46 games, mean reward -20.522, speed 356.38 f/s\n",
      "51590: done 47 games, mean reward -20.511, speed 365.70 f/s\n",
      "55644: done 48 games, mean reward -20.458, speed 369.62 f/s\n",
      "60156: done 49 games, mean reward -20.408, speed 372.92 f/s\n",
      "60204: done 50 games, mean reward -20.320, speed 296.83 f/s\n",
      "77224: done 51 games, mean reward -20.333, speed 373.62 f/s\n",
      "78557: done 52 games, mean reward -20.346, speed 374.01 f/s\n",
      "81485: done 53 games, mean reward -20.358, speed 372.38 f/s\n",
      "81915: done 54 games, mean reward -20.370, speed 385.75 f/s\n",
      "82100: done 55 games, mean reward -20.364, speed 370.02 f/s\n",
      "82137: done 56 games, mean reward -20.375, speed 347.34 f/s\n",
      "82627: done 57 games, mean reward -20.386, speed 360.62 f/s\n",
      "82772: done 58 games, mean reward -20.397, speed 382.68 f/s\n",
      "83202: done 59 games, mean reward -20.407, speed 368.62 f/s\n",
      "83417: done 60 games, mean reward -20.400, speed 382.43 f/s\n",
      "84037: done 61 games, mean reward -20.410, speed 373.71 f/s\n",
      "85278: done 62 games, mean reward -20.419, speed 372.03 f/s\n",
      "85410: done 63 games, mean reward -20.429, speed 342.84 f/s\n",
      "85460: done 64 games, mean reward -20.438, speed 396.57 f/s\n",
      "86175: done 65 games, mean reward -20.446, speed 375.27 f/s\n",
      "86202: done 66 games, mean reward -20.439, speed 298.35 f/s\n",
      "86673: done 67 games, mean reward -20.448, speed 371.45 f/s\n",
      "86886: done 68 games, mean reward -20.456, speed 360.80 f/s\n",
      "87576: done 69 games, mean reward -20.464, speed 378.51 f/s\n",
      "87879: done 70 games, mean reward -20.457, speed 359.59 f/s\n",
      "88654: done 71 games, mean reward -20.465, speed 374.46 f/s\n",
      "88872: done 72 games, mean reward -20.472, speed 370.94 f/s\n",
      "89462: done 73 games, mean reward -20.452, speed 375.80 f/s\n",
      "89493: done 74 games, mean reward -20.459, speed 341.73 f/s\n",
      "90114: done 75 games, mean reward -20.453, speed 366.59 f/s\n",
      "90282: done 76 games, mean reward -20.434, speed 368.23 f/s\n",
      "90811: done 77 games, mean reward -20.442, speed 371.75 f/s\n",
      "91463: done 78 games, mean reward -20.449, speed 366.79 f/s\n",
      "91612: done 79 games, mean reward -20.456, speed 368.81 f/s\n",
      "91796: done 80 games, mean reward -20.462, speed 355.28 f/s\n",
      "91847: done 81 games, mean reward -20.444, speed 316.39 f/s\n",
      "91902: done 82 games, mean reward -20.439, speed 399.43 f/s\n",
      "92519: done 83 games, mean reward -20.434, speed 362.06 f/s\n",
      "93003: done 84 games, mean reward -20.429, speed 358.94 f/s\n",
      "93880: done 85 games, mean reward -20.412, speed 365.81 f/s\n",
      "93922: done 86 games, mean reward -20.419, speed 288.31 f/s\n",
      "94083: done 87 games, mean reward -20.402, speed 354.88 f/s\n",
      "94218: done 88 games, mean reward -20.386, speed 369.18 f/s\n",
      "94323: done 89 games, mean reward -20.371, speed 367.16 f/s\n",
      "94646: done 90 games, mean reward -20.356, speed 374.31 f/s\n",
      "95838: done 91 games, mean reward -20.352, speed 379.46 f/s\n",
      "96259: done 92 games, mean reward -20.359, speed 363.20 f/s\n",
      "97542: done 93 games, mean reward -20.344, speed 377.92 f/s\n",
      "98844: done 94 games, mean reward -20.340, speed 372.32 f/s\n",
      "99190: done 95 games, mean reward -20.326, speed 369.47 f/s\n",
      "100641: done 96 games, mean reward -20.323, speed 366.64 f/s\n",
      "103356: done 97 games, mean reward -20.330, speed 367.36 f/s\n",
      "104651: done 98 games, mean reward -20.327, speed 371.61 f/s\n",
      "105000: done 99 games, mean reward -20.333, speed 356.14 f/s\n",
      "106684: done 100 games, mean reward -20.330, speed 373.35 f/s\n",
      "120799: done 101 games, mean reward -20.330, speed 377.20 f/s\n",
      "121333: done 102 games, mean reward -20.330, speed 371.39 f/s\n",
      "122702: done 103 games, mean reward -20.330, speed 378.36 f/s\n",
      "122915: done 104 games, mean reward -20.330, speed 359.88 f/s\n",
      "123527: done 105 games, mean reward -20.330, speed 367.09 f/s\n",
      "124124: done 106 games, mean reward -20.330, speed 380.25 f/s\n",
      "125407: done 107 games, mean reward -20.330, speed 381.64 f/s\n",
      "126258: done 108 games, mean reward -20.330, speed 370.80 f/s\n",
      "129148: done 109 games, mean reward -20.320, speed 378.23 f/s\n",
      "129481: done 110 games, mean reward -20.320, speed 360.39 f/s\n",
      "129674: done 111 games, mean reward -20.320, speed 349.72 f/s\n",
      "131326: done 112 games, mean reward -20.320, speed 367.01 f/s\n",
      "131887: done 113 games, mean reward -20.300, speed 374.58 f/s\n",
      "132255: done 114 games, mean reward -20.280, speed 379.98 f/s\n",
      "132460: done 115 games, mean reward -20.280, speed 356.71 f/s\n",
      "132817: done 116 games, mean reward -20.270, speed 375.58 f/s\n",
      "133229: done 117 games, mean reward -20.270, speed 375.32 f/s\n",
      "133365: done 118 games, mean reward -20.260, speed 373.17 f/s\n",
      "134146: done 119 games, mean reward -20.260, speed 371.88 f/s\n",
      "135028: done 120 games, mean reward -20.260, speed 375.03 f/s\n",
      "135535: done 121 games, mean reward -20.250, speed 372.87 f/s\n",
      "135862: done 122 games, mean reward -20.260, speed 377.03 f/s\n",
      "135997: done 123 games, mean reward -20.250, speed 375.50 f/s\n",
      "136519: done 124 games, mean reward -20.260, speed 368.37 f/s\n",
      "136645: done 125 games, mean reward -20.260, speed 371.28 f/s\n",
      "136725: done 126 games, mean reward -20.250, speed 385.54 f/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "137059: done 127 games, mean reward -20.250, speed 366.45 f/s\n",
      "137173: done 128 games, mean reward -20.240, speed 392.05 f/s\n",
      "137647: done 129 games, mean reward -20.260, speed 358.93 f/s\n",
      "137826: done 130 games, mean reward -20.250, speed 357.62 f/s\n",
      "137923: done 131 games, mean reward -20.230, speed 344.84 f/s\n",
      "138112: done 132 games, mean reward -20.230, speed 347.16 f/s\n",
      "139480: done 133 games, mean reward -20.220, speed 379.55 f/s\n",
      "139714: done 134 games, mean reward -20.220, speed 354.85 f/s\n",
      "139818: done 135 games, mean reward -20.220, speed 369.22 f/s\n",
      "140354: done 136 games, mean reward -20.210, speed 373.29 f/s\n",
      "140393: done 137 games, mean reward -20.200, speed 385.98 f/s\n",
      "140990: done 138 games, mean reward -20.200, speed 365.14 f/s\n",
      "141842: done 139 games, mean reward -20.190, speed 373.09 f/s\n",
      "142453: done 140 games, mean reward -20.180, speed 370.56 f/s\n",
      "144832: done 141 games, mean reward -20.180, speed 370.55 f/s\n",
      "144840: done 142 games, mean reward -20.180, speed 305.10 f/s\n",
      "145618: done 143 games, mean reward -20.180, speed 370.53 f/s\n",
      "148256: done 144 games, mean reward -20.180, speed 371.43 f/s\n",
      "148841: done 145 games, mean reward -20.190, speed 377.48 f/s\n",
      "149088: done 146 games, mean reward -20.200, speed 363.18 f/s\n",
      "150044: done 147 games, mean reward -20.200, speed 370.68 f/s\n",
      "153350: done 148 games, mean reward -20.220, speed 374.01 f/s\n",
      "154151: done 149 games, mean reward -20.240, speed 359.68 f/s\n",
      "154434: done 150 games, mean reward -20.290, speed 369.88 f/s\n",
      "165152: done 151 games, mean reward -20.290, speed 376.09 f/s\n",
      "168824: done 152 games, mean reward -20.280, speed 377.49 f/s\n",
      "170583: done 153 games, mean reward -20.260, speed 375.91 f/s\n",
      "171277: done 154 games, mean reward -20.250, speed 382.38 f/s\n",
      "172665: done 155 games, mean reward -20.260, speed 377.46 f/s\n",
      "174672: done 156 games, mean reward -20.260, speed 375.28 f/s\n",
      "174760: done 157 games, mean reward -20.240, speed 327.80 f/s\n",
      "175599: done 158 games, mean reward -20.210, speed 369.96 f/s\n",
      "178171: done 159 games, mean reward -20.210, speed 373.79 f/s\n",
      "178312: done 160 games, mean reward -20.210, speed 339.96 f/s\n",
      "178759: done 161 games, mean reward -20.200, speed 368.65 f/s\n",
      "178866: done 162 games, mean reward -20.200, speed 358.84 f/s\n",
      "179079: done 163 games, mean reward -20.190, speed 349.64 f/s\n",
      "179487: done 164 games, mean reward -20.180, speed 386.55 f/s\n",
      "179717: done 165 games, mean reward -20.180, speed 355.66 f/s\n",
      "180273: done 166 games, mean reward -20.170, speed 379.88 f/s\n",
      "180628: done 167 games, mean reward -20.160, speed 378.01 f/s\n",
      "180649: done 168 games, mean reward -20.150, speed 293.70 f/s\n",
      "180670: done 169 games, mean reward -20.150, speed 424.92 f/s\n",
      "180718: done 170 games, mean reward -20.140, speed 303.61 f/s\n",
      "181143: done 171 games, mean reward -20.130, speed 368.88 f/s\n",
      "181265: done 172 games, mean reward -20.120, speed 343.41 f/s\n",
      "181286: done 173 games, mean reward -20.140, speed 263.82 f/s\n",
      "181393: done 174 games, mean reward -20.140, speed 365.06 f/s\n",
      "181488: done 175 games, mean reward -20.140, speed 342.69 f/s\n",
      "181747: done 176 games, mean reward -20.150, speed 369.27 f/s\n",
      "182804: done 177 games, mean reward -20.150, speed 360.79 f/s\n",
      "183125: done 178 games, mean reward -20.150, speed 353.25 f/s\n",
      "183681: done 179 games, mean reward -20.130, speed 370.46 f/s\n",
      "183946: done 180 games, mean reward -20.130, speed 379.86 f/s\n",
      "184211: done 181 games, mean reward -20.140, speed 373.18 f/s\n",
      "185698: done 182 games, mean reward -20.130, speed 378.79 f/s\n",
      "186006: done 183 games, mean reward -20.140, speed 379.83 f/s\n",
      "186020: done 184 games, mean reward -20.140, speed 211.58 f/s\n",
      "187076: done 185 games, mean reward -20.130, speed 381.32 f/s\n",
      "187156: done 186 games, mean reward -20.120, speed 386.43 f/s\n",
      "187224: done 187 games, mean reward -20.130, speed 351.94 f/s\n",
      "187393: done 188 games, mean reward -20.150, speed 351.52 f/s\n",
      "188868: done 189 games, mean reward -20.170, speed 382.26 f/s\n",
      "190394: done 190 games, mean reward -20.190, speed 371.47 f/s\n",
      "190540: done 191 games, mean reward -20.190, speed 351.25 f/s\n",
      "191300: done 192 games, mean reward -20.190, speed 375.77 f/s\n",
      "191920: done 193 games, mean reward -20.190, speed 375.63 f/s\n",
      "192882: done 194 games, mean reward -20.190, speed 373.85 f/s\n",
      "193290: done 195 games, mean reward -20.210, speed 353.13 f/s\n",
      "194041: done 196 games, mean reward -20.220, speed 362.11 f/s\n",
      "194584: done 197 games, mean reward -20.220, speed 372.70 f/s\n",
      "195201: done 198 games, mean reward -20.230, speed 371.84 f/s\n",
      "195555: done 199 games, mean reward -20.190, speed 377.11 f/s\n",
      "209136: done 200 games, mean reward -20.170, speed 378.15 f/s\n",
      "210565: done 201 games, mean reward -20.170, speed 376.53 f/s\n",
      "213872: done 202 games, mean reward -20.170, speed 371.27 f/s\n",
      "214833: done 203 games, mean reward -20.170, speed 375.84 f/s\n",
      "214902: done 204 games, mean reward -20.170, speed 366.20 f/s\n",
      "215402: done 205 games, mean reward -20.160, speed 370.96 f/s\n",
      "218507: done 206 games, mean reward -20.160, speed 373.01 f/s\n",
      "218871: done 207 games, mean reward -20.160, speed 366.07 f/s\n",
      "220062: done 208 games, mean reward -20.160, speed 364.91 f/s\n",
      "220130: done 209 games, mean reward -20.160, speed 311.60 f/s\n",
      "220854: done 210 games, mean reward -20.160, speed 381.73 f/s\n",
      "223674: done 211 games, mean reward -20.140, speed 374.37 f/s\n",
      "223742: done 212 games, mean reward -20.140, speed 346.11 f/s\n",
      "223924: done 213 games, mean reward -20.150, speed 358.74 f/s\n",
      "223998: done 214 games, mean reward -20.170, speed 384.00 f/s\n",
      "224186: done 215 games, mean reward -20.160, speed 356.11 f/s\n",
      "224558: done 216 games, mean reward -20.170, speed 370.08 f/s\n",
      "224793: done 217 games, mean reward -20.170, speed 377.05 f/s\n",
      "224977: done 218 games, mean reward -20.180, speed 359.36 f/s\n",
      "226017: done 219 games, mean reward -20.170, speed 371.06 f/s\n",
      "227029: done 220 games, mean reward -20.170, speed 377.88 f/s\n",
      "227466: done 221 games, mean reward -20.170, speed 355.11 f/s\n",
      "227491: done 222 games, mean reward -20.160, speed 277.59 f/s\n",
      "228578: done 223 games, mean reward -20.170, speed 361.53 f/s\n",
      "228614: done 224 games, mean reward -20.150, speed 331.76 f/s\n",
      "228914: done 225 games, mean reward -20.160, speed 358.44 f/s\n",
      "228957: done 226 games, mean reward -20.180, speed 370.73 f/s\n",
      "229397: done 227 games, mean reward -20.180, speed 348.41 f/s\n",
      "229549: done 228 games, mean reward -20.190, speed 356.52 f/s\n",
      "230263: done 229 games, mean reward -20.180, speed 372.33 f/s\n",
      "230420: done 230 games, mean reward -20.180, speed 358.72 f/s\n",
      "230581: done 231 games, mean reward -20.210, speed 369.79 f/s\n",
      "231218: done 232 games, mean reward -20.220, speed 370.94 f/s\n",
      "231263: done 233 games, mean reward -20.200, speed 360.98 f/s\n",
      "231500: done 234 games, mean reward -20.210, speed 341.48 f/s\n",
      "232231: done 235 games, mean reward -20.210, speed 354.14 f/s\n",
      "233696: done 236 games, mean reward -20.200, speed 360.86 f/s\n",
      "234090: done 237 games, mean reward -20.220, speed 357.22 f/s\n",
      "234245: done 238 games, mean reward -20.230, speed 341.57 f/s\n",
      "234856: done 239 games, mean reward -20.220, speed 376.75 f/s\n",
      "235338: done 240 games, mean reward -20.230, speed 376.88 f/s\n",
      "237575: done 241 games, mean reward -20.220, speed 367.72 f/s\n",
      "237595: done 242 games, mean reward -20.240, speed 441.32 f/s\n",
      "237772: done 243 games, mean reward -20.230, speed 353.02 f/s\n",
      "237886: done 244 games, mean reward -20.240, speed 375.82 f/s\n",
      "239301: done 245 games, mean reward -20.250, speed 379.58 f/s\n",
      "239364: done 246 games, mean reward -20.250, speed 346.19 f/s\n",
      "244455: done 247 games, mean reward -20.240, speed 375.23 f/s\n",
      "244594: done 248 games, mean reward -20.230, speed 379.73 f/s\n",
      "245882: done 249 games, mean reward -20.230, speed 378.75 f/s\n",
      "253115: done 250 games, mean reward -20.230, speed 377.25 f/s\n",
      "256272: done 251 games, mean reward -20.230, speed 376.95 f/s\n",
      "256586: done 252 games, mean reward -20.220, speed 367.37 f/s\n",
      "259233: done 253 games, mean reward -20.230, speed 370.97 f/s\n",
      "261574: done 254 games, mean reward -20.240, speed 380.76 f/s\n",
      "261862: done 255 games, mean reward -20.240, speed 349.00 f/s\n",
      "262985: done 256 games, mean reward -20.240, speed 373.81 f/s\n",
      "263649: done 257 games, mean reward -20.250, speed 373.49 f/s\n",
      "264657: done 258 games, mean reward -20.270, speed 368.16 f/s\n",
      "264804: done 259 games, mean reward -20.260, speed 358.50 f/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "265077: done 260 games, mean reward -20.270, speed 381.56 f/s\n",
      "265654: done 261 games, mean reward -20.270, speed 367.66 f/s\n",
      "266021: done 262 games, mean reward -20.270, speed 372.53 f/s\n",
      "266098: done 263 games, mean reward -20.280, speed 386.11 f/s\n",
      "266543: done 264 games, mean reward -20.290, speed 359.00 f/s\n",
      "267758: done 265 games, mean reward -20.290, speed 374.33 f/s\n",
      "267793: done 266 games, mean reward -20.310, speed 380.37 f/s\n",
      "267835: done 267 games, mean reward -20.320, speed 374.03 f/s\n",
      "270273: done 268 games, mean reward -20.320, speed 376.45 f/s\n",
      "271000: done 269 games, mean reward -20.320, speed 372.91 f/s\n",
      "272416: done 270 games, mean reward -20.340, speed 367.20 f/s\n",
      "273288: done 271 games, mean reward -20.350, speed 360.33 f/s\n",
      "273613: done 272 games, mean reward -20.360, speed 354.11 f/s\n",
      "273931: done 273 games, mean reward -20.350, speed 351.47 f/s\n",
      "274069: done 274 games, mean reward -20.330, speed 348.58 f/s\n",
      "274298: done 275 games, mean reward -20.330, speed 356.08 f/s\n",
      "275119: done 276 games, mean reward -20.340, speed 371.37 f/s\n",
      "275179: done 277 games, mean reward -20.320, speed 325.40 f/s\n",
      "276518: done 278 games, mean reward -20.320, speed 368.05 f/s\n",
      "277609: done 279 games, mean reward -20.320, speed 378.54 f/s\n",
      "277804: done 280 games, mean reward -20.310, speed 372.30 f/s\n",
      "278379: done 281 games, mean reward -20.300, speed 373.90 f/s\n",
      "279296: done 282 games, mean reward -20.320, speed 361.73 f/s\n",
      "279448: done 283 games, mean reward -20.310, speed 373.07 f/s\n",
      "279675: done 284 games, mean reward -20.320, speed 368.18 f/s\n",
      "279740: done 285 games, mean reward -20.340, speed 361.43 f/s\n",
      "280094: done 286 games, mean reward -20.340, speed 358.47 f/s\n",
      "281660: done 287 games, mean reward -20.330, speed 374.48 f/s\n",
      "282341: done 288 games, mean reward -20.320, speed 371.62 f/s\n",
      "282670: done 289 games, mean reward -20.310, speed 374.66 f/s\n",
      "284064: done 290 games, mean reward -20.300, speed 378.62 f/s\n",
      "285034: done 291 games, mean reward -20.300, speed 380.55 f/s\n",
      "286505: done 292 games, mean reward -20.300, speed 375.62 f/s\n",
      "286780: done 293 games, mean reward -20.300, speed 379.37 f/s\n",
      "287961: done 294 games, mean reward -20.310, speed 372.84 f/s\n",
      "288590: done 295 games, mean reward -20.290, speed 373.90 f/s\n",
      "289501: done 296 games, mean reward -20.280, speed 376.62 f/s\n",
      "291306: done 297 games, mean reward -20.260, speed 374.32 f/s\n",
      "293892: done 298 games, mean reward -20.260, speed 375.23 f/s\n",
      "294115: done 299 games, mean reward -20.300, speed 342.80 f/s\n",
      "296832: done 300 games, mean reward -20.320, speed 377.37 f/s\n",
      "304577: done 301 games, mean reward -20.320, speed 373.76 f/s\n",
      "305508: done 302 games, mean reward -20.320, speed 355.89 f/s\n",
      "305605: done 303 games, mean reward -20.320, speed 349.85 f/s\n",
      "307443: done 304 games, mean reward -20.320, speed 368.38 f/s\n",
      "310095: done 305 games, mean reward -20.320, speed 375.11 f/s\n",
      "310307: done 306 games, mean reward -20.310, speed 372.31 f/s\n",
      "310437: done 307 games, mean reward -20.300, speed 383.03 f/s\n",
      "311312: done 308 games, mean reward -20.290, speed 372.49 f/s\n",
      "312886: done 309 games, mean reward -20.290, speed 372.61 f/s\n",
      "313023: done 310 games, mean reward -20.290, speed 345.95 f/s\n",
      "313173: done 311 games, mean reward -20.290, speed 349.74 f/s\n",
      "314449: done 312 games, mean reward -20.280, speed 382.71 f/s\n",
      "314565: done 313 games, mean reward -20.290, speed 343.19 f/s\n",
      "314604: done 314 games, mean reward -20.290, speed 386.92 f/s\n",
      "315339: done 315 games, mean reward -20.290, speed 356.99 f/s\n",
      "315524: done 316 games, mean reward -20.290, speed 369.53 f/s\n",
      "316304: done 317 games, mean reward -20.270, speed 371.46 f/s\n",
      "316969: done 318 games, mean reward -20.270, speed 385.83 f/s\n",
      "318424: done 319 games, mean reward -20.280, speed 382.69 f/s\n",
      "318499: done 320 games, mean reward -20.290, speed 314.58 f/s\n",
      "318975: done 321 games, mean reward -20.300, speed 363.84 f/s\n",
      "319331: done 322 games, mean reward -20.310, speed 351.48 f/s\n",
      "319337: done 323 games, mean reward -20.290, speed 319.14 f/s\n",
      "319628: done 324 games, mean reward -20.300, speed 360.06 f/s\n",
      "320953: done 325 games, mean reward -20.300, speed 380.94 f/s\n",
      "321053: done 326 games, mean reward -20.290, speed 366.25 f/s\n",
      "321121: done 327 games, mean reward -20.290, speed 305.86 f/s\n",
      "321729: done 328 games, mean reward -20.290, speed 370.96 f/s\n",
      "321861: done 329 games, mean reward -20.290, speed 354.00 f/s\n",
      "322144: done 330 games, mean reward -20.300, speed 361.81 f/s\n",
      "323914: done 331 games, mean reward -20.300, speed 377.92 f/s\n",
      "324176: done 332 games, mean reward -20.290, speed 354.09 f/s\n",
      "325738: done 333 games, mean reward -20.310, speed 371.14 f/s\n",
      "326468: done 334 games, mean reward -20.310, speed 383.98 f/s\n",
      "326697: done 335 games, mean reward -20.300, speed 367.07 f/s\n",
      "326822: done 336 games, mean reward -20.320, speed 364.77 f/s\n",
      "326943: done 337 games, mean reward -20.310, speed 370.91 f/s\n",
      "328405: done 338 games, mean reward -20.310, speed 379.37 f/s\n",
      "330630: done 339 games, mean reward -20.330, speed 366.17 f/s\n",
      "330719: done 340 games, mean reward -20.320, speed 389.17 f/s\n",
      "331687: done 341 games, mean reward -20.320, speed 368.43 f/s\n",
      "331714: done 342 games, mean reward -20.320, speed 293.03 f/s\n",
      "331902: done 343 games, mean reward -20.340, speed 385.95 f/s\n",
      "331919: done 344 games, mean reward -20.340, speed 234.59 f/s\n",
      "333206: done 345 games, mean reward -20.340, speed 381.18 f/s\n",
      "337684: done 346 games, mean reward -20.330, speed 377.97 f/s\n",
      "338232: done 347 games, mean reward -20.350, speed 384.07 f/s\n",
      "338461: done 348 games, mean reward -20.350, speed 369.85 f/s\n",
      "340792: done 349 games, mean reward -20.360, speed 372.24 f/s\n",
      "348890: done 350 games, mean reward -20.350, speed 378.49 f/s\n",
      "350508: done 351 games, mean reward -20.350, speed 375.66 f/s\n",
      "350743: done 352 games, mean reward -20.360, speed 374.33 f/s\n",
      "352499: done 353 games, mean reward -20.370, speed 372.17 f/s\n",
      "353677: done 354 games, mean reward -20.360, speed 372.18 f/s\n",
      "354945: done 355 games, mean reward -20.350, speed 363.92 f/s\n",
      "355026: done 356 games, mean reward -20.350, speed 354.74 f/s\n",
      "356471: done 357 games, mean reward -20.350, speed 372.84 f/s\n",
      "356702: done 358 games, mean reward -20.340, speed 374.90 f/s\n",
      "356876: done 359 games, mean reward -20.350, speed 352.49 f/s\n",
      "357098: done 360 games, mean reward -20.350, speed 351.59 f/s\n",
      "357284: done 361 games, mean reward -20.360, speed 350.62 f/s\n",
      "358285: done 362 games, mean reward -20.350, speed 381.21 f/s\n",
      "358807: done 363 games, mean reward -20.340, speed 365.28 f/s\n",
      "359053: done 364 games, mean reward -20.340, speed 368.23 f/s\n",
      "359474: done 365 games, mean reward -20.340, speed 355.35 f/s\n",
      "359986: done 366 games, mean reward -20.330, speed 372.80 f/s\n",
      "360559: done 367 games, mean reward -20.330, speed 373.03 f/s\n",
      "360615: done 368 games, mean reward -20.340, speed 327.92 f/s\n",
      "360972: done 369 games, mean reward -20.330, speed 372.03 f/s\n",
      "361531: done 370 games, mean reward -20.330, speed 360.72 f/s\n",
      "362354: done 371 games, mean reward -20.320, speed 375.80 f/s\n",
      "362540: done 372 games, mean reward -20.310, speed 372.33 f/s\n",
      "362746: done 373 games, mean reward -20.320, speed 379.47 f/s\n",
      "363269: done 374 games, mean reward -20.330, speed 372.83 f/s\n",
      "364500: done 375 games, mean reward -20.340, speed 384.08 f/s\n",
      "364764: done 376 games, mean reward -20.340, speed 375.27 f/s\n",
      "364844: done 377 games, mean reward -20.360, speed 342.07 f/s\n",
      "365113: done 378 games, mean reward -20.340, speed 373.46 f/s\n",
      "365246: done 379 games, mean reward -20.360, speed 374.05 f/s\n",
      "365876: done 380 games, mean reward -20.360, speed 371.21 f/s\n",
      "366317: done 381 games, mean reward -20.380, speed 363.25 f/s\n",
      "368028: done 382 games, mean reward -20.380, speed 376.05 f/s\n",
      "368238: done 383 games, mean reward -20.390, speed 363.61 f/s\n",
      "368431: done 384 games, mean reward -20.390, speed 370.57 f/s\n",
      "370097: done 385 games, mean reward -20.400, speed 374.04 f/s\n",
      "370158: done 386 games, mean reward -20.410, speed 354.02 f/s\n",
      "370570: done 387 games, mean reward -20.430, speed 363.29 f/s\n",
      "372229: done 388 games, mean reward -20.440, speed 379.47 f/s\n",
      "373410: done 389 games, mean reward -20.440, speed 377.06 f/s\n",
      "375365: done 390 games, mean reward -20.440, speed 375.85 f/s\n",
      "376751: done 391 games, mean reward -20.450, speed 369.60 f/s\n",
      "380268: done 392 games, mean reward -20.440, speed 374.27 f/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "380408: done 393 games, mean reward -20.450, speed 356.70 f/s\n",
      "382816: done 394 games, mean reward -20.440, speed 373.50 f/s\n",
      "384011: done 395 games, mean reward -20.450, speed 371.35 f/s\n",
      "385037: done 396 games, mean reward -20.430, speed 377.89 f/s\n",
      "386034: done 397 games, mean reward -20.440, speed 382.25 f/s\n",
      "386790: done 398 games, mean reward -20.440, speed 373.24 f/s\n",
      "391727: done 399 games, mean reward -20.440, speed 372.70 f/s\n",
      "391942: done 400 games, mean reward -20.430, speed 363.76 f/s\n",
      "393299: done 401 games, mean reward -20.430, speed 378.50 f/s\n",
      "393723: done 402 games, mean reward -20.430, speed 376.03 f/s\n",
      "393909: done 403 games, mean reward -20.430, speed 357.65 f/s\n",
      "395643: done 404 games, mean reward -20.430, speed 378.39 f/s\n",
      "396607: done 405 games, mean reward -20.440, speed 369.55 f/s\n",
      "397225: done 406 games, mean reward -20.450, speed 360.00 f/s\n",
      "398772: done 407 games, mean reward -20.460, speed 371.78 f/s\n",
      "399403: done 408 games, mean reward -20.470, speed 366.94 f/s\n",
      "401602: done 409 games, mean reward -20.470, speed 370.35 f/s\n",
      "401790: done 410 games, mean reward -20.470, speed 384.52 f/s\n",
      "402236: done 411 games, mean reward -20.490, speed 369.40 f/s\n",
      "402334: done 412 games, mean reward -20.500, speed 354.39 f/s\n",
      "402388: done 413 games, mean reward -20.490, speed 318.10 f/s\n",
      "402662: done 414 games, mean reward -20.490, speed 348.61 f/s\n",
      "403082: done 415 games, mean reward -20.470, speed 351.23 f/s\n",
      "403187: done 416 games, mean reward -20.470, speed 378.62 f/s\n",
      "403496: done 417 games, mean reward -20.490, speed 359.94 f/s\n",
      "404371: done 418 games, mean reward -20.480, speed 372.34 f/s\n",
      "404462: done 419 games, mean reward -20.480, speed 336.35 f/s\n",
      "406354: done 420 games, mean reward -20.480, speed 366.91 f/s\n",
      "406546: done 421 games, mean reward -20.470, speed 373.36 f/s\n",
      "407548: done 422 games, mean reward -20.460, speed 377.48 f/s\n",
      "408713: done 423 games, mean reward -20.480, speed 371.09 f/s\n",
      "409038: done 424 games, mean reward -20.490, speed 368.80 f/s\n",
      "409474: done 425 games, mean reward -20.490, speed 348.35 f/s\n",
      "411144: done 426 games, mean reward -20.490, speed 371.61 f/s\n",
      "412300: done 427 games, mean reward -20.480, speed 366.08 f/s\n",
      "412492: done 428 games, mean reward -20.480, speed 360.84 f/s\n",
      "412628: done 429 games, mean reward -20.470, speed 362.35 f/s\n",
      "413729: done 430 games, mean reward -20.460, speed 350.30 f/s\n",
      "414078: done 431 games, mean reward -20.460, speed 380.20 f/s\n",
      "414207: done 432 games, mean reward -20.470, speed 337.23 f/s\n",
      "416060: done 433 games, mean reward -20.480, speed 364.49 f/s\n",
      "416068: done 434 games, mean reward -20.470, speed 147.49 f/s\n",
      "416920: done 435 games, mean reward -20.470, speed 372.24 f/s\n",
      "418915: done 436 games, mean reward -20.470, speed 364.38 f/s\n",
      "420769: done 437 games, mean reward -20.450, speed 367.18 f/s\n",
      "421717: done 438 games, mean reward -20.420, speed 364.66 f/s\n",
      "422180: done 439 games, mean reward -20.410, speed 363.58 f/s\n",
      "423018: done 440 games, mean reward -20.420, speed 365.58 f/s\n",
      "423247: done 441 games, mean reward -20.440, speed 338.33 f/s\n",
      "425111: done 442 games, mean reward -20.440, speed 369.68 f/s\n",
      "425307: done 443 games, mean reward -20.440, speed 346.29 f/s\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\")\n",
    "\n",
    "make_env = lambda: ptan.common.wrappers.wrap_dqn(gym.make(\"PongNoFrameskip-v4\"))\n",
    "envs = [make_env() for _ in range(NUM_ENVS)]\n",
    "writer = SummaryWriter(comment=\"-pong-a2c_\" + \"test\")\n",
    "\n",
    "net = AtariA2C(envs[0].observation_space.shape, envs[0].action_space.n).to(device)\n",
    "print(net)\n",
    "\n",
    "agent = ptan.agent.PolicyAgent(lambda x: net(x)[0], apply_softmax=True, device=device)\n",
    "exp_source = ptan.experience.ExperienceSourceFirstLast(envs, agent, gamma=GAMMA, steps_count=REWARD_STEPS)\n",
    "\n",
    "optimizer = optim.Adam(net.parameters(), lr=LEARNING_RATE, eps=1e-3)\n",
    "\n",
    "batch = []\n",
    "\n",
    "with common.RewardTracker(writer, stop_reward=18) as tracker:\n",
    "    with ptan.common.utils.TBMeanTracker(writer, batch_size=10) as tb_tracker:\n",
    "        for step_idx, exp in enumerate(exp_source):\n",
    "            batch.append(exp)\n",
    "\n",
    "            # handle new rewards\n",
    "            new_rewards = exp_source.pop_total_rewards()\n",
    "            if new_rewards:\n",
    "                if tracker.reward(new_rewards[0], step_idx):\n",
    "                    break\n",
    "\n",
    "            if len(batch) < BATCH_SIZE:\n",
    "                continue\n",
    "\n",
    "            states_v, actions_t, vals_ref_v = unpack_batch(batch, net, device=device)\n",
    "            batch.clear()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            logits_v, value_v = net(states_v)\n",
    "            loss_value_v = F.mse_loss(value_v.squeeze(-1), vals_ref_v)\n",
    "\n",
    "            log_prob_v = F.log_softmax(logits_v, dim=1)\n",
    "            adv_v = vals_ref_v - value_v.detach()\n",
    "            log_prob_actions_v = adv_v * log_prob_v[range(BATCH_SIZE), actions_t]\n",
    "            loss_policy_v = -log_prob_actions_v.mean()\n",
    "\n",
    "            prob_v = F.softmax(logits_v, dim=1)\n",
    "            entropy_loss_v = ENTROPY_BETA * (prob_v * log_prob_v).sum(dim=1).mean()\n",
    "\n",
    "            # calculate policy gradients only\n",
    "            loss_policy_v.backward(retain_graph=True)\n",
    "            grads = np.concatenate([p.grad.data.cpu().numpy().flatten()\n",
    "                                    for p in net.parameters()\n",
    "                                    if p.grad is not None])\n",
    "\n",
    "            # apply entropy and value gradients\n",
    "            loss_v = entropy_loss_v + loss_value_v\n",
    "            loss_v.backward()\n",
    "            nn_utils.clip_grad_norm_(net.parameters(), CLIP_GRAD)\n",
    "            optimizer.step()\n",
    "            # get full loss\n",
    "            loss_v += loss_policy_v\n",
    "\n",
    "            tb_tracker.track(\"advantage\",       adv_v, step_idx)\n",
    "            tb_tracker.track(\"values\",          value_v, step_idx)\n",
    "            tb_tracker.track(\"batch_rewards\",   vals_ref_v, step_idx)\n",
    "            tb_tracker.track(\"loss_entropy\",    entropy_loss_v, step_idx)\n",
    "            tb_tracker.track(\"loss_policy\",     loss_policy_v, step_idx)\n",
    "            tb_tracker.track(\"loss_value\",      loss_value_v, step_idx)\n",
    "            tb_tracker.track(\"loss_total\",      loss_v, step_idx)\n",
    "            tb_tracker.track(\"grad_l2\",         np.sqrt(np.mean(np.square(grads))), step_idx)\n",
    "            tb_tracker.track(\"grad_max\",        np.max(np.abs(grads)), step_idx)\n",
    "            tb_tracker.track(\"grad_var\",        np.var(grads), step_idx)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
