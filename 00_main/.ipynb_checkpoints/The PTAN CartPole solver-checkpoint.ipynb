{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The PTAN CartPole solver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import ptan\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "HIDDEN_SIZE = 128\n",
    "BATCH_SIZE = 16\n",
    "TGT_NET_SYNC = 10\n",
    "GAMMA = 0.9\n",
    "REPLAY_SIZE = 1000\n",
    "LR = 1e-3\n",
    "EPS_DECAY=0.99\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, obs_size, hidden_size, n_actions):\n",
    "        super(Net, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(obs_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, n_actions)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x.float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def unpack_batch(batch, net, gamma):\n",
    "    states = []\n",
    "    actions = []\n",
    "    rewards = []\n",
    "    done_masks = []\n",
    "    last_states = []\n",
    "    for exp in batch:\n",
    "        states.append(exp.state)\n",
    "        actions.append(exp.action)\n",
    "        rewards.append(exp.reward)\n",
    "        done_masks.append(exp.last_state is None)\n",
    "        if exp.last_state is None:\n",
    "            last_states.append(exp.state)\n",
    "        else:\n",
    "            last_states.append(exp.last_state)\n",
    "\n",
    "    states_v = torch.tensor(states)\n",
    "    actions_v = torch.tensor(actions)\n",
    "    rewards_v = torch.tensor(rewards)\n",
    "    last_states_v = torch.tensor(last_states)\n",
    "    last_state_q_v = net(last_states_v)\n",
    "    best_last_q_v = torch.max(last_state_q_v, dim=1)[0]\n",
    "    best_last_q_v[done_masks] = 0.0\n",
    "    return states_v, actions_v, best_last_q_v * gamma + rewards_v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v0\")\n",
    "obs_size = env.observation_space.shape[0]\n",
    "n_actions = env.action_space.n\n",
    "\n",
    "net = Net(obs_size, HIDDEN_SIZE, n_actions)\n",
    "tgt_net = ptan.agent.TargetNet(net)\n",
    "selector = ptan.actions.ArgmaxActionSelector()\n",
    "selector = ptan.actions.EpsilonGreedyActionSelector(\n",
    "    epsilon=1, selector=selector)\n",
    "agent = ptan.agent.DQNAgent(net, selector)\n",
    "exp_source = ptan.experience.ExperienceSourceFirstLast(\n",
    "    env, agent, gamma=GAMMA)\n",
    "buffer = ptan.experience.ExperienceReplayBuffer(\n",
    "    exp_source, buffer_size=REPLAY_SIZE)\n",
    "optimizer = optim.Adam(net.parameters(), LR)\n",
    "\n",
    "step = 0\n",
    "episode = 0\n",
    "solved = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the beginning, we create the NN (the simple two-layer feed-forward NN that\n",
    "we used for CartPole before) and target the NN epsilon-greedy action selector and\n",
    "DQNAgent. Then the experience source and replay buffer are created. With those few\n",
    "lines, we have finished with our data pipeline. Now we just need to call populate()\n",
    "on the buffer and sample training batches from it.\n",
    "\n",
    "In the beginning of every training loop iteration, we ask the buffer to fetch one\n",
    "sample from the experience source and then check for the finished episode. The\n",
    "method pop_rewards_steps() in the ExperienceSource class returns the list of\n",
    "tuples with information about episodes completed since the last call to the method.\n",
    "\n",
    "Later in the training loop, we convert a batch of ExperienceFirstLast objects into\n",
    "tensors suitable for DQN training, calculate the loss, and do a backpropagation\n",
    "step. Finally, we decay epsilon in our action selector (with the hyperparameters\n",
    "used, epsilon decays to zero at training step 500) and ask the target network to sync\n",
    "every 10 training iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14: episode 1 done, reward=13.000, epsilon=1.00\n",
      "30: episode 2 done, reward=16.000, epsilon=1.00\n",
      "55: episode 3 done, reward=25.000, epsilon=0.79\n",
      "67: episode 4 done, reward=12.000, epsilon=0.70\n",
      "80: episode 5 done, reward=13.000, epsilon=0.62\n",
      "91: episode 6 done, reward=11.000, epsilon=0.55\n",
      "104: episode 7 done, reward=13.000, epsilon=0.48\n",
      "116: episode 8 done, reward=12.000, epsilon=0.43\n",
      "140: episode 9 done, reward=24.000, epsilon=0.34\n",
      "153: episode 10 done, reward=13.000, epsilon=0.30\n",
      "164: episode 11 done, reward=11.000, epsilon=0.27\n",
      "174: episode 12 done, reward=10.000, epsilon=0.24\n",
      "185: episode 13 done, reward=11.000, epsilon=0.21\n",
      "194: episode 14 done, reward=9.000, epsilon=0.20\n",
      "203: episode 15 done, reward=9.000, epsilon=0.18\n",
      "213: episode 16 done, reward=10.000, epsilon=0.16\n",
      "221: episode 17 done, reward=8.000, epsilon=0.15\n",
      "229: episode 18 done, reward=8.000, epsilon=0.14\n",
      "237: episode 19 done, reward=8.000, epsilon=0.13\n",
      "250: episode 20 done, reward=13.000, epsilon=0.11\n",
      "259: episode 21 done, reward=9.000, epsilon=0.10\n",
      "269: episode 22 done, reward=10.000, epsilon=0.09\n",
      "282: episode 23 done, reward=13.000, epsilon=0.08\n",
      "293: episode 24 done, reward=11.000, epsilon=0.07\n",
      "301: episode 25 done, reward=8.000, epsilon=0.07\n",
      "310: episode 26 done, reward=9.000, epsilon=0.06\n",
      "319: episode 27 done, reward=9.000, epsilon=0.06\n",
      "328: episode 28 done, reward=9.000, epsilon=0.05\n",
      "336: episode 29 done, reward=8.000, epsilon=0.05\n",
      "344: episode 30 done, reward=8.000, epsilon=0.04\n",
      "357: episode 31 done, reward=13.000, epsilon=0.04\n",
      "371: episode 32 done, reward=14.000, epsilon=0.03\n",
      "386: episode 33 done, reward=15.000, epsilon=0.03\n",
      "400: episode 34 done, reward=14.000, epsilon=0.02\n",
      "411: episode 35 done, reward=11.000, epsilon=0.02\n",
      "425: episode 36 done, reward=14.000, epsilon=0.02\n",
      "439: episode 37 done, reward=14.000, epsilon=0.02\n",
      "458: episode 38 done, reward=19.000, epsilon=0.01\n",
      "482: episode 39 done, reward=24.000, epsilon=0.01\n",
      "516: episode 40 done, reward=34.000, epsilon=0.01\n",
      "533: episode 41 done, reward=17.000, epsilon=0.01\n",
      "543: episode 42 done, reward=10.000, epsilon=0.01\n",
      "553: episode 43 done, reward=10.000, epsilon=0.01\n",
      "561: episode 44 done, reward=8.000, epsilon=0.00\n",
      "570: episode 45 done, reward=9.000, epsilon=0.00\n",
      "598: episode 46 done, reward=28.000, epsilon=0.00\n",
      "618: episode 47 done, reward=20.000, epsilon=0.00\n",
      "650: episode 48 done, reward=32.000, epsilon=0.00\n",
      "713: episode 49 done, reward=63.000, epsilon=0.00\n",
      "725: episode 50 done, reward=12.000, epsilon=0.00\n",
      "770: episode 51 done, reward=45.000, epsilon=0.00\n",
      "809: episode 52 done, reward=39.000, epsilon=0.00\n",
      "854: episode 53 done, reward=45.000, epsilon=0.00\n",
      "932: episode 54 done, reward=78.000, epsilon=0.00\n",
      "972: episode 55 done, reward=40.000, epsilon=0.00\n",
      "1006: episode 56 done, reward=34.000, epsilon=0.00\n",
      "1045: episode 57 done, reward=39.000, epsilon=0.00\n",
      "1169: episode 58 done, reward=124.000, epsilon=0.00\n",
      "1284: episode 59 done, reward=115.000, epsilon=0.00\n",
      "1320: episode 60 done, reward=36.000, epsilon=0.00\n",
      "1369: episode 61 done, reward=49.000, epsilon=0.00\n",
      "1398: episode 62 done, reward=29.000, epsilon=0.00\n",
      "1419: episode 63 done, reward=21.000, epsilon=0.00\n",
      "1456: episode 64 done, reward=37.000, epsilon=0.00\n",
      "1488: episode 65 done, reward=32.000, epsilon=0.00\n",
      "1504: episode 66 done, reward=16.000, epsilon=0.00\n",
      "1524: episode 67 done, reward=20.000, epsilon=0.00\n",
      "1575: episode 68 done, reward=51.000, epsilon=0.00\n",
      "1628: episode 69 done, reward=53.000, epsilon=0.00\n",
      "1698: episode 70 done, reward=70.000, epsilon=0.00\n",
      "1795: episode 71 done, reward=97.000, epsilon=0.00\n",
      "1843: episode 72 done, reward=48.000, epsilon=0.00\n",
      "1879: episode 73 done, reward=36.000, epsilon=0.00\n",
      "1927: episode 74 done, reward=48.000, epsilon=0.00\n",
      "1962: episode 75 done, reward=35.000, epsilon=0.00\n",
      "1994: episode 76 done, reward=32.000, epsilon=0.00\n",
      "2015: episode 77 done, reward=21.000, epsilon=0.00\n",
      "2045: episode 78 done, reward=30.000, epsilon=0.00\n",
      "2087: episode 79 done, reward=42.000, epsilon=0.00\n",
      "2126: episode 80 done, reward=39.000, epsilon=0.00\n",
      "2148: episode 81 done, reward=22.000, epsilon=0.00\n",
      "2177: episode 82 done, reward=29.000, epsilon=0.00\n",
      "2226: episode 83 done, reward=49.000, epsilon=0.00\n",
      "2267: episode 84 done, reward=41.000, epsilon=0.00\n",
      "2313: episode 85 done, reward=46.000, epsilon=0.00\n",
      "2348: episode 86 done, reward=35.000, epsilon=0.00\n",
      "2378: episode 87 done, reward=30.000, epsilon=0.00\n",
      "2403: episode 88 done, reward=25.000, epsilon=0.00\n",
      "2436: episode 89 done, reward=33.000, epsilon=0.00\n",
      "2456: episode 90 done, reward=20.000, epsilon=0.00\n",
      "2489: episode 91 done, reward=33.000, epsilon=0.00\n",
      "2555: episode 92 done, reward=66.000, epsilon=0.00\n",
      "2597: episode 93 done, reward=42.000, epsilon=0.00\n",
      "2631: episode 94 done, reward=34.000, epsilon=0.00\n",
      "2655: episode 95 done, reward=24.000, epsilon=0.00\n",
      "2685: episode 96 done, reward=30.000, epsilon=0.00\n",
      "2722: episode 97 done, reward=37.000, epsilon=0.00\n",
      "2768: episode 98 done, reward=46.000, epsilon=0.00\n",
      "2801: episode 99 done, reward=33.000, epsilon=0.00\n",
      "2835: episode 100 done, reward=34.000, epsilon=0.00\n",
      "2870: episode 101 done, reward=35.000, epsilon=0.00\n",
      "2899: episode 102 done, reward=29.000, epsilon=0.00\n",
      "2925: episode 103 done, reward=26.000, epsilon=0.00\n",
      "2965: episode 104 done, reward=40.000, epsilon=0.00\n",
      "3007: episode 105 done, reward=42.000, epsilon=0.00\n",
      "3040: episode 106 done, reward=33.000, epsilon=0.00\n",
      "3083: episode 107 done, reward=43.000, epsilon=0.00\n",
      "3132: episode 108 done, reward=49.000, epsilon=0.00\n",
      "3176: episode 109 done, reward=44.000, epsilon=0.00\n",
      "3209: episode 110 done, reward=33.000, epsilon=0.00\n",
      "3235: episode 111 done, reward=26.000, epsilon=0.00\n",
      "3274: episode 112 done, reward=39.000, epsilon=0.00\n",
      "3312: episode 113 done, reward=38.000, epsilon=0.00\n",
      "3357: episode 114 done, reward=45.000, epsilon=0.00\n",
      "3402: episode 115 done, reward=45.000, epsilon=0.00\n",
      "3457: episode 116 done, reward=55.000, epsilon=0.00\n",
      "3501: episode 117 done, reward=44.000, epsilon=0.00\n",
      "3533: episode 118 done, reward=32.000, epsilon=0.00\n",
      "3573: episode 119 done, reward=40.000, epsilon=0.00\n",
      "3619: episode 120 done, reward=46.000, epsilon=0.00\n",
      "3671: episode 121 done, reward=52.000, epsilon=0.00\n",
      "3735: episode 122 done, reward=64.000, epsilon=0.00\n",
      "3786: episode 123 done, reward=51.000, epsilon=0.00\n",
      "3855: episode 124 done, reward=69.000, epsilon=0.00\n",
      "3931: episode 125 done, reward=76.000, epsilon=0.00\n",
      "4025: episode 126 done, reward=94.000, epsilon=0.00\n",
      "4099: episode 127 done, reward=74.000, epsilon=0.00\n",
      "4202: episode 128 done, reward=103.000, epsilon=0.00\n",
      "4402: episode 129 done, reward=200.000, epsilon=0.00\n",
      "Congrats!\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    step += 1\n",
    "    buffer.populate(1)\n",
    "\n",
    "    for reward, steps in exp_source.pop_rewards_steps():\n",
    "        episode += 1\n",
    "        print(\"%d: episode %d done, reward=%.3f, epsilon=%.2f\" % (\n",
    "            step, episode, reward, selector.epsilon))\n",
    "        solved = reward > 150\n",
    "    if solved:\n",
    "        print(\"Congrats!\")\n",
    "        break\n",
    "\n",
    "    if len(buffer) < 2*BATCH_SIZE:\n",
    "        continue\n",
    "\n",
    "    batch = buffer.sample(BATCH_SIZE)\n",
    "    states_v, actions_v, tgt_q_v = unpack_batch(\n",
    "        batch, tgt_net.target_model, GAMMA)\n",
    "    optimizer.zero_grad()\n",
    "    q_v = net(states_v)\n",
    "    q_v = q_v.gather(1, actions_v.unsqueeze(-1)).squeeze(-1)\n",
    "    loss_v = F.mse_loss(q_v, tgt_q_v)\n",
    "    loss_v.backward()\n",
    "    optimizer.step()\n",
    "    selector.epsilon *= EPS_DECAY\n",
    "\n",
    "    if step % TGT_NET_SYNC == 0:\n",
    "        tgt_net.sync()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "??torch.gather"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
