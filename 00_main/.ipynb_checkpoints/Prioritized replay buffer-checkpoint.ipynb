{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prioritized replay buffer\n",
    "\n",
    "To implement this method, we have to introduce several changes in our code.\n",
    "First of all, we need a new replay buffer that will track priorities, sample a batch\n",
    "according to them, calculate weights, and let us update priorities after the loss has\n",
    "become known. The second change will be the loss function itself. Now we not only\n",
    "need to incorporate weights for every sample, but we need to pass loss values back\n",
    "to the replay buffer to adjust the priorities of the sampled transitions.\n",
    "In the example file Chapter08/05_dqn_prio_replay.py, we have all those changes\n",
    "implemented. For the sake of simplicity, the new priority replay buffer class uses\n",
    "a very similar storage scheme to our previous replay buffer. Unfortunately, new\n",
    "requirements for prioritization make it impossible to implement sampling in O(1)\n",
    "time to buffer size. If we are using simple lists, every time that we sample a new\n",
    "batch, we need to process all the priorities, which makes our sampling have O(N)\n",
    "time complexity in proportion to the buffer size. It's not a big deal if our buffer is\n",
    "small, such as 100k samples, but may become an issue for real-life large buffers\n",
    "of millions of transitions. There are other storage schemes that support efficient\n",
    "sampling in O(log N) time, for example, using the segment tree data structure. You\n",
    "can find such implementation in the OpenAI Baselines project: https://github.com/openai/baselines. The PTAN library also provides an efficient prioritized\n",
    "replay buffer in the class ptan.experience.PrioritizedReplayBuffer. You can\n",
    "update the example to use the more efficient version and check the effect on training\n",
    "performance.\n",
    "But, for now, let's take a look at the na√Øve version, whose source code is in lib/dqn_extra.py.\n",
    "\n",
    "``` python\n",
    "class PrioReplayBuffer:\n",
    "    def __init__(self, exp_source, buf_size, prob_alpha=0.6):\n",
    "        self.exp_source_iter = iter(exp_source)\n",
    "        self.prob_alpha = prob_alpha\n",
    "        self.capacity = buf_size\n",
    "        self.pos = 0\n",
    "        self.buffer = []\n",
    "        self.priorities = np.zeros(\n",
    "            (buf_size, ), dtype=np.float32)\n",
    "        self.beta = BETA_START\n",
    "\n",
    "    def update_beta(self, idx):\n",
    "        v = BETA_START + idx * (1.0 - BETA_START) / \\\n",
    "            BETA_FRAMES\n",
    "        self.beta = min(1.0, v)\n",
    "        return self.beta\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "    def populate(self, count):\n",
    "        max_prio = self.priorities.max() if \\\n",
    "            self.buffer else 1.0\n",
    "        for _ in range(count):\n",
    "            sample = next(self.exp_source_iter)\n",
    "            if len(self.buffer) < self.capacity:\n",
    "                self.buffer.append(sample)\n",
    "            else:\n",
    "                self.buffer[self.pos] = sample\n",
    "            self.priorities[self.pos] = max_prio\n",
    "            self.pos = (self.pos + 1) % self.capacity\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        if len(self.buffer) == self.capacity:\n",
    "            prios = self.priorities\n",
    "        else:\n",
    "            prios = self.priorities[:self.pos]\n",
    "        probs = prios ** self.prob_alpha\n",
    "\n",
    "        probs /= probs.sum()\n",
    "        indices = np.random.choice(len(self.buffer),\n",
    "                                   batch_size, p=probs)\n",
    "        samples = [self.buffer[idx] for idx in indices]\n",
    "        total = len(self.buffer)\n",
    "        weights = (total * probs[indices]) ** (-self.beta)\n",
    "        weights /= weights.max()\n",
    "        return samples, indices, \\\n",
    "               np.array(weights, dtype=np.float32)\n",
    "\n",
    "    def update_priorities(self, batch_indices,\n",
    "                          batch_priorities):\n",
    "        for idx, prio in zip(batch_indices,\n",
    "                             batch_priorities):\n",
    "            self.priorities[idx] = prio\n",
    "```\n",
    "\n",
    "In the beginning, we define parameters for the ùõΩ increase rate. Our beta will be changed\n",
    "from 0.4 to 1.0 during the first 100k frames.\n",
    "\n",
    "The class for the priority replay buffer stores samples in a circular buffer (it allows\n",
    "us to keep a fixed amount of entries without reallocating the list) and a NumPy array\n",
    "to keep priorities. We also store the iterator to the experience source object to pull\n",
    "the samples from the environment.\n",
    "\n",
    "The populate() method needs to pull the given number of transitions from the\n",
    "ExperienceSource object and store them in the buffer. As our storage for the\n",
    "transitions is implemented as a circular buffer, we have two different situations\n",
    "with this buffer:\n",
    "- When our buffer hasn't reached the maximum capacity, we just need to\n",
    "append a new transition to the buffer.\n",
    "- If the buffer is already full, we need to overwrite the oldest transition, which\n",
    "is tracked by the pos class field, and adjust this position modulo buffer's size.\n",
    "The method update_beta needs to be called periodically to increase beta according\n",
    "to schedule.\n",
    "\n",
    "In the sample method, we need to convert priorities to probabilities using our ùõºùõº\n",
    "hyperparameter.\n",
    "\n",
    "Then, using those probabilities, we sample our buffer to obtain a batch of samples.\n",
    "\n",
    "As the last step, we calculate weights for samples in the batch and return three\n",
    "objects: the batch, indices, and weights. Indices for batch samples are required to\n",
    "update priorities for sampled items.\n",
    "\n",
    "The last function of the priority replay buffer allows us to update new priorities for\n",
    "the processed batch. It's the responsibility of the caller to use this function with the\n",
    "calculated losses for the batch.\n",
    "The next custom function that we have in our example is the loss calculation. As\n",
    "the MSELoss class in PyTorch doesn't support weights (which is understandable, as\n",
    "MSE is loss used in regression problems, but weighting of the samples is commonly\n",
    "utilized in classification losses), we need to calculate the MSE and explicitly multiply\n",
    "the result on the weights:\n",
    "\n",
    "``` python\n",
    "def calc_loss(batch, batch_weights, net, tgt_net,\n",
    "              gamma, device=\"cpu\"):\n",
    "    states, actions, rewards, dones, next_states = \\\n",
    "        common.unpack_batch(batch)\n",
    "\n",
    "    states_v = torch.tensor(states).to(device)\n",
    "    actions_v = torch.tensor(actions).to(device)\n",
    "    rewards_v = torch.tensor(rewards).to(device)\n",
    "    done_mask = torch.BoolTensor(dones).to(device)\n",
    "    batch_weights_v = torch.tensor(batch_weights).to(device)\n",
    "\n",
    "    actions_v = actions_v.unsqueeze(-1)\n",
    "    state_action_vals = net(states_v).gather(1, actions_v)\n",
    "    state_action_vals = state_action_vals.squeeze(-1)\n",
    "    with torch.no_grad():\n",
    "        next_states_v = torch.tensor(next_states).to(device)\n",
    "        next_s_vals = tgt_net(next_states_v).max(1)[0]\n",
    "        next_s_vals[done_mask] = 0.0\n",
    "        exp_sa_vals = next_s_vals.detach() * gamma + rewards_v\n",
    "    l = (state_action_vals - exp_sa_vals) ** 2\n",
    "    losses_v = batch_weights_v * l\n",
    "    return losses_v.mean(), \\\n",
    "           (losses_v + 1e-5).data.cpu().numpy()\n",
    "```\n",
    "\n",
    "In the last part of the loss calculation, we implement the same MSE loss but write\n",
    "our expression explicitly, rather than using the library. This allows us to take into\n",
    "account the weights of samples and keep individual loss values for every sample.\n",
    "Those values will be passed to the priority replay buffer to update priorities. A small\n",
    "value is added to every loss to handle the situation of zero loss value, which will lead\n",
    "to zero priority for an entry in the replay buffer.\n",
    "\n",
    "In the main section of the utility, we have only two updates: the creation of the\n",
    "replay buffer and our processing function. Buffer creation is straightforward,\n",
    "so we will take a look at only a new processing function, there are several changes here:\n",
    "\n",
    "- Our batch now contains three entities: the batch of data, indices of sampled\n",
    "items, and samples' weights.\n",
    "- We call our new loss function, which accepts weights and returns the\n",
    "additional items' priorities. They are passed to the buffer.update_\n",
    "priorities function to reprioritize items that we have sampled.\n",
    "- We call the update_beta method of the buffer to change the beta parameter\n",
    "according to schedule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../Chapter08/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1: reward=-21, steps=908, speed=0.0 f/s, elapsed=0:00:27\n",
      "Episode 2: reward=-19, steps=1130, speed=0.0 f/s, elapsed=0:00:27\n",
      "Episode 3: reward=-20, steps=969, speed=0.0 f/s, elapsed=0:00:27\n",
      "Episode 4: reward=-20, steps=1064, speed=0.0 f/s, elapsed=0:00:27\n",
      "Episode 5: reward=-21, steps=838, speed=0.0 f/s, elapsed=0:00:27\n",
      "Episode 6: reward=-20, steps=955, speed=0.0 f/s, elapsed=0:00:27\n",
      "Episode 7: reward=-20, steps=894, speed=0.0 f/s, elapsed=0:00:27\n",
      "Episode 8: reward=-21, steps=780, speed=0.0 f/s, elapsed=0:00:27\n",
      "Episode 9: reward=-19, steps=976, speed=0.0 f/s, elapsed=0:00:27\n",
      "Episode 10: reward=-20, steps=925, speed=0.0 f/s, elapsed=0:00:27\n",
      "Episode 11: reward=-21, steps=882, speed=61.1 f/s, elapsed=0:00:32\n",
      "Episode 12: reward=-20, steps=935, speed=61.1 f/s, elapsed=0:00:48\n",
      "Episode 13: reward=-20, steps=1020, speed=61.1 f/s, elapsed=0:01:04\n",
      "Episode 14: reward=-19, steps=993, speed=61.1 f/s, elapsed=0:01:21\n",
      "Episode 15: reward=-21, steps=897, speed=61.1 f/s, elapsed=0:01:36\n",
      "Episode 16: reward=-21, steps=851, speed=61.1 f/s, elapsed=0:01:50\n",
      "Episode 17: reward=-21, steps=761, speed=61.1 f/s, elapsed=0:02:02\n",
      "Episode 18: reward=-21, steps=785, speed=61.0 f/s, elapsed=0:02:15\n",
      "Episode 19: reward=-20, steps=868, speed=61.0 f/s, elapsed=0:02:30\n",
      "Episode 20: reward=-21, steps=846, speed=61.0 f/s, elapsed=0:02:44\n",
      "Episode 21: reward=-20, steps=942, speed=60.9 f/s, elapsed=0:03:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import ptan\n",
    "import argparse\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "\n",
    "from ignite.engine import Engine\n",
    "\n",
    "from lib import dqn_model, common, dqn_extra\n",
    "\n",
    "NAME = \"05_prio_replay\"\n",
    "PRIO_REPLAY_ALPHA = 0.6\n",
    "\n",
    "\n",
    "def calc_loss(batch, batch_weights, net, tgt_net,\n",
    "              gamma, device=\"cpu\"):\n",
    "    states, actions, rewards, dones, next_states = \\\n",
    "        common.unpack_batch(batch)\n",
    "\n",
    "    states_v = torch.tensor(states).to(device)\n",
    "    actions_v = torch.tensor(actions).to(device)\n",
    "    rewards_v = torch.tensor(rewards).to(device)\n",
    "    done_mask = torch.BoolTensor(dones).to(device)\n",
    "    batch_weights_v = torch.tensor(batch_weights).to(device)\n",
    "\n",
    "    actions_v = actions_v.unsqueeze(-1)\n",
    "    state_action_vals = net(states_v).gather(1, actions_v)\n",
    "    state_action_vals = state_action_vals.squeeze(-1)\n",
    "    with torch.no_grad():\n",
    "        next_states_v = torch.tensor(next_states).to(device)\n",
    "        next_s_vals = tgt_net(next_states_v).max(1)[0]\n",
    "        next_s_vals[done_mask] = 0.0\n",
    "        exp_sa_vals = next_s_vals.detach() * gamma + rewards_v\n",
    "    l = (state_action_vals - exp_sa_vals) ** 2\n",
    "    losses_v = batch_weights_v * l\n",
    "    return losses_v.mean(), \\\n",
    "           (losses_v + 1e-5).data.cpu().numpy()\n",
    "\n",
    "\n",
    "random.seed(common.SEED)\n",
    "torch.manual_seed(common.SEED)\n",
    "params = common.HYPERPARAMS['pong']\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "env = gym.make(params.env_name)\n",
    "env = ptan.common.wrappers.wrap_dqn(env)\n",
    "env.seed(common.SEED)\n",
    "\n",
    "net = dqn_model.DQN(env.observation_space.shape, env.action_space.n).to(device)\n",
    "\n",
    "tgt_net = ptan.agent.TargetNet(net)\n",
    "selector = ptan.actions.EpsilonGreedyActionSelector(epsilon=params.epsilon_start)\n",
    "epsilon_tracker = common.EpsilonTracker(selector, params)\n",
    "agent = ptan.agent.DQNAgent(net, selector, device=device)\n",
    "\n",
    "exp_source = ptan.experience.ExperienceSourceFirstLast(\n",
    "    env, agent, gamma=params.gamma)\n",
    "buffer = dqn_extra.PrioReplayBuffer(\n",
    "    exp_source, params.replay_size, PRIO_REPLAY_ALPHA)\n",
    "optimizer = optim.Adam(net.parameters(), lr=params.learning_rate)\n",
    "\n",
    "def process_batch(engine, batch_data):\n",
    "    batch, batch_indices, batch_weights = batch_data\n",
    "    optimizer.zero_grad()\n",
    "    loss_v, sample_prios = calc_loss(\n",
    "        batch, batch_weights, net, tgt_net.target_model,\n",
    "        gamma=params.gamma, device=device)\n",
    "    loss_v.backward()\n",
    "    optimizer.step()\n",
    "    buffer.update_priorities(batch_indices, sample_prios)\n",
    "    epsilon_tracker.frame(engine.state.iteration)\n",
    "    if engine.state.iteration % params.target_net_sync == 0:\n",
    "        tgt_net.sync()\n",
    "    return {\n",
    "        \"loss\": loss_v.item(),\n",
    "        \"epsilon\": selector.epsilon,\n",
    "        \"beta\": buffer.update_beta(engine.state.iteration),\n",
    "    }\n",
    "\n",
    "engine = Engine(process_batch)\n",
    "common.setup_ignite(engine, params, exp_source, NAME)\n",
    "engine.run(common.batch_generator(buffer, params.replay_initial, params.batch_size))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
