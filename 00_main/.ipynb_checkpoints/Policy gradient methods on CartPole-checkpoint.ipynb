{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy gradient methods on CartPole"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import ptan\n",
    "import numpy as np\n",
    "from tensorboardX import SummaryWriter\n",
    "from typing import Optional\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Besides the already familiar hyperparameters, we have two new ones: the ENTROPY_\n",
    "BETA value is the scale of the entropy bonus and the REWARD_STEPS value specifies\n",
    "how many steps ahead the Bellman equation is unrolled to estimate the discounted\n",
    "total reward of every transition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "GAMMA = 0.99\n",
    "LEARNING_RATE = 0.001\n",
    "ENTROPY_BETA = 0.01\n",
    "BATCH_SIZE = 8\n",
    "\n",
    "REWARD_STEPS = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The network architecture is exactly the same as in the previous examples for\n",
    "CartPole: a two-layer network with 128 neurons in the hidden layer. The preparation\n",
    "code is also the same as before, except the experience source is asked to unroll\n",
    "the Bellman equation for 10 steps.\n",
    "The following is the part that differs from 04_cartpole_pg.py:\n",
    "\n",
    "``` python\n",
    "exp_source = ptan.experience.ExperienceSourceFirstLast(\n",
    "env, agent, gamma=GAMMA, steps_count=REWARD_STEPS)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PGN(nn.Module):\n",
    "    def __init__(self, input_size, n_actions):\n",
    "        super(PGN, self).__init__()\n",
    "\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_size, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, n_actions)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smooth(old: Optional[float], val: float, alpha: float = 0.95) -> float:\n",
    "    if old is None:\n",
    "        return val\n",
    "    return old * alpha + (1-alpha)*val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the loss calculation, we use the same code as before to calculate the policy loss\n",
    "(which is the negated policy gradient):\n",
    "\n",
    "``` python\n",
    "optimizer.zero_grad()\n",
    "logits_v = net(states_v)\n",
    "log_prob_v = F.log_softmax(logits_v, dim=1)\n",
    "log_prob_actions_v = batch_scale_v * log_prob_v[range(BATCH_SIZE),batch_actions_t]\n",
    "loss_policy_v = -log_prob_actions_v.mean()\n",
    "```\n",
    "\n",
    "Then we add the entropy bonus to the loss by calculating the entropy of the batch\n",
    "and subtracting it from the loss. As entropy has a maximum for uniform probability\n",
    "distribution and we want to push the training toward this maximum, we need to\n",
    "subtract from the loss.\n",
    "\n",
    "``` python\n",
    "prob_v = F.softmax(logits_v, dim=1)\n",
    "entropy_v = -(prob_v * log_prob_v).sum(dim=1).mean()\n",
    "entropy_loss_v = -ENTROPY_BETA * entropy_v\n",
    "loss_v = loss_policy_v + entropy_loss_v\n",
    "loss_v.backward()\n",
    "optimizer.step()\n",
    "```\n",
    "\n",
    "Then, we calculate the Kullback-Leibler (KL) divergence between the new policy\n",
    "and the old policy. KL divergence is an information theory measurement of how one\n",
    "probability distribution diverges from another expected probability distribution. In\n",
    "our example, it is being used to compare the policy returned by the model before and\n",
    "after the optimization step. High spikes in KL are usually a bad sign, showing that\n",
    "our policy was pushed too far from the previous policy, which is a bad idea most of\n",
    "the time (as our NN is a very nonlinear function in a high-dimensional space, such\n",
    "large changes in the model weight could have a very strong influence on the policy).\n",
    "\n",
    "``` python\n",
    "new_logits_v = net(states_v)\n",
    "new_prob_v = F.softmax(new_logits_v, dim=1)\n",
    "kl_div_v = -((new_prob_v / prob_v).log() *\n",
    "prob_v).sum(dim=1).mean()\n",
    "writer.add_scalar(\"kl\", kl_div_v.item(), step_idx)\n",
    "```\n",
    "\n",
    "Finally, we calculate the statistics about the gradients on this training step. It's usually good practice to show the graph of the maximum and L2 norm of gradients to get an idea about the training dynamics.\n",
    "\n",
    "``` python\n",
    "grad_max = 0.0\n",
    "grad_means = 0.0\n",
    "grad_count = 0\n",
    "for p in net.parameters():\n",
    "grad_max = max(grad_max, p.grad.abs().max().item())\n",
    "grad_means += (p.grad ** 2).mean().sqrt().item()\n",
    "grad_count += 1\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PGN(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=4, out_features=128, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=128, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "38: reward:  38.00, mean_100:  38.00, episodes: 1\n",
      "70: reward:  32.00, mean_100:  35.00, episodes: 2\n",
      "140: reward:  70.00, mean_100:  46.67, episodes: 3\n",
      "216: reward:  76.00, mean_100:  54.00, episodes: 4\n",
      "239: reward:  23.00, mean_100:  47.80, episodes: 5\n",
      "255: reward:  16.00, mean_100:  42.50, episodes: 6\n",
      "283: reward:  28.00, mean_100:  40.43, episodes: 7\n",
      "302: reward:  19.00, mean_100:  37.75, episodes: 8\n",
      "318: reward:  16.00, mean_100:  35.33, episodes: 9\n",
      "354: reward:  36.00, mean_100:  35.40, episodes: 10\n",
      "367: reward:  13.00, mean_100:  33.36, episodes: 11\n",
      "386: reward:  19.00, mean_100:  32.17, episodes: 12\n",
      "426: reward:  40.00, mean_100:  32.77, episodes: 13\n",
      "437: reward:  11.00, mean_100:  31.21, episodes: 14\n",
      "498: reward:  61.00, mean_100:  33.20, episodes: 15\n",
      "542: reward:  44.00, mean_100:  33.88, episodes: 16\n",
      "588: reward:  46.00, mean_100:  34.59, episodes: 17\n",
      "617: reward:  29.00, mean_100:  34.28, episodes: 18\n",
      "653: reward:  36.00, mean_100:  34.37, episodes: 19\n",
      "708: reward:  55.00, mean_100:  35.40, episodes: 20\n",
      "734: reward:  26.00, mean_100:  34.95, episodes: 21\n",
      "747: reward:  13.00, mean_100:  33.95, episodes: 22\n",
      "780: reward:  33.00, mean_100:  33.91, episodes: 23\n",
      "830: reward:  50.00, mean_100:  34.58, episodes: 24\n",
      "845: reward:  15.00, mean_100:  33.80, episodes: 25\n",
      "877: reward:  32.00, mean_100:  33.73, episodes: 26\n",
      "927: reward:  50.00, mean_100:  34.33, episodes: 27\n",
      "962: reward:  35.00, mean_100:  34.36, episodes: 28\n",
      "1009: reward:  47.00, mean_100:  34.79, episodes: 29\n",
      "1034: reward:  25.00, mean_100:  34.47, episodes: 30\n",
      "1060: reward:  26.00, mean_100:  34.19, episodes: 31\n",
      "1118: reward:  58.00, mean_100:  34.94, episodes: 32\n",
      "1135: reward:  17.00, mean_100:  34.39, episodes: 33\n",
      "1169: reward:  34.00, mean_100:  34.38, episodes: 34\n",
      "1188: reward:  19.00, mean_100:  33.94, episodes: 35\n",
      "1217: reward:  29.00, mean_100:  33.81, episodes: 36\n",
      "1248: reward:  31.00, mean_100:  33.73, episodes: 37\n",
      "1271: reward:  23.00, mean_100:  33.45, episodes: 38\n",
      "1297: reward:  26.00, mean_100:  33.26, episodes: 39\n",
      "1357: reward:  60.00, mean_100:  33.92, episodes: 40\n",
      "1385: reward:  28.00, mean_100:  33.78, episodes: 41\n",
      "1402: reward:  17.00, mean_100:  33.38, episodes: 42\n",
      "1434: reward:  32.00, mean_100:  33.35, episodes: 43\n",
      "1455: reward:  21.00, mean_100:  33.07, episodes: 44\n",
      "1477: reward:  22.00, mean_100:  32.82, episodes: 45\n",
      "1532: reward:  55.00, mean_100:  33.30, episodes: 46\n",
      "1564: reward:  32.00, mean_100:  33.28, episodes: 47\n",
      "1619: reward:  55.00, mean_100:  33.73, episodes: 48\n",
      "1650: reward:  31.00, mean_100:  33.67, episodes: 49\n",
      "1671: reward:  21.00, mean_100:  33.42, episodes: 50\n",
      "1715: reward:  44.00, mean_100:  33.63, episodes: 51\n",
      "1752: reward:  37.00, mean_100:  33.69, episodes: 52\n",
      "1792: reward:  40.00, mean_100:  33.81, episodes: 53\n",
      "1840: reward:  48.00, mean_100:  34.07, episodes: 54\n",
      "1925: reward:  85.00, mean_100:  35.00, episodes: 55\n",
      "1956: reward:  31.00, mean_100:  34.93, episodes: 56\n",
      "1997: reward:  41.00, mean_100:  35.04, episodes: 57\n",
      "2018: reward:  21.00, mean_100:  34.79, episodes: 58\n",
      "2057: reward:  39.00, mean_100:  34.86, episodes: 59\n",
      "2095: reward:  38.00, mean_100:  34.92, episodes: 60\n",
      "2174: reward:  79.00, mean_100:  35.64, episodes: 61\n",
      "2227: reward:  53.00, mean_100:  35.92, episodes: 62\n",
      "2301: reward:  74.00, mean_100:  36.52, episodes: 63\n",
      "2347: reward:  46.00, mean_100:  36.67, episodes: 64\n",
      "2375: reward:  28.00, mean_100:  36.54, episodes: 65\n",
      "2417: reward:  42.00, mean_100:  36.62, episodes: 66\n",
      "2439: reward:  22.00, mean_100:  36.40, episodes: 67\n",
      "2521: reward:  82.00, mean_100:  37.07, episodes: 68\n",
      "2538: reward:  17.00, mean_100:  36.78, episodes: 69\n",
      "2619: reward:  81.00, mean_100:  37.41, episodes: 70\n",
      "2740: reward: 121.00, mean_100:  38.59, episodes: 71\n",
      "2775: reward:  35.00, mean_100:  38.54, episodes: 72\n",
      "2812: reward:  37.00, mean_100:  38.52, episodes: 73\n",
      "2925: reward: 113.00, mean_100:  39.53, episodes: 74\n",
      "3055: reward: 130.00, mean_100:  40.73, episodes: 75\n",
      "3083: reward:  28.00, mean_100:  40.57, episodes: 76\n",
      "3103: reward:  20.00, mean_100:  40.30, episodes: 77\n",
      "3223: reward: 120.00, mean_100:  41.32, episodes: 78\n",
      "3324: reward: 101.00, mean_100:  42.08, episodes: 79\n",
      "3353: reward:  29.00, mean_100:  41.91, episodes: 80\n",
      "3468: reward: 115.00, mean_100:  42.81, episodes: 81\n",
      "3613: reward: 145.00, mean_100:  44.06, episodes: 82\n",
      "3650: reward:  37.00, mean_100:  43.98, episodes: 83\n",
      "3771: reward: 121.00, mean_100:  44.89, episodes: 84\n",
      "3890: reward: 119.00, mean_100:  45.76, episodes: 85\n",
      "3929: reward:  39.00, mean_100:  45.69, episodes: 86\n",
      "4030: reward: 101.00, mean_100:  46.32, episodes: 87\n",
      "4136: reward: 106.00, mean_100:  47.00, episodes: 88\n",
      "4246: reward: 110.00, mean_100:  47.71, episodes: 89\n",
      "4348: reward: 102.00, mean_100:  48.31, episodes: 90\n",
      "4548: reward: 200.00, mean_100:  49.98, episodes: 91\n",
      "4748: reward: 200.00, mean_100:  51.61, episodes: 92\n",
      "4774: reward:  26.00, mean_100:  51.33, episodes: 93\n",
      "4802: reward:  28.00, mean_100:  51.09, episodes: 94\n",
      "4956: reward: 154.00, mean_100:  52.17, episodes: 95\n",
      "4976: reward:  20.00, mean_100:  51.83, episodes: 96\n",
      "5035: reward:  59.00, mean_100:  51.91, episodes: 97\n",
      "5172: reward: 137.00, mean_100:  52.78, episodes: 98\n",
      "5372: reward: 200.00, mean_100:  54.26, episodes: 99\n",
      "5501: reward: 129.00, mean_100:  55.01, episodes: 100\n",
      "5677: reward: 176.00, mean_100:  56.39, episodes: 101\n",
      "5816: reward: 139.00, mean_100:  57.46, episodes: 102\n",
      "5991: reward: 175.00, mean_100:  58.51, episodes: 103\n",
      "6115: reward: 124.00, mean_100:  58.99, episodes: 104\n",
      "6222: reward: 107.00, mean_100:  59.83, episodes: 105\n",
      "6373: reward: 151.00, mean_100:  61.18, episodes: 106\n",
      "6466: reward:  93.00, mean_100:  61.83, episodes: 107\n",
      "6586: reward: 120.00, mean_100:  62.84, episodes: 108\n",
      "6741: reward: 155.00, mean_100:  64.23, episodes: 109\n",
      "6941: reward: 200.00, mean_100:  65.87, episodes: 110\n",
      "7141: reward: 200.00, mean_100:  67.74, episodes: 111\n",
      "7222: reward:  81.00, mean_100:  68.36, episodes: 112\n",
      "7422: reward: 200.00, mean_100:  69.96, episodes: 113\n",
      "7562: reward: 140.00, mean_100:  71.25, episodes: 114\n",
      "7723: reward: 161.00, mean_100:  72.25, episodes: 115\n",
      "7850: reward: 127.00, mean_100:  73.08, episodes: 116\n",
      "8050: reward: 200.00, mean_100:  74.62, episodes: 117\n",
      "8141: reward:  91.00, mean_100:  75.24, episodes: 118\n",
      "8341: reward: 200.00, mean_100:  76.88, episodes: 119\n",
      "8533: reward: 192.00, mean_100:  78.25, episodes: 120\n",
      "8658: reward: 125.00, mean_100:  79.24, episodes: 121\n",
      "8691: reward:  33.00, mean_100:  79.44, episodes: 122\n",
      "8891: reward: 200.00, mean_100:  81.11, episodes: 123\n",
      "9091: reward: 200.00, mean_100:  82.61, episodes: 124\n",
      "9261: reward: 170.00, mean_100:  84.16, episodes: 125\n",
      "9461: reward: 200.00, mean_100:  85.84, episodes: 126\n",
      "9661: reward: 200.00, mean_100:  87.34, episodes: 127\n",
      "9813: reward: 152.00, mean_100:  88.51, episodes: 128\n",
      "9936: reward: 123.00, mean_100:  89.27, episodes: 129\n",
      "10136: reward: 200.00, mean_100:  91.02, episodes: 130\n",
      "10336: reward: 200.00, mean_100:  92.76, episodes: 131\n",
      "10534: reward: 198.00, mean_100:  94.16, episodes: 132\n",
      "10718: reward: 184.00, mean_100:  95.83, episodes: 133\n",
      "10918: reward: 200.00, mean_100:  97.49, episodes: 134\n",
      "11118: reward: 200.00, mean_100:  99.30, episodes: 135\n",
      "11298: reward: 180.00, mean_100: 100.81, episodes: 136\n",
      "11498: reward: 200.00, mean_100: 102.50, episodes: 137\n",
      "11698: reward: 200.00, mean_100: 104.27, episodes: 138\n",
      "11898: reward: 200.00, mean_100: 106.01, episodes: 139\n",
      "12098: reward: 200.00, mean_100: 107.41, episodes: 140\n",
      "12298: reward: 200.00, mean_100: 109.13, episodes: 141\n",
      "12498: reward: 200.00, mean_100: 110.96, episodes: 142\n",
      "12698: reward: 200.00, mean_100: 112.64, episodes: 143\n",
      "12898: reward: 200.00, mean_100: 114.43, episodes: 144\n",
      "13098: reward: 200.00, mean_100: 116.21, episodes: 145\n",
      "13298: reward: 200.00, mean_100: 117.66, episodes: 146\n",
      "13402: reward: 104.00, mean_100: 118.38, episodes: 147\n",
      "13602: reward: 200.00, mean_100: 119.83, episodes: 148\n",
      "13782: reward: 180.00, mean_100: 121.32, episodes: 149\n",
      "13982: reward: 200.00, mean_100: 123.11, episodes: 150\n",
      "14182: reward: 200.00, mean_100: 124.67, episodes: 151\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14359: reward: 177.00, mean_100: 126.07, episodes: 152\n",
      "14559: reward: 200.00, mean_100: 127.67, episodes: 153\n",
      "14697: reward: 138.00, mean_100: 128.57, episodes: 154\n",
      "14839: reward: 142.00, mean_100: 129.14, episodes: 155\n",
      "14967: reward: 128.00, mean_100: 130.11, episodes: 156\n",
      "15135: reward: 168.00, mean_100: 131.38, episodes: 157\n",
      "15286: reward: 151.00, mean_100: 132.68, episodes: 158\n",
      "15416: reward: 130.00, mean_100: 133.59, episodes: 159\n",
      "15566: reward: 150.00, mean_100: 134.71, episodes: 160\n",
      "15766: reward: 200.00, mean_100: 135.92, episodes: 161\n",
      "15966: reward: 200.00, mean_100: 137.39, episodes: 162\n",
      "16085: reward: 119.00, mean_100: 137.84, episodes: 163\n",
      "16285: reward: 200.00, mean_100: 139.38, episodes: 164\n",
      "16485: reward: 200.00, mean_100: 141.10, episodes: 165\n",
      "16685: reward: 200.00, mean_100: 142.68, episodes: 166\n",
      "16858: reward: 173.00, mean_100: 144.19, episodes: 167\n",
      "17034: reward: 176.00, mean_100: 145.13, episodes: 168\n",
      "17234: reward: 200.00, mean_100: 146.96, episodes: 169\n",
      "17434: reward: 200.00, mean_100: 148.15, episodes: 170\n",
      "17634: reward: 200.00, mean_100: 148.94, episodes: 171\n",
      "17834: reward: 200.00, mean_100: 150.59, episodes: 172\n",
      "18034: reward: 200.00, mean_100: 152.22, episodes: 173\n",
      "18234: reward: 200.00, mean_100: 153.09, episodes: 174\n",
      "18434: reward: 200.00, mean_100: 153.79, episodes: 175\n",
      "18634: reward: 200.00, mean_100: 155.51, episodes: 176\n",
      "18834: reward: 200.00, mean_100: 157.31, episodes: 177\n",
      "18959: reward: 125.00, mean_100: 157.36, episodes: 178\n",
      "19159: reward: 200.00, mean_100: 158.35, episodes: 179\n",
      "19359: reward: 200.00, mean_100: 160.06, episodes: 180\n",
      "19519: reward: 160.00, mean_100: 160.51, episodes: 181\n",
      "19719: reward: 200.00, mean_100: 161.06, episodes: 182\n",
      "19919: reward: 200.00, mean_100: 162.69, episodes: 183\n",
      "20119: reward: 200.00, mean_100: 163.48, episodes: 184\n",
      "20319: reward: 200.00, mean_100: 164.29, episodes: 185\n",
      "20519: reward: 200.00, mean_100: 165.90, episodes: 186\n",
      "20719: reward: 200.00, mean_100: 166.89, episodes: 187\n",
      "20884: reward: 165.00, mean_100: 167.48, episodes: 188\n",
      "21084: reward: 200.00, mean_100: 168.38, episodes: 189\n",
      "21284: reward: 200.00, mean_100: 169.36, episodes: 190\n",
      "21469: reward: 185.00, mean_100: 169.21, episodes: 191\n",
      "21653: reward: 184.00, mean_100: 169.05, episodes: 192\n",
      "21825: reward: 172.00, mean_100: 170.51, episodes: 193\n",
      "21983: reward: 158.00, mean_100: 171.81, episodes: 194\n",
      "22119: reward: 136.00, mean_100: 171.63, episodes: 195\n",
      "22190: reward:  71.00, mean_100: 172.14, episodes: 196\n",
      "22273: reward:  83.00, mean_100: 172.38, episodes: 197\n",
      "22391: reward: 118.00, mean_100: 172.19, episodes: 198\n",
      "22553: reward: 162.00, mean_100: 171.81, episodes: 199\n",
      "22753: reward: 200.00, mean_100: 172.52, episodes: 200\n",
      "22953: reward: 200.00, mean_100: 172.76, episodes: 201\n",
      "23153: reward: 200.00, mean_100: 173.37, episodes: 202\n",
      "23353: reward: 200.00, mean_100: 173.62, episodes: 203\n",
      "23553: reward: 200.00, mean_100: 174.38, episodes: 204\n",
      "23753: reward: 200.00, mean_100: 175.31, episodes: 205\n",
      "23953: reward: 200.00, mean_100: 175.80, episodes: 206\n",
      "24153: reward: 200.00, mean_100: 176.87, episodes: 207\n",
      "24353: reward: 200.00, mean_100: 177.67, episodes: 208\n",
      "24553: reward: 200.00, mean_100: 178.12, episodes: 209\n",
      "24753: reward: 200.00, mean_100: 178.12, episodes: 210\n",
      "24953: reward: 200.00, mean_100: 178.12, episodes: 211\n",
      "25153: reward: 200.00, mean_100: 179.31, episodes: 212\n",
      "25353: reward: 200.00, mean_100: 179.31, episodes: 213\n",
      "25553: reward: 200.00, mean_100: 179.91, episodes: 214\n",
      "25753: reward: 200.00, mean_100: 180.30, episodes: 215\n",
      "25953: reward: 200.00, mean_100: 181.03, episodes: 216\n",
      "26153: reward: 200.00, mean_100: 181.03, episodes: 217\n",
      "26331: reward: 178.00, mean_100: 181.90, episodes: 218\n",
      "26484: reward: 153.00, mean_100: 181.43, episodes: 219\n",
      "26607: reward: 123.00, mean_100: 180.74, episodes: 220\n",
      "26769: reward: 162.00, mean_100: 181.11, episodes: 221\n",
      "26969: reward: 200.00, mean_100: 182.78, episodes: 222\n",
      "27169: reward: 200.00, mean_100: 182.78, episodes: 223\n",
      "27369: reward: 200.00, mean_100: 182.78, episodes: 224\n",
      "27569: reward: 200.00, mean_100: 183.08, episodes: 225\n",
      "27769: reward: 200.00, mean_100: 183.08, episodes: 226\n",
      "27969: reward: 200.00, mean_100: 183.08, episodes: 227\n",
      "28169: reward: 200.00, mean_100: 183.56, episodes: 228\n",
      "28369: reward: 200.00, mean_100: 184.33, episodes: 229\n",
      "28569: reward: 200.00, mean_100: 184.33, episodes: 230\n",
      "28769: reward: 200.00, mean_100: 184.33, episodes: 231\n",
      "28969: reward: 200.00, mean_100: 184.35, episodes: 232\n",
      "29169: reward: 200.00, mean_100: 184.51, episodes: 233\n",
      "29369: reward: 200.00, mean_100: 184.51, episodes: 234\n",
      "29569: reward: 200.00, mean_100: 184.51, episodes: 235\n",
      "29702: reward: 133.00, mean_100: 184.04, episodes: 236\n",
      "29902: reward: 200.00, mean_100: 184.04, episodes: 237\n",
      "30102: reward: 200.00, mean_100: 184.04, episodes: 238\n",
      "30254: reward: 152.00, mean_100: 183.56, episodes: 239\n",
      "30418: reward: 164.00, mean_100: 183.20, episodes: 240\n",
      "30618: reward: 200.00, mean_100: 183.20, episodes: 241\n",
      "30818: reward: 200.00, mean_100: 183.20, episodes: 242\n",
      "31018: reward: 200.00, mean_100: 183.20, episodes: 243\n",
      "31218: reward: 200.00, mean_100: 183.20, episodes: 244\n",
      "31418: reward: 200.00, mean_100: 183.20, episodes: 245\n",
      "31618: reward: 200.00, mean_100: 183.20, episodes: 246\n",
      "31818: reward: 200.00, mean_100: 184.16, episodes: 247\n",
      "32018: reward: 200.00, mean_100: 184.16, episodes: 248\n",
      "32218: reward: 200.00, mean_100: 184.36, episodes: 249\n",
      "32418: reward: 200.00, mean_100: 184.36, episodes: 250\n",
      "32502: reward:  84.00, mean_100: 183.20, episodes: 251\n",
      "32702: reward: 200.00, mean_100: 183.43, episodes: 252\n",
      "32902: reward: 200.00, mean_100: 183.43, episodes: 253\n",
      "33102: reward: 200.00, mean_100: 184.05, episodes: 254\n",
      "33257: reward: 155.00, mean_100: 184.18, episodes: 255\n",
      "33288: reward:  31.00, mean_100: 183.21, episodes: 256\n",
      "33452: reward: 164.00, mean_100: 183.17, episodes: 257\n",
      "33652: reward: 200.00, mean_100: 183.66, episodes: 258\n",
      "33852: reward: 200.00, mean_100: 184.36, episodes: 259\n",
      "34052: reward: 200.00, mean_100: 184.86, episodes: 260\n",
      "34252: reward: 200.00, mean_100: 184.86, episodes: 261\n",
      "34452: reward: 200.00, mean_100: 184.86, episodes: 262\n",
      "34652: reward: 200.00, mean_100: 185.67, episodes: 263\n",
      "34852: reward: 200.00, mean_100: 185.67, episodes: 264\n",
      "35052: reward: 200.00, mean_100: 185.67, episodes: 265\n",
      "35252: reward: 200.00, mean_100: 185.67, episodes: 266\n",
      "35452: reward: 200.00, mean_100: 185.94, episodes: 267\n",
      "35652: reward: 200.00, mean_100: 186.18, episodes: 268\n",
      "35852: reward: 200.00, mean_100: 186.18, episodes: 269\n",
      "36052: reward: 200.00, mean_100: 186.18, episodes: 270\n",
      "36252: reward: 200.00, mean_100: 186.18, episodes: 271\n",
      "36452: reward: 200.00, mean_100: 186.18, episodes: 272\n",
      "36652: reward: 200.00, mean_100: 186.18, episodes: 273\n",
      "36852: reward: 200.00, mean_100: 186.18, episodes: 274\n",
      "37052: reward: 200.00, mean_100: 186.18, episodes: 275\n",
      "37148: reward:  96.00, mean_100: 185.14, episodes: 276\n",
      "37348: reward: 200.00, mean_100: 185.14, episodes: 277\n",
      "37509: reward: 161.00, mean_100: 185.50, episodes: 278\n",
      "37709: reward: 200.00, mean_100: 185.50, episodes: 279\n",
      "37909: reward: 200.00, mean_100: 185.50, episodes: 280\n",
      "38109: reward: 200.00, mean_100: 185.90, episodes: 281\n",
      "38256: reward: 147.00, mean_100: 185.37, episodes: 282\n",
      "38323: reward:  67.00, mean_100: 184.04, episodes: 283\n",
      "38511: reward: 188.00, mean_100: 183.92, episodes: 284\n",
      "38711: reward: 200.00, mean_100: 183.92, episodes: 285\n",
      "38911: reward: 200.00, mean_100: 183.92, episodes: 286\n",
      "39111: reward: 200.00, mean_100: 183.92, episodes: 287\n",
      "39311: reward: 200.00, mean_100: 184.27, episodes: 288\n",
      "39511: reward: 200.00, mean_100: 184.27, episodes: 289\n",
      "39707: reward: 196.00, mean_100: 184.23, episodes: 290\n",
      "39876: reward: 169.00, mean_100: 184.07, episodes: 291\n",
      "40039: reward: 163.00, mean_100: 183.86, episodes: 292\n",
      "40187: reward: 148.00, mean_100: 183.62, episodes: 293\n",
      "40219: reward:  32.00, mean_100: 182.36, episodes: 294\n",
      "40325: reward: 106.00, mean_100: 182.06, episodes: 295\n",
      "40466: reward: 141.00, mean_100: 182.76, episodes: 296\n",
      "40618: reward: 152.00, mean_100: 183.45, episodes: 297\n",
      "40758: reward: 140.00, mean_100: 183.67, episodes: 298\n",
      "40936: reward: 178.00, mean_100: 183.83, episodes: 299\n",
      "41100: reward: 164.00, mean_100: 183.47, episodes: 300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41300: reward: 200.00, mean_100: 183.47, episodes: 301\n",
      "41500: reward: 200.00, mean_100: 183.47, episodes: 302\n",
      "41700: reward: 200.00, mean_100: 183.47, episodes: 303\n",
      "41900: reward: 200.00, mean_100: 183.47, episodes: 304\n",
      "42100: reward: 200.00, mean_100: 183.47, episodes: 305\n",
      "42300: reward: 200.00, mean_100: 183.47, episodes: 306\n",
      "42487: reward: 187.00, mean_100: 183.34, episodes: 307\n",
      "42687: reward: 200.00, mean_100: 183.34, episodes: 308\n",
      "42860: reward: 173.00, mean_100: 183.07, episodes: 309\n",
      "42982: reward: 122.00, mean_100: 182.29, episodes: 310\n",
      "43089: reward: 107.00, mean_100: 181.36, episodes: 311\n",
      "43220: reward: 131.00, mean_100: 180.67, episodes: 312\n",
      "43335: reward: 115.00, mean_100: 179.82, episodes: 313\n",
      "43448: reward: 113.00, mean_100: 178.95, episodes: 314\n",
      "43571: reward: 123.00, mean_100: 178.18, episodes: 315\n",
      "43735: reward: 164.00, mean_100: 177.82, episodes: 316\n",
      "43879: reward: 144.00, mean_100: 177.26, episodes: 317\n",
      "44079: reward: 200.00, mean_100: 177.48, episodes: 318\n",
      "44279: reward: 200.00, mean_100: 177.95, episodes: 319\n",
      "44479: reward: 200.00, mean_100: 178.72, episodes: 320\n",
      "44679: reward: 200.00, mean_100: 179.10, episodes: 321\n",
      "44879: reward: 200.00, mean_100: 179.10, episodes: 322\n",
      "45079: reward: 200.00, mean_100: 179.10, episodes: 323\n",
      "45279: reward: 200.00, mean_100: 179.10, episodes: 324\n",
      "45350: reward:  71.00, mean_100: 177.81, episodes: 325\n",
      "45550: reward: 200.00, mean_100: 177.81, episodes: 326\n",
      "45750: reward: 200.00, mean_100: 177.81, episodes: 327\n",
      "45950: reward: 200.00, mean_100: 177.81, episodes: 328\n",
      "46150: reward: 200.00, mean_100: 177.81, episodes: 329\n",
      "46350: reward: 200.00, mean_100: 177.81, episodes: 330\n",
      "46550: reward: 200.00, mean_100: 177.81, episodes: 331\n",
      "46750: reward: 200.00, mean_100: 177.81, episodes: 332\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"CartPole-v0\")\n",
    "writer = SummaryWriter(comment=\"-cartpole-pg\")\n",
    "\n",
    "net = PGN(env.observation_space.shape[0], env.action_space.n)\n",
    "print(net)\n",
    "\n",
    "agent = ptan.agent.PolicyAgent(net, preprocessor=ptan.agent.float32_preprocessor,\n",
    "                               apply_softmax=True)\n",
    "exp_source = ptan.experience.ExperienceSourceFirstLast(\n",
    "    env, agent, gamma=GAMMA, steps_count=REWARD_STEPS)\n",
    "\n",
    "optimizer = optim.Adam(net.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "total_rewards = []\n",
    "step_rewards = []\n",
    "step_idx = 0\n",
    "done_episodes = 0\n",
    "reward_sum = 0.0\n",
    "bs_smoothed = entropy = l_entropy = l_policy = l_total = None\n",
    "\n",
    "batch_states, batch_actions, batch_scales = [], [], []\n",
    "\n",
    "for step_idx, exp in enumerate(exp_source):\n",
    "    reward_sum += exp.reward\n",
    "    baseline = reward_sum / (step_idx + 1)\n",
    "    writer.add_scalar(\"baseline\", baseline, step_idx)\n",
    "    batch_states.append(exp.state)\n",
    "    batch_actions.append(int(exp.action))\n",
    "    batch_scales.append(exp.reward - baseline)\n",
    "\n",
    "    # handle new rewards\n",
    "    new_rewards = exp_source.pop_total_rewards()\n",
    "    if new_rewards:\n",
    "        done_episodes += 1\n",
    "        reward = new_rewards[0]\n",
    "        total_rewards.append(reward)\n",
    "        mean_rewards = float(np.mean(total_rewards[-100:]))\n",
    "        print(\"%d: reward: %6.2f, mean_100: %6.2f, episodes: %d\" % (\n",
    "            step_idx, reward, mean_rewards, done_episodes))\n",
    "        writer.add_scalar(\"reward\", reward, step_idx)\n",
    "        writer.add_scalar(\"reward_100\", mean_rewards, step_idx)\n",
    "        writer.add_scalar(\"episodes\", done_episodes, step_idx)\n",
    "        if mean_rewards > 195:\n",
    "            print(\"Solved in %d steps and %d episodes!\" % (step_idx, done_episodes))\n",
    "            break\n",
    "\n",
    "    if len(batch_states) < BATCH_SIZE:\n",
    "        continue\n",
    "\n",
    "    states_v = torch.FloatTensor(batch_states)\n",
    "    batch_actions_t = torch.LongTensor(batch_actions)\n",
    "    batch_scale_v = torch.FloatTensor(batch_scales)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    logits_v = net(states_v)\n",
    "    log_prob_v = F.log_softmax(logits_v, dim=1)\n",
    "    log_prob_actions_v = batch_scale_v * log_prob_v[range(BATCH_SIZE), batch_actions_t]\n",
    "    loss_policy_v = -log_prob_actions_v.mean()\n",
    "\n",
    "    prob_v = F.softmax(logits_v, dim=1)\n",
    "    entropy_v = -(prob_v * log_prob_v).sum(dim=1).mean()\n",
    "    entropy_loss_v = -ENTROPY_BETA * entropy_v\n",
    "    loss_v = loss_policy_v + entropy_loss_v\n",
    "\n",
    "    loss_v.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # calc KL-div\n",
    "    new_logits_v = net(states_v)\n",
    "    new_prob_v = F.softmax(new_logits_v, dim=1)\n",
    "    kl_div_v = -((new_prob_v / prob_v).log() * prob_v).sum(dim=1).mean()\n",
    "    writer.add_scalar(\"kl\", kl_div_v.item(), step_idx)\n",
    "\n",
    "    grad_max = 0.0\n",
    "    grad_means = 0.0\n",
    "    grad_count = 0\n",
    "    for p in net.parameters():\n",
    "        grad_max = max(grad_max, p.grad.abs().max().item())\n",
    "        grad_means += (p.grad ** 2).mean().sqrt().item()\n",
    "        grad_count += 1\n",
    "\n",
    "    bs_smoothed = smooth(bs_smoothed, np.mean(batch_scales))\n",
    "    entropy = smooth(entropy, entropy_v.item())\n",
    "    l_entropy = smooth(l_entropy, entropy_loss_v.item())\n",
    "    l_policy = smooth(l_policy, loss_policy_v.item())\n",
    "    l_total = smooth(l_total, loss_v.item())\n",
    "\n",
    "    writer.add_scalar(\"baseline\", baseline, step_idx)\n",
    "    writer.add_scalar(\"entropy\", entropy, step_idx)\n",
    "    writer.add_scalar(\"loss_entropy\", l_entropy, step_idx)\n",
    "    writer.add_scalar(\"loss_policy\", l_policy, step_idx)\n",
    "    writer.add_scalar(\"loss_total\", l_total, step_idx)\n",
    "    writer.add_scalar(\"grad_l2\", grad_means / grad_count, step_idx)\n",
    "    writer.add_scalar(\"grad_max\", grad_max, step_idx)\n",
    "    writer.add_scalar(\"batch_scales\", bs_smoothed, step_idx)\n",
    "\n",
    "    batch_states.clear()\n",
    "    batch_actions.clear()\n",
    "    batch_scales.clear()\n",
    "\n",
    "writer.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
