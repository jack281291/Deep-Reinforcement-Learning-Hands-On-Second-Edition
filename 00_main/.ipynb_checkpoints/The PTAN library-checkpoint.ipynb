{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The PTAN library\n",
    "\n",
    "At a high level, PTAN provides the following entities:\n",
    "- Agent: a class that knows how to convert a batch of observations to a batch\n",
    "of actions to be executed. It can contain an optional state, in case you need\n",
    "to track some information between consequent actions in one episode.\n",
    "(We will use this approach in Chapter 17, Continuous Action Space, in the\n",
    "deep deterministic policy gradient (DDPG) method, which includes the\n",
    "Ornsteinâ€“Uhlenbeck random process for exploration.) The library provides\n",
    "several agents for the most common RL cases, but you always can write your\n",
    "own subclass of BaseAgent.\n",
    "- ActionSelector: a small piece of logic that knows how to choose the action\n",
    "from some output of the network. It works in tandem with Agent.\n",
    "- ExperienceSource and variations: the Agent instance and a Gym\n",
    "environment object can provide information about the trajectory from\n",
    "episodes. In its simplest form, it is one single (a, r, s') transition at a time, but\n",
    "its functionality goes beyond this.\n",
    "- ExperienceSourceBuffer and friends: replay buffers with various\n",
    "characteristics. They include a simple replay buffer and two versions of\n",
    "prioritized replay buffers.\n",
    "- Various utility classes, like TargetNet and wrappers for time series\n",
    "preprocessing (used for tracking training progress in TensorBoard).\n",
    "- PyTorch Ignite helpers to integrate PTAN into the Ignite framework.\n",
    "- Wrappers for Gym environments, for example, wrappers for Atari games\n",
    "(copied and pasted from OpenAI Baselines with some tweaks)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Action selectors\n",
    "In PTAN terminology, an action selector is an object that helps with going from\n",
    "network output to concrete action values. The most common cases include:\n",
    "- Argmax: commonly used by Q-value methods when the network predicts\n",
    "Q-values for a set of actions and the desired action is the action with the\n",
    "largest Q(s, a).\n",
    "- Policy-based: the network outputs the probability distribution (in the form\n",
    "of logits or normalized distribution), and an action needs to be sampled from\n",
    "this distribution. You have already seen this case in Chapter 4, The Cross-\n",
    "Entropy Method, where we discussed the cross-entropy method.\n",
    "An action selector is used by the Agent and rarely needs to be customized (but you\n",
    "have this option). The concrete classes provided by the library are:\n",
    "- ArgmaxActionSelector: applies argmax on the second axis of a passed\n",
    "tensor. (It assumes a matrix with batch dimension along the first axis.)\n",
    "- ProbabilityActionSelector: samples from the probability distribution\n",
    "of a discrete set of actions.\n",
    "- EpsilonGreedyActionSelector: has the parameter epsilon, which\n",
    "specifies the probability of a random action to be taken."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "q_vals\n",
      "[[ 1  2  3]\n",
      " [ 1 -1  0]]\n",
      "argmax: [2 0]\n",
      "epsilon=0.0: [2 0]\n",
      "epsilon=1.0: [0 2]\n",
      "epsilon=0.5: [2 0]\n",
      "epsilon=0.1: [2 0]\n",
      "Actions sampled from three prob distributions:\n",
      "[1 2 0]\n",
      "[1 2 0]\n",
      "[0 2 0]\n",
      "[2 2 0]\n",
      "[2 2 1]\n",
      "[1 2 1]\n",
      "[2 2 1]\n",
      "[1 2 1]\n",
      "[1 2 1]\n",
      "[1 2 1]\n"
     ]
    }
   ],
   "source": [
    "import ptan\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "q_vals = np.array([[1, 2, 3], [1, -1, 0]])\n",
    "print(\"q_vals\")\n",
    "print(q_vals)\n",
    "\n",
    "selector = ptan.actions.ArgmaxActionSelector()\n",
    "print(\"argmax:\", selector(q_vals))\n",
    "\n",
    "selector = ptan.actions.EpsilonGreedyActionSelector(epsilon=0.0)\n",
    "print(\"epsilon=0.0:\", selector(q_vals))\n",
    "\n",
    "selector.epsilon = 1.0\n",
    "print(\"epsilon=1.0:\", selector(q_vals))\n",
    "\n",
    "selector.epsilon = 0.5\n",
    "print(\"epsilon=0.5:\", selector(q_vals))\n",
    "selector.epsilon = 0.1\n",
    "print(\"epsilon=0.1:\", selector(q_vals))\n",
    "\n",
    "selector = ptan.actions.ProbabilityActionSelector()\n",
    "print(\"Actions sampled from three prob distributions:\")\n",
    "for _ in range(10):\n",
    "    acts = selector(np.array([\n",
    "        [0.1, 0.8, 0.1],\n",
    "        [0.0, 0.0, 1.0],\n",
    "        [0.5, 0.5, 0.0]\n",
    "    ]))\n",
    "    print(acts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the classes assume that NumPy arrays will be passed to them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The agent\n",
    "\n",
    "The agent entity provides a unified way of bridging observations from the\n",
    "environment and the actions that we want to execute. So far, you have seen only\n",
    "a simple, stateless DQN agent that uses a neural network (NN) to obtain actions'\n",
    "values from the current observation and behaves greedily on those values. We have\n",
    "used epsilon-greedy behavior to explore the environment, but this doesn't change\n",
    "the picture much.\n",
    "\n",
    "In the RL field, this could be more complicated. For example, instead of predicting\n",
    "the values of the actions, our agent could predict a probability distribution over the\n",
    "actions. Such agents are called policy agents, and we will talk about those methods\n",
    "in part three of the book.\n",
    "\n",
    "The other requirement could be the necessity for the agent to keep a state\n",
    "between observations. For example, very often one observation (or even the k last\n",
    "observation) is not enough to make a decision about the action, and we want to keep\n",
    "some memory in the agent to capture the necessary information. There is a whole\n",
    "subdomain of RL that tries to address this complication with partially observable\n",
    "Markov decision process (POMDP) formalism, which is not covered in the book.\n",
    "The third variant of the agent is very common in continuous control problems, which\n",
    "will be discussed in part four of the book. For now, it will be enough to say that in\n",
    "such cases, actions are not discrete anymore but some continuous value, and the\n",
    "agent needs to predict them from the observations.\n",
    "\n",
    "To capture all those variants and make the code flexible, the agent in PTAN is\n",
    "implemented as an extensible hierarchy of classes with the ptan.agent.BaseAgent\n",
    "abstract class at the top. From a high level, the agent needs to accept the batch of\n",
    "observations (in the form of a NumPy array) and return the batch of actions that it\n",
    "wants to take. The batch is used to make the processing more efficient, as processing\n",
    "several observations in one pass in a graphics processing unit (GPU) is frequently\n",
    "much faster than processing them individually.\n",
    "The abstract base class doesn't define the types of input and output, which makes\n",
    "it very flexible and easy to extend. For example, in the continuous domain, our\n",
    "actions will no longer be indices of discrete actions, but float values.\n",
    "\n",
    "In any case, the agent can be seen as something that knows how to convert\n",
    "observations into actions, and it's up to the agent how to do this. In general,\n",
    "there are no assumptions made on observation and action types, but the concrete\n",
    "implementation of agents is more limiting. PTAN provides two of the most common\n",
    "ways to convert observations into actions: DQNAgent and PolicyAgent.\n",
    "In real problems, a custom agent is often needed. These are some of the reasons:\n",
    "- The architecture of the NN is fancyâ€”its action space is a mixture of\n",
    "continuous and discrete, it has multimodal observations (text and pixels,\n",
    "for example), or something like that.\n",
    "- You want to use non-standard exploration strategies, for example, the\n",
    "Ornsteinâ€“Uhlenbeck process (a very popular exploration strategy in the\n",
    "continuous control domain).\n",
    "- You have a POMDP environment, and the agent's decision is not fully\n",
    "defined by observations, but by some internal agent state (which is also\n",
    "the case for Ornsteinâ€“Uhlenbeck exploration).\n",
    "All those cases are easily supported by subclassing the BaseAgent class, and in the\n",
    "rest of the book, several examples of such redefinition will be given."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DQNAgent and PolicyAgent\n",
    "This class is applicable in Q-learning when the action space is not very large,\n",
    "which covers Atari games and lots of classical problems. This representation is not\n",
    "universal, and later in the book, you will see ways of dealing with that. DQNAgent\n",
    "takes a batch of observations on input (as a NumPy array), applies the network\n",
    "on them to get Q-values, and then uses the provided ActionSelector to convert\n",
    "Q-values to indices of actions.\n",
    "Let's consider a small example. For simplicity, our network always produces the\n",
    "same output for the input batch.\n",
    "\n",
    "PolicyAgent expects the network to produce policy distribution over a discrete set of actions. Policy distribution could be either logits (unnormalized) or a normalized distribution. In practice, you should always use logits to improve the numeric stability of the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dqn_net:\n",
      "tensor([[1., 0., 0.],\n",
      "        [0., 1., 0.]])\n",
      "Argmax: (array([0, 1]), [None, None])\n",
      "eps=1.0: [1 2 2 0 2 0 1 1 2 0]\n",
      "eps=0.5: [0 2 2 2 2 0 1 0 1 0]\n",
      "eps=0.1: [0 1 2 0 0 0 0 1 0 0]\n",
      "policy_net:\n",
      "tensor([[1., 1., 0., 0., 0.],\n",
      "        [1., 1., 0., 0., 0.],\n",
      "        [1., 1., 0., 0., 0.],\n",
      "        [1., 1., 0., 0., 0.],\n",
      "        [1., 1., 0., 0., 0.],\n",
      "        [1., 1., 0., 0., 0.]])\n",
      "[1 4 0 1 2 1]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class DQNNet(nn.Module):\n",
    "    def __init__(self, actions: int):\n",
    "        super(DQNNet, self).__init__()\n",
    "        self.actions = actions\n",
    "\n",
    "    def forward(self, x):\n",
    "        # we always produce diagonal tensor of shape (batch_size, actions)\n",
    "        return torch.eye(x.size()[0], self.actions)\n",
    "\n",
    "\n",
    "class PolicyNet(nn.Module):\n",
    "    def __init__(self, actions: int):\n",
    "        super(PolicyNet, self).__init__()\n",
    "        self.actions = actions\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Now we produce the tensor with first two actions\n",
    "        # having the same logit scores\n",
    "        shape = (x.size()[0], self.actions)\n",
    "        res = torch.zeros(shape, dtype=torch.float32)\n",
    "        res[:, 0] = 1\n",
    "        res[:, 1] = 1\n",
    "        return res\n",
    "\n",
    "\n",
    "net = DQNNet(actions=3)\n",
    "net_out = net(torch.zeros(2, 10))\n",
    "print(\"dqn_net:\")\n",
    "print(net_out)\n",
    "\n",
    "selector = ptan.actions.ArgmaxActionSelector()\n",
    "agent = ptan.agent.DQNAgent(dqn_model=net, action_selector=selector)\n",
    "ag_out = agent(torch.zeros(2, 5))\n",
    "print(\"Argmax:\", ag_out)\n",
    "\n",
    "selector = ptan.actions.EpsilonGreedyActionSelector(epsilon=1.0)\n",
    "agent = ptan.agent.DQNAgent(dqn_model=net, action_selector=selector)\n",
    "ag_out = agent(torch.zeros(10, 5))[0]\n",
    "print(\"eps=1.0:\", ag_out)\n",
    "\n",
    "selector.epsilon = 0.5\n",
    "ag_out = agent(torch.zeros(10, 5))[0]\n",
    "print(\"eps=0.5:\", ag_out)\n",
    "\n",
    "selector.epsilon = 0.1\n",
    "ag_out = agent(torch.zeros(10, 5))[0]\n",
    "print(\"eps=0.1:\", ag_out)\n",
    "\n",
    "net = PolicyNet(actions=5)\n",
    "net_out = net(torch.zeros(6, 10))\n",
    "print(\"policy_net:\")\n",
    "print(net_out)\n",
    "\n",
    "selector = ptan.actions.ProbabilityActionSelector()\n",
    "agent = ptan.agent.PolicyAgent(model=net, action_selector=selector, apply_softmax=True)\n",
    "ag_out = agent(torch.zeros(6, 5))[0]\n",
    "print(ag_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experience source\n",
    "\n",
    "The agent abstraction described in the previous section allows us to implement\n",
    "environment communications in a generic way. These communications happen in the\n",
    "form of trajectories, produced by applying the agent's actions to the Gym environment.\n",
    "At a high level, experience source classes take the agent instance and environment\n",
    "and provide you with step-by step data from the trajectories. The functionality\n",
    "of those classes includes:\n",
    "- Support of multiple environments being communicated at the same time.\n",
    "This allows efficient GPU utilization as a batch of observations is being\n",
    "processed by the agent at once.\n",
    "\n",
    "- A trajectory can be preprocessed and presented in a convenient form for\n",
    "further training. For example, there is an implementation of subtrajectory\n",
    "rollouts with accumulation of the reward. That preprocessing is convenient\n",
    "for DQN and n-step DQN, when we are not interested in individual\n",
    "intermediate steps in subtrajectories, so they can be dropped. This\n",
    "saves memory and reduces the amount of code we need to write.\n",
    "- Support of vectorized environments from OpenAI Universe. We will\n",
    "cover this in Chapter 17, Continuous Action Space, for web automation and\n",
    "MiniWoB environments.\n",
    "So, the experience source classes act as a \"magic black box\" to hide the environment\n",
    "interaction and trajectory handling complexities from the library user. But the overall\n",
    "PTAN philosophy is to be flexible and extensible, so if you want, you can subclass\n",
    "one of the existing classes or implement your own version as needed.\n",
    "There are three classes provided by the system:\n",
    "    - ExperienceSource: using the agent and the set of environments, it produces\n",
    "n-step subtrajectories with all intermediate steps\n",
    "    - ExperienceSourceFirstLast: this is the same as ExperienceSource, but\n",
    "instead of a full subtrajectory (with all steps), it keeps only the first and last\n",
    "steps, with proper reward accumulation in between. This can save a lot of\n",
    "memory in the case of n-step DQN or advantage actor-critic (A2C) rollouts\n",
    "    - ExperienceSourceRollouts: this follows the asynchronous advantage\n",
    "actor-critic (A3C) rollouts scheme described in Mnih's paper about Atari\n",
    "games (referenced in Chapter 12, The Actor-Critic Method)\n",
    "All the classes are written to be efficient both in terms of central processing unit\n",
    "(CPU) and memory, which is not very important for toy problems, but might become\n",
    "an issue when you want to solve Atari games and need to keep 10M samples in the\n",
    "replay buffer using commodity hardware.\n",
    "\n",
    "For demonstration, we will implement a very simple Gym environment with\n",
    "a small predictable observation state to show how experience source classes work.\n",
    "This environment has integer observation, which increases from 0 to 4, integer\n",
    "action, and a reward equal to the action given."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import ptan\n",
    "from typing import List, Optional, Tuple, Any\n",
    "\n",
    "\n",
    "class ToyEnv(gym.Env):\n",
    "    \"\"\"\n",
    "    Environment with observation 0..4 and actions 0..2\n",
    "    Observations are rotated sequentialy mod 5, reward is equal to given action.\n",
    "    Episodes are having fixed length of 10\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(ToyEnv, self).__init__()\n",
    "        self.observation_space = gym.spaces.Discrete(n=5)\n",
    "        self.action_space = gym.spaces.Discrete(n=3)\n",
    "        self.step_index = 0\n",
    "\n",
    "    def reset(self):\n",
    "        self.step_index = 0\n",
    "        return self.step_index\n",
    "\n",
    "    def step(self, action):\n",
    "        is_done = self.step_index == 10\n",
    "        if is_done:\n",
    "            return self.step_index % self.observation_space.n, \\\n",
    "                   0.0, is_done, {}\n",
    "        self.step_index += 1\n",
    "        return self.step_index % self.observation_space.n, \\\n",
    "               float(action), self.step_index == 10, {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to this environment, we will use an agent that always generates fixed\n",
    "actions regardless of observations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DullAgent(ptan.agent.BaseAgent):\n",
    "    \"\"\"\n",
    "    Agent always returns the fixed action\n",
    "    \"\"\"\n",
    "    def __init__(self, action: int):\n",
    "        self.action = action\n",
    "\n",
    "    def __call__(self, observations: List[Any],\n",
    "                 state: Optional[List] = None) \\\n",
    "            -> Tuple[List[int], Optional[List]]:\n",
    "        return [self.action for _ in observations], state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env.reset() -> 0\n",
      "env.step(1) -> (1, 1.0, False, {})\n",
      "env.step(2) -> (2, 2.0, False, {})\n",
      "(3, 0.0, False, {})\n",
      "(4, 0.0, False, {})\n",
      "(0, 0.0, False, {})\n",
      "(1, 0.0, False, {})\n",
      "(2, 0.0, False, {})\n",
      "(3, 0.0, False, {})\n",
      "(4, 0.0, False, {})\n",
      "(0, 0.0, True, {})\n",
      "(0, 0.0, True, {})\n",
      "(0, 0.0, True, {})\n",
      "agent: [1, 1]\n"
     ]
    }
   ],
   "source": [
    "env = ToyEnv()\n",
    "s = env.reset()\n",
    "print(\"env.reset() -> %s\" % s)\n",
    "s = env.step(1)\n",
    "print(\"env.step(1) -> %s\" % str(s))\n",
    "s = env.step(2)\n",
    "print(\"env.step(2) -> %s\" % str(s))\n",
    "\n",
    "for _ in range(10):\n",
    "    r = env.step(0)\n",
    "    print(r)\n",
    "\n",
    "agent = DullAgent(action=1)\n",
    "print(\"agent:\", agent([1, 2])[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first class is ptan.experience.ExperienceSource, which generates chunks\n",
    "of agent trajectories of the given length. The implementation automatically handles\n",
    "the end of episode situation (when the step() method in the environment returns\n",
    "is_done=True) and resets the environment.\n",
    "The constructor accepts several arguments:\n",
    "- The Gym environment to be used. Alternatively, it could be the list of\n",
    "environments.\n",
    "- The agent instance.\n",
    "- steps_count=2: the length of subtrajectories to be generated.\n",
    "- vectorized=False: if set to True, the environment needs to be an OpenAI\n",
    "Universe vectorized environment. We will discuss such environments in\n",
    "detail in Chapter 16, Web Navigation.\n",
    "\n",
    "The class instance provides the standard Python iterator interface, so you can just\n",
    "iterate over this to get subtrajectories:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Experience(state=0, action=1, reward=1.0, done=False), Experience(state=1, action=1, reward=1.0, done=False))\n",
      "(Experience(state=1, action=1, reward=1.0, done=False), Experience(state=2, action=1, reward=1.0, done=False))\n",
      "(Experience(state=2, action=1, reward=1.0, done=False), Experience(state=3, action=1, reward=1.0, done=False))\n",
      "(Experience(state=3, action=1, reward=1.0, done=False), Experience(state=4, action=1, reward=1.0, done=False))\n",
      "(Experience(state=4, action=1, reward=1.0, done=False), Experience(state=0, action=1, reward=1.0, done=False))\n",
      "(Experience(state=0, action=1, reward=1.0, done=False), Experience(state=1, action=1, reward=1.0, done=False))\n",
      "(Experience(state=1, action=1, reward=1.0, done=False), Experience(state=2, action=1, reward=1.0, done=False))\n",
      "(Experience(state=2, action=1, reward=1.0, done=False), Experience(state=3, action=1, reward=1.0, done=False))\n",
      "(Experience(state=3, action=1, reward=1.0, done=False), Experience(state=4, action=1, reward=1.0, done=True))\n",
      "(Experience(state=4, action=1, reward=1.0, done=True),)\n",
      "(Experience(state=0, action=1, reward=1.0, done=False), Experience(state=1, action=1, reward=1.0, done=False))\n",
      "(Experience(state=1, action=1, reward=1.0, done=False), Experience(state=2, action=1, reward=1.0, done=False))\n",
      "(Experience(state=2, action=1, reward=1.0, done=False), Experience(state=3, action=1, reward=1.0, done=False))\n",
      "(Experience(state=3, action=1, reward=1.0, done=False), Experience(state=4, action=1, reward=1.0, done=False))\n",
      "(Experience(state=4, action=1, reward=1.0, done=False), Experience(state=0, action=1, reward=1.0, done=False))\n",
      "(Experience(state=0, action=1, reward=1.0, done=False), Experience(state=1, action=1, reward=1.0, done=False))\n"
     ]
    }
   ],
   "source": [
    "env = ToyEnv()\n",
    "agent = DullAgent(action=1)\n",
    "exp_source = ptan.experience.ExperienceSource(env=env, agent=agent, steps_count=2)\n",
    "for idx, exp in enumerate(exp_source):\n",
    "    if idx > 15:\n",
    "        break\n",
    "    print(exp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On every iteration, ExperienceSource returns a piece of the agent's trajectory\n",
    "in environment communication. It might look simple, but there are several things\n",
    "happening under the hood of our example:\n",
    "1. reset() was called in the environment to get the initial state\n",
    "2. The agent was asked to select the action to execute from the state returned\n",
    "3. The step() method was executed to get the reward and the next state\n",
    "4. This next state was passed to the agent for the next action\n",
    "5. Information about the transition from one state to the next state was returned\n",
    "6. The process iterated (from step 3) until it iterated over the experience source\n",
    "If the agent changes the way it generates actions (we can get this by updating the\n",
    "network weights, decreasing epsilon, or by some other means), it will immediately\n",
    "affect the experience trajectories that we get.\n",
    "\n",
    "The ExperienceSource instance returns tuples of length equal to or less than the\n",
    "argument step_count passed on construction. In our case, we asked for two-step\n",
    "subtrajectories, so tuples will be of length 2 or 1 (at the end of episodes). Every object\n",
    "in a tuple is an instance of the ptan.experience.Experience class, which is a\n",
    "namedtuple with the following fields:\n",
    "- state: the state we observed before taking the action\n",
    "- action: the action we completed\n",
    "- reward: the immediate reward we got from env\n",
    "- done: whether the episode was done\n",
    "\n",
    "If the episode reaches the end, the subtrajectory will be shorter and the underlying\n",
    "environment will be reset automatically, so we don't need to bother with this and\n",
    "can just keep iterating.\n",
    "\n",
    "We can ask ExperienceSource for subtrajectories of any length:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Experience(state=0, action=1, reward=1.0, done=False), Experience(state=1, action=1, reward=1.0, done=False), Experience(state=2, action=1, reward=1.0, done=False), Experience(state=3, action=1, reward=1.0, done=False))\n"
     ]
    }
   ],
   "source": [
    "exp_source = ptan.experience.ExperienceSource(env=env, agent=agent, steps_count=4)\n",
    "print(next(iter(exp_source)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can pass it several instances of gym.Env. In that case, they will be used in roundrobin\n",
    "fashion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Experience(state=0, action=1, reward=1.0, done=False), Experience(state=1, action=1, reward=1.0, done=False))\n",
      "(Experience(state=0, action=1, reward=1.0, done=False), Experience(state=1, action=1, reward=1.0, done=False))\n",
      "(Experience(state=1, action=1, reward=1.0, done=False), Experience(state=2, action=1, reward=1.0, done=False))\n",
      "(Experience(state=1, action=1, reward=1.0, done=False), Experience(state=2, action=1, reward=1.0, done=False))\n",
      "(Experience(state=2, action=1, reward=1.0, done=False), Experience(state=3, action=1, reward=1.0, done=False))\n"
     ]
    }
   ],
   "source": [
    "exp_source = ptan.experience.ExperienceSource(env=[ToyEnv(), ToyEnv()], agent=agent, steps_count=2)\n",
    "for idx, exp in enumerate(exp_source):\n",
    "    if idx > 4:\n",
    "        break\n",
    "    print(exp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The class ExperienceSource provides us with full subtrajectories of the given length\n",
    "as the list of (s, a, r) objects. The next state, s', is returned in the next tuple, which is\n",
    "not always convenient. For example, in DQN training, we want to have tuples (s, a,\n",
    "r, s') at once to do one-step Bellman approximation during the training. \n",
    "\n",
    "In addition, some extension of DQN, like n-step DQN, might want to collapse longer sequences\n",
    "of observations into (first-state, action, total-reward-for-n-steps, state-after-step-n).\n",
    "To support this in a generic way, a simple subclass of ExperienceSource is\n",
    "implemented: ExperienceSourceFirstLast. It accepts almost the same arguments\n",
    "in the constructor, but returns different data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ExperienceFirstLast(state=0, action=1, reward=1.0, last_state=1)\n",
      "ExperienceFirstLast(state=1, action=1, reward=1.0, last_state=2)\n",
      "ExperienceFirstLast(state=2, action=1, reward=1.0, last_state=3)\n",
      "ExperienceFirstLast(state=3, action=1, reward=1.0, last_state=4)\n",
      "ExperienceFirstLast(state=4, action=1, reward=1.0, last_state=0)\n",
      "ExperienceFirstLast(state=0, action=1, reward=1.0, last_state=1)\n",
      "ExperienceFirstLast(state=1, action=1, reward=1.0, last_state=2)\n",
      "ExperienceFirstLast(state=2, action=1, reward=1.0, last_state=3)\n",
      "ExperienceFirstLast(state=3, action=1, reward=1.0, last_state=4)\n",
      "ExperienceFirstLast(state=4, action=1, reward=1.0, last_state=None)\n",
      "ExperienceFirstLast(state=0, action=1, reward=1.0, last_state=1)\n",
      "ExperienceFirstLast(state=1, action=1, reward=1.0, last_state=2)\n"
     ]
    }
   ],
   "source": [
    "exp_source = ptan.experience.ExperienceSourceFirstLast(env, agent, gamma=1.0, steps_count=1)\n",
    "for idx, exp in enumerate(exp_source):\n",
    "    print(exp)\n",
    "    if idx > 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it returns a single object on every iteration, which is again a namedtuple\n",
    "with the following fields:\n",
    "- state: the state we used to decide on the action to take\n",
    "- action: the action we took at this step\n",
    "- reward: the partial accumulated reward for steps_count (in our case,\n",
    "steps_count=1, so it is equal to the immediate reward)\n",
    "- last_state: the state we got after executing the action. If our episode ends,\n",
    "we have None here\n",
    "\n",
    "This data is much more convenient for DQN training, as we can apply Bellman\n",
    "approximation directly to it.\n",
    "Let's check the result with a larger number of steps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ExperienceFirstLast(state=0, action=1, reward=4.0, last_state=4)\n",
      "ExperienceFirstLast(state=1, action=1, reward=4.0, last_state=0)\n",
      "ExperienceFirstLast(state=2, action=1, reward=4.0, last_state=1)\n",
      "ExperienceFirstLast(state=3, action=1, reward=4.0, last_state=2)\n",
      "ExperienceFirstLast(state=4, action=1, reward=4.0, last_state=3)\n",
      "ExperienceFirstLast(state=0, action=1, reward=4.0, last_state=4)\n",
      "ExperienceFirstLast(state=1, action=1, reward=4.0, last_state=None)\n",
      "ExperienceFirstLast(state=2, action=1, reward=3.0, last_state=None)\n",
      "ExperienceFirstLast(state=3, action=1, reward=2.0, last_state=None)\n",
      "ExperienceFirstLast(state=4, action=1, reward=1.0, last_state=None)\n",
      "ExperienceFirstLast(state=0, action=1, reward=4.0, last_state=4)\n",
      "ExperienceFirstLast(state=1, action=1, reward=4.0, last_state=0)\n"
     ]
    }
   ],
   "source": [
    "exp_source = ptan.experience.ExperienceSourceFirstLast(env, agent, gamma=1.0, steps_count=4)\n",
    "for idx, exp in enumerate(exp_source):\n",
    "    print(exp)\n",
    "    if idx > 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, now we are collapsing 4 steps on every iteration and calculating the immediate\n",
    "reward. As the episode ends, we have last_state=None in those samples, but additionally,\n",
    "we calculate the reward for the tail of the episode. Those tiny details are very easy\n",
    "to implement wrongly if you are doing all the trajectory handling yourself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experience replay buffers\n",
    "\n",
    "In DQN, we rarely deal with immediate experience samples, as they are heavily\n",
    "correlated, which leads to instability in the training. Normally, we have large replay\n",
    "buffers, which are populated with experience pieces. Then the buffer is sampled\n",
    "(randomly or with priority weights) to get the training batch. The replay buffer\n",
    "normally has a maximum capacity, so old samples are pushed out when the replay\n",
    "buffer reaches the limit.\n",
    "\n",
    "There are several implementation tricks here, which become extremely important\n",
    "when you need to deal with large problems:\n",
    "- How to efficiently sample from a large buffer\n",
    "- How to push old samples from the buffer\n",
    "- In the case of a prioritized buffer, how priorities need to be maintained and\n",
    "handled in the most efficient way\n",
    "\n",
    "All this becomes a quite non-trivial task if you want to solve Atari, keeping 10-100M\n",
    "samples, where every sample is an image from the game. A small mistake can lead\n",
    "to a 10-100x memory increase and major slowdowns of the training process.\n",
    "PTAN provides several variants of replay buffers, which integrate simply with\n",
    "ExperienceSource and Agent machinery. Normally, what you need to do is ask\n",
    "the buffer to pull a new sample from the source and sample the training batch. The\n",
    "provided classes are:\n",
    "- ExperienceReplayBuffer: a simple replay buffer of predefined size with\n",
    "uniform sampling.\n",
    "- PrioReplayBufferNaive: a simple, but not very efficient, prioritized replay\n",
    "buffer implementation. The complexity of sampling is O(n), which might\n",
    "become an issue with large buffers. This version has the advantage over the\n",
    "optimized class, having much easier code.\n",
    "- PrioritizedReplayBuffer: uses segment trees for sampling, which makes\n",
    "the code cryptic, but with O(log(n)) sampling complexity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = ToyEnv()\n",
    "agent = DullAgent(action=1)\n",
    "exp_source = ptan.experience.ExperienceSourceFirstLast(env, agent, gamma=1.0, steps_count=1)\n",
    "buffer = ptan.experience.ExperienceReplayBuffer(exp_source, buffer_size=100)\n",
    "len(buffer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All replay buffers provide the following interface:\n",
    "- A Python iterator interface to walk over all the samples in the buffer\n",
    "- The method populate(N) to get N samples from the experience source\n",
    "and put them into the buffer\n",
    "- The method sample(N) to get the batch of N experience objects\n",
    "\n",
    "So, the normal training loop for DQN looks like an infinite repetition of the following\n",
    "steps:\n",
    "1. Call buffer.populate(1) to get a fresh sample from the environment\n",
    "2. batch = buffer.sample(BATCH_SIZE) to get the batch from the buffer\n",
    "3. Calculate the loss on the sampled batch\n",
    "4. Backpropagate\n",
    "5. Repeat until convergence (hopefully)\n",
    "\n",
    "All the rest happens automatically: resetting the environment, handling\n",
    "subtrajectories, buffer size maintenance, and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train time, 4 batch samples:\n",
      "ExperienceFirstLast(state=1, action=1, reward=1.0, last_state=2)\n",
      "ExperienceFirstLast(state=1, action=1, reward=1.0, last_state=2)\n",
      "ExperienceFirstLast(state=4, action=1, reward=1.0, last_state=0)\n",
      "ExperienceFirstLast(state=0, action=1, reward=1.0, last_state=1)\n",
      "Train time, 4 batch samples:\n",
      "ExperienceFirstLast(state=0, action=1, reward=1.0, last_state=1)\n",
      "ExperienceFirstLast(state=0, action=1, reward=1.0, last_state=1)\n",
      "ExperienceFirstLast(state=0, action=1, reward=1.0, last_state=1)\n",
      "ExperienceFirstLast(state=1, action=1, reward=1.0, last_state=2)\n"
     ]
    }
   ],
   "source": [
    "for step in range(6):\n",
    "    buffer.populate(1)\n",
    "    # if buffer is small enough, do nothing\n",
    "    if len(buffer) < 5:\n",
    "        continue\n",
    "    batch = buffer.sample(4)\n",
    "    print(\"Train time, %d batch samples:\" % len(batch))\n",
    "    for s in batch:\n",
    "        print(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The TargetNet class\n",
    "\n",
    "TargetNet is a small but useful class that allows us to synchronize two NNs\n",
    "of the same architecture. The purpose of this was described in the previous\n",
    "chapter: improving training stability. TargetNet supports two modes of such\n",
    "synchronization:\n",
    "- sync(): weights from the source network are copied into the target network.\n",
    "- alpha_sync(): the source network's weights are blended into the target\n",
    "network with some alpha weight (between 0 and 1).\n",
    "\n",
    "The first mode is the standard way to perform a target network sync in discrete\n",
    "action space problems, like Atari and CartPole. We did this in Chapter 6, Deep\n",
    "Q-Networks. \n",
    "\n",
    "The latter mode is used in continuous control problems, which will\n",
    "be described in several chapters in part four of the book. In such problems, the\n",
    "transition between two networks' parameters should be smooth, so alpha blending is\n",
    "used, given by the formula ð‘¤ð‘– = ð‘¤i*ð›¼ + ð‘ ð‘–(1 âˆ’ ð›¼) , where wi is the target network's ith\n",
    "parameter and si is the source network's weight. \n",
    "\n",
    "The following is a small example of\n",
    "how TargetNet should be used in code. Assume we have the following network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DQNNet, self).__init__()\n",
    "        self.ff = nn.Linear(5, 3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.ff(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DQNNet(\n",
      "  (ff): Linear(in_features=5, out_features=3, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "net = DQNNet()\n",
    "print(net)\n",
    "tgt_net = ptan.agent.TargetNet(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The target network contains two fields: model, which is the reference to the original\n",
    "network, and target_model, which is a deep copy of it. If we examine both\n",
    "networks' weights, they will be the same:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main net: Parameter containing:\n",
      "tensor([[ 0.1682, -0.4226,  0.1878,  0.3536,  0.2065],\n",
      "        [-0.1958, -0.1072, -0.3952, -0.0881, -0.1878],\n",
      "        [ 0.3368,  0.2458,  0.2772, -0.2961, -0.0219]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(\"Main net:\", net.ff.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.1682, -0.4226,  0.1878,  0.3536,  0.2065],\n",
       "        [-0.1958, -0.1072, -0.3952, -0.0881, -0.1878],\n",
       "        [ 0.3368,  0.2458,  0.2772, -0.2961, -0.0219]], requires_grad=True)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tgt_net.target_model.ff.weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "They are independent of each other, however, just having the same architecture:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.ff.weight.data += 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After update\n",
      "Main net: Parameter containing:\n",
      "tensor([[1.1682, 0.5774, 1.1878, 1.3536, 1.2065],\n",
      "        [0.8042, 0.8928, 0.6048, 0.9119, 0.8122],\n",
      "        [1.3368, 1.2458, 1.2772, 0.7039, 0.9781]], requires_grad=True)\n",
      "Target net: Parameter containing:\n",
      "tensor([[ 0.1682, -0.4226,  0.1878,  0.3536,  0.2065],\n",
      "        [-0.1958, -0.1072, -0.3952, -0.0881, -0.1878],\n",
      "        [ 0.3368,  0.2458,  0.2772, -0.2961, -0.0219]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(\"After update\")\n",
    "print(\"Main net:\", net.ff.weight)\n",
    "print(\"Target net:\", tgt_net.target_model.ff.weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To synchronize them again, the sync() method can be used:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After sync\n",
      "Main net: Parameter containing:\n",
      "tensor([[1.1682, 0.5774, 1.1878, 1.3536, 1.2065],\n",
      "        [0.8042, 0.8928, 0.6048, 0.9119, 0.8122],\n",
      "        [1.3368, 1.2458, 1.2772, 0.7039, 0.9781]], requires_grad=True)\n",
      "Target net: Parameter containing:\n",
      "tensor([[1.1682, 0.5774, 1.1878, 1.3536, 1.2065],\n",
      "        [0.8042, 0.8928, 0.6048, 0.9119, 0.8122],\n",
      "        [1.3368, 1.2458, 1.2772, 0.7039, 0.9781]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "tgt_net.sync()\n",
    "print(\"After sync\")\n",
    "print(\"Main net:\", net.ff.weight)\n",
    "print(\"Target net:\", tgt_net.target_model.ff.weight)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
