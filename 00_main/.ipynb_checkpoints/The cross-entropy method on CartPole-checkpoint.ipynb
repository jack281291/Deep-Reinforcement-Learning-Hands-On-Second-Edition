{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from collections import namedtuple\n",
    "import numpy as np\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define constants at the top of the file and they include the count of neurons\n",
    "in the hidden layer, the count of episodes we play on every iteration (16), and the percentile of episodes' total rewards that we use for \"elite\" episode filtering. **We will take the 70th percentile, which means that we will leave the top 30% of episodes sorted by reward.**\n",
    "\n",
    "Rather than calculating softmax (which uses exponentiation) and then calculating cross-entropy loss (which uses\n",
    "a logarithm of probabilities), **we can use the PyTorch class nn.CrossEntropyLoss, which combines both softmax and cross-entropy in a single, more numerically stable expression. CrossEntropyLoss requires raw, unnormalized values from the NN (also called logits). The downside of this is that we need to remember to apply softmax every time we need to get probabilities from our NN's output.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "HIDDEN_SIZE = 128\n",
    "BATCH_SIZE = 16\n",
    "PERCENTILE = 70\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, obs_size, hidden_size, n_actions):\n",
    "        super(Net, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(obs_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, n_actions)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will define two helper classes that are named tuples from the collections package in the standard library:\n",
    "\n",
    "- EpisodeStep: This will be used to represent one single step that our agent made in the episode, and it stores the observation from the environment and what action the agent completed. We will use episode steps from \"elite\" episodes as training data.\n",
    "- Episode: This is a single episode stored as total undiscounted reward and a collection of EpisodeStep."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "Episode = namedtuple('Episode', field_names=['reward', 'steps'])\n",
    "EpisodeStep = namedtuple('EpisodeStep', field_names=['observation', 'action'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function accepts the environment (the Env class instance from the Gym library), our NN, and the count of episodes it should generate on every iteration. \n",
    "\n",
    "The batch variable will be used to accumulate our batch (which is a list of Episode instances). We also declare a reward counter for the current episode and its list of steps (the EpisodeStep objects). Then we reset our environment to obtain the first observation and create a softmax layer, which will be used to convert the NN's output to a probability distribution of actions. That's our preparations complete, so we are ready to start the environment loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterate_batches(env, net, batch_size):\n",
    "    batch = []\n",
    "    episode_reward = 0.0\n",
    "    episode_steps = []\n",
    "    obs = env.reset()\n",
    "    sm = nn.Softmax(dim=1)\n",
    "    while True:\n",
    "        obs_v = torch.FloatTensor([obs])\n",
    "        act_probs_v = sm(net(obs_v))\n",
    "        act_probs = act_probs_v.data.numpy()[0]\n",
    "        action = np.random.choice(len(act_probs), p=act_probs)\n",
    "        next_obs, reward, is_done, _ = env.step(action)\n",
    "        episode_reward += reward\n",
    "        step = EpisodeStep(observation=obs, action=action)\n",
    "        episode_steps.append(step)\n",
    "        if is_done:\n",
    "            e = Episode(reward=episode_reward, steps=episode_steps)\n",
    "            batch.append(e)\n",
    "            episode_reward = 0.0\n",
    "            episode_steps = []\n",
    "            next_obs = env.reset()\n",
    "            if len(batch) == batch_size:\n",
    "                yield batch\n",
    "                batch = []\n",
    "        obs = next_obs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At every iteration, we convert our current observation to a PyTorch tensor and pass it to the NN to obtain action probabilities. There are several things to note here:\n",
    "- All nn.Module instances in PyTorch expect a batch of data items and the same is true for our NN, so we convert our observation (which is a vector of four numbers in CartPole) into a tensor of size 1×4 (to achieve this, we pass an observation in a single-element list).\n",
    "- As we haven't used nonlinearity at the output of our NN, it outputs raw action scores, which we need to feed through the softmax function.\n",
    "- Both our NN and the softmax layer return tensors that track gradients, so we need to unpack this by accessing the tensor.data field and then converting the tensor into a NumPy array. This array will have the same two-dimensional structure as the input, with the batch dimension on axis 0, so we need to get the first batch element to obtain a one-dimensional vector of action probabilities.\n",
    "\n",
    "We append the finalized episode to the batch, saving the total reward (as the episode\n",
    "has been completed and we have accumulated all the reward) and steps we have\n",
    "taken. Then we reset our total reward accumulator and clean the list of steps. After\n",
    "that, we reset our environment to start over.\n",
    "**In case our batch has reached the desired count of episodes, we return it to the\n",
    "caller for processing using yield. Our function is a generator, so every time the\n",
    "yield operator is executed, the control is transferred to the outer iteration loop and\n",
    "then continues after the yield line.**\n",
    "\n",
    "**One very important fact to understand in this function logic is that the training\n",
    "of our NN and the generation of our episodes are performed at the same time. They\n",
    "are not completely in parallel, but every time our loop accumulates enough episodes\n",
    "(16), it passes control to this function caller, which is supposed to train the NN using\n",
    "gradient descent. So, when yield is returned, the NN will have different, slightly\n",
    "better (we hope) behavior.\n",
    "We don't need to explore proper synchronization, as our training and data gathering\n",
    "activities are performed at the same thread of execution, but you need to understand\n",
    "those constant jumps from NN training to its utilization.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, now we need to define yet another function and then we will be ready to\n",
    "switch to the training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_batch(batch, percentile):\n",
    "    rewards = list(map(lambda s: s.reward, batch))\n",
    "    reward_bound = np.percentile(rewards, percentile)\n",
    "    reward_mean = float(np.mean(rewards))\n",
    "\n",
    "    train_obs = []\n",
    "    train_act = []\n",
    "    for reward, steps in batch:\n",
    "        if reward < reward_bound:\n",
    "            continue\n",
    "        train_obs.extend(map(lambda step: step.observation, steps))\n",
    "        train_act.extend(map(lambda step: step.action, steps))\n",
    "\n",
    "    train_obs_v = torch.FloatTensor(train_obs)\n",
    "    train_act_v = torch.LongTensor(train_act)\n",
    "    return train_obs_v, train_act_v, reward_bound, reward_mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**This function is at the core of the cross-entropy method—from the given batch\n",
    "of episodes and percentile value, it calculates a boundary reward, which is used\n",
    "to filter \"elite\" episodes to train on. To obtain the boundary reward, we will use\n",
    "NumPy's percentile function, which, from the list of values and the desired\n",
    "percentile, calculates the percentile's value. Then, we will calculate the mean\n",
    "reward, which is used only for monitoring.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, the final chunk of code that glues everything together, and mostly consists\n",
    "of the training loop.\n",
    "\n",
    "In the beginning, we create all the required objects: the environment, our NN, the\n",
    "objective function, the optimizer, and the summary writer for TensorBoard.\n",
    "\n",
    "The commented line creates a monitor to write videos of your agent's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v0\")\n",
    "# env = gym.wrappers.Monitor(env, directory=\"mon\", force=True)\n",
    "obs_size = env.observation_space.shape[0]\n",
    "n_actions = env.action_space.n\n",
    "\n",
    "net = Net(obs_size, HIDDEN_SIZE, n_actions)\n",
    "objective = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(params=net.parameters(), lr=0.01)\n",
    "writer = SummaryWriter(comment=\"-cartpole\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the training loop, we iterate our batches (a list of Episode objects), then we\n",
    "perform filtering of the \"elite\" episodes using the filter_batch function. \n",
    "\n",
    "The result is variables of observations and taken actions, the reward boundary used for\n",
    "filtering, and the mean reward. After that, we zero gradients of our NN and pass\n",
    "observations to the NN, obtaining its action scores. \n",
    "\n",
    "These scores are passed to the\n",
    "objective function, which will calculate cross-entropy between the NN output\n",
    "and the actions that the agent took. The idea of this is to reinforce our NN to carry\n",
    "out those \"elite\" actions that have led to good rewards. Then, we calculate gradients\n",
    "on the loss and ask the optimizer to adjust our NN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: loss=0.680, reward_mean=17.4, rw_bound=21.5\n",
      "1: loss=0.648, reward_mean=16.2, rw_bound=18.5\n",
      "2: loss=0.648, reward_mean=19.1, rw_bound=19.5\n",
      "3: loss=0.699, reward_mean=19.2, rw_bound=23.0\n",
      "4: loss=0.699, reward_mean=20.1, rw_bound=23.5\n",
      "5: loss=0.702, reward_mean=20.5, rw_bound=23.0\n",
      "6: loss=0.682, reward_mean=22.5, rw_bound=28.5\n",
      "7: loss=0.679, reward_mean=38.6, rw_bound=48.0\n",
      "8: loss=0.666, reward_mean=26.3, rw_bound=35.0\n",
      "9: loss=0.658, reward_mean=26.6, rw_bound=31.5\n",
      "10: loss=0.661, reward_mean=30.1, rw_bound=28.0\n",
      "11: loss=0.651, reward_mean=32.2, rw_bound=39.0\n",
      "12: loss=0.638, reward_mean=36.1, rw_bound=36.0\n",
      "13: loss=0.639, reward_mean=34.2, rw_bound=40.5\n",
      "14: loss=0.656, reward_mean=42.0, rw_bound=46.0\n",
      "15: loss=0.635, reward_mean=73.0, rw_bound=94.0\n",
      "16: loss=0.625, reward_mean=56.6, rw_bound=71.0\n",
      "17: loss=0.621, reward_mean=56.3, rw_bound=66.5\n",
      "18: loss=0.630, reward_mean=46.8, rw_bound=59.0\n",
      "19: loss=0.615, reward_mean=43.6, rw_bound=50.5\n",
      "20: loss=0.613, reward_mean=42.9, rw_bound=47.0\n",
      "21: loss=0.615, reward_mean=48.6, rw_bound=51.5\n",
      "22: loss=0.596, reward_mean=43.6, rw_bound=52.0\n",
      "23: loss=0.602, reward_mean=52.4, rw_bound=58.0\n",
      "24: loss=0.607, reward_mean=62.9, rw_bound=73.5\n",
      "25: loss=0.588, reward_mean=82.6, rw_bound=98.0\n",
      "26: loss=0.590, reward_mean=84.3, rw_bound=98.5\n",
      "27: loss=0.564, reward_mean=72.5, rw_bound=77.5\n",
      "28: loss=0.583, reward_mean=73.2, rw_bound=87.5\n",
      "29: loss=0.582, reward_mean=88.4, rw_bound=98.0\n",
      "30: loss=0.600, reward_mean=97.1, rw_bound=116.0\n",
      "31: loss=0.567, reward_mean=72.6, rw_bound=85.5\n",
      "32: loss=0.571, reward_mean=93.4, rw_bound=112.0\n",
      "33: loss=0.581, reward_mean=94.6, rw_bound=117.5\n",
      "34: loss=0.576, reward_mean=139.9, rw_bound=170.0\n",
      "35: loss=0.573, reward_mean=159.5, rw_bound=199.5\n",
      "36: loss=0.565, reward_mean=158.1, rw_bound=180.0\n",
      "37: loss=0.570, reward_mean=189.1, rw_bound=200.0\n",
      "38: loss=0.579, reward_mean=142.3, rw_bound=182.5\n",
      "39: loss=0.567, reward_mean=169.9, rw_bound=200.0\n",
      "40: loss=0.561, reward_mean=189.0, rw_bound=200.0\n",
      "41: loss=0.566, reward_mean=172.8, rw_bound=200.0\n",
      "42: loss=0.573, reward_mean=184.4, rw_bound=200.0\n",
      "43: loss=0.558, reward_mean=192.8, rw_bound=200.0\n",
      "44: loss=0.560, reward_mean=181.8, rw_bound=200.0\n",
      "45: loss=0.563, reward_mean=187.8, rw_bound=200.0\n",
      "46: loss=0.550, reward_mean=181.6, rw_bound=200.0\n",
      "47: loss=0.555, reward_mean=189.6, rw_bound=200.0\n",
      "48: loss=0.559, reward_mean=196.9, rw_bound=200.0\n",
      "49: loss=0.553, reward_mean=188.0, rw_bound=200.0\n",
      "50: loss=0.549, reward_mean=193.9, rw_bound=200.0\n",
      "51: loss=0.550, reward_mean=181.7, rw_bound=200.0\n",
      "52: loss=0.554, reward_mean=184.6, rw_bound=200.0\n",
      "53: loss=0.547, reward_mean=186.8, rw_bound=200.0\n",
      "54: loss=0.550, reward_mean=192.1, rw_bound=200.0\n",
      "55: loss=0.550, reward_mean=189.1, rw_bound=200.0\n",
      "56: loss=0.549, reward_mean=197.4, rw_bound=200.0\n",
      "57: loss=0.548, reward_mean=195.1, rw_bound=200.0\n",
      "58: loss=0.553, reward_mean=199.5, rw_bound=200.0\n",
      "Solved!\n"
     ]
    }
   ],
   "source": [
    "for iter_no, batch in enumerate(iterate_batches(\n",
    "        env, net, BATCH_SIZE)):\n",
    "    obs_v, acts_v, reward_b, reward_m = \\\n",
    "        filter_batch(batch, PERCENTILE)\n",
    "    optimizer.zero_grad()\n",
    "    action_scores_v = net(obs_v)\n",
    "    loss_v = objective(action_scores_v, acts_v)\n",
    "    loss_v.backward()\n",
    "    optimizer.step()\n",
    "    print(\"%d: loss=%.3f, reward_mean=%.1f, rw_bound=%.1f\" % (\n",
    "        iter_no, loss_v.item(), reward_m, reward_b))\n",
    "    writer.add_scalar(\"loss\", loss_v.item(), iter_no)\n",
    "    writer.add_scalar(\"reward_bound\", reward_b, iter_no)\n",
    "    writer.add_scalar(\"reward_mean\", reward_m, iter_no)\n",
    "    if reward_m > 199:\n",
    "        print(\"Solved!\")\n",
    "        break\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last check in the loop is the comparison of the mean rewards of our batch\n",
    "episodes. When this becomes greater than 199, we stop our training. \n",
    "\n",
    "Why 199? In\n",
    "Gym, the CartPole environment is considered to be solved when the mean reward\n",
    "for the last 100 episodes is greater than 195, but our method converges so quickly\n",
    "that 100 episodes are usually what we need. The properly trained agent can balance\n",
    "the stick infinitely long (obtaining any amount of score), but the length of an episode\n",
    "in CartPole is limited to 200 steps (if you look at the environment variable of\n",
    "CartPole, you may notice the TimeLimit wrapper, which stops the episode after 200\n",
    "steps). With all this in mind, we will stop training after the mean reward in the batch\n",
    "is greater than 199, which is a good indication that our agent knows how to balance\n",
    "the stick like a pro."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
