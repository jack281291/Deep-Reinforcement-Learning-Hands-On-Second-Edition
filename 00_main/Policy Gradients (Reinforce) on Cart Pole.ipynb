{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy Gradients (Reinforce) on Cart Pole\n",
    "\n",
    "In the beginning, we define hyperparameters. The EPISODES_\n",
    "TO_TRAIN value specifies how many complete episodes we will use for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import ptan\n",
    "import numpy as np\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "GAMMA = 0.99\n",
    "LEARNING_RATE = 0.01\n",
    "EPISODES_TO_TRAIN = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following network should also be familiar to you. Note that despite the fact our network\n",
    "returns probabilities, we are not applying softmax nonlinearity to the output. The\n",
    "reason behind this is that we will use the PyTorch log_softmax function to calculate\n",
    "the logarithm of the softmax output at once. This method of calculation is much more\n",
    "numerically stable; however, we need to remember that output from the network\n",
    "is not probability, but raw scores (usually called logits)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PGN(nn.Module):\n",
    "    def __init__(self, input_size, n_actions):\n",
    "        super(PGN, self).__init__()\n",
    "\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_size, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, n_actions)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function is a bit tricky. It accepts a list of rewards for the whole episode and\n",
    "needs to calculate the discounted total reward for every step. To do this efficiently,\n",
    "we calculate the reward from the end of the local reward list. Indeed, the last step\n",
    "of the episode will have a total reward equal to its local reward. The step before the\n",
    "last will have the total reward of ùëüùë°‚àí1 + ùõæùëüùë° (if t is an index of the last step)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_qvals(rewards):\n",
    "    res = []\n",
    "    sum_r = 0.0\n",
    "    for r in reversed(rewards):\n",
    "        sum_r *= GAMMA\n",
    "        sum_r += r\n",
    "        res.append(sum_r)\n",
    "    return list(reversed(res))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our sum_r variable contains the total reward for the previous steps, so to get the\n",
    "total reward for the previous step, we need to multiply sum_r by gamma and sum\n",
    "the local reward.\n",
    "\n",
    "The preparation steps before the training loop should also be familiar to you.\n",
    "\n",
    "The only new element is the agent class from the PTAN library. Here, we are using\n",
    "ptan.agent.PolicyAgent, which needs to make a decision about actions for every\n",
    "observation. As our network now returns the policy as the probabilities of the\n",
    "actions, in order to select the action to take, we need to obtain the probabilities from\n",
    "the network and then perform random sampling from this probability distribution.\n",
    "\n",
    "When we worked with DQN, the output of the network was Q-values, so if some\n",
    "action had the value of 0.4 and another action had 0.5, the second action was\n",
    "preferred 100% of the time. In the case of the probability distribution, if the first\n",
    "action has a probability of 0.4 and the second 0.5, our agent should take the first\n",
    "action with a 40% chance and the second with a 50% chance. Of course, our network\n",
    "can decide to take the second action 100% of the time, and in this case, it returns the\n",
    "probability 0 for the first action and the probability 1 for the second action.\n",
    "\n",
    "This difference is important to understand, but the change in the implementation is\n",
    "not large. Our PolicyAgent internally calls the NumPy random.choice function with\n",
    "probabilities from the network. The apply_softmax argument instructs it to convert\n",
    "the network output to probabilities by calling softmax first. \n",
    "\n",
    "The third argument preprocessor is a way to get around the fact that the CartPole environment in Gym\n",
    "returns the observation as float64 instead of the float32 required by PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PGN(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=4, out_features=128, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=128, out_features=2, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"CartPole-v0\")\n",
    "writer = SummaryWriter(comment=\"-cartpole-reinforce\")\n",
    "\n",
    "net = PGN(env.observation_space.shape[0], env.action_space.n)\n",
    "print(net)\n",
    "\n",
    "agent = ptan.agent.PolicyAgent(net, preprocessor=ptan.agent.float32_preprocessor,\n",
    "                               apply_softmax=True)\n",
    "exp_source = ptan.experience.ExperienceSourceFirstLast(env, agent, gamma=GAMMA)\n",
    "\n",
    "optimizer = optim.Adam(net.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "total_rewards = []\n",
    "step_idx = 0\n",
    "done_episodes = 0\n",
    "\n",
    "batch_episodes = 0\n",
    "batch_states, batch_actions, batch_qvals = [], [], []\n",
    "cur_rewards = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we can start the training loop, we need several variables. The first group of\n",
    "these is used for reporting and contains the total rewards for the episodes and the\n",
    "count of completed episodes. \n",
    "\n",
    "The second group is used to gather the training data.\n",
    "The cur_rewards list contains local rewards for the episode being currently played.\n",
    "\n",
    "As this episode reaches the end, we calculate the discounted total rewards from\n",
    "local rewards using the calc_qvals function and append them to the batch_qvals\n",
    "list. The batch_states and batch_actions lists contain states and actions that we\n",
    "saw from the last training.\n",
    "\n",
    "When enough episodes have passed since the last training step, we perform\n",
    "optimization on the gathered examples. As a first step, we convert states, actions,\n",
    "and Q-values into the appropriate PyTorch form.\n",
    "\n",
    "Then we calculate the loss from the steps. To do this, we ask our network to calculate\n",
    "states into logits and calculate the logarithm + softmax of them. On the third line,\n",
    "we select log probabilities from the actions taken and scale them with Q-values.\n",
    "On the last line, we average those scaled values and do negation to obtain the loss\n",
    "to minimize. Once again, this minus sign is very important, as our policy gradient\n",
    "needs to be maximized to improve the policy. As the optimizer in PyTorch does\n",
    "minimization in respect to the loss function, we need to negate the policy gradient.\n",
    "\n",
    "Then we perform backpropagation to gather gradients in our\n",
    "variables and ask the optimizer to perform an SGD update. At the end of the training\n",
    "loop, we reset the episodes counter and clear our lists for fresh data to gather."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20: reward:  20.00, mean_100:  20.00, episodes: 1\n",
      "34: reward:  14.00, mean_100:  17.00, episodes: 2\n",
      "46: reward:  12.00, mean_100:  15.33, episodes: 3\n",
      "77: reward:  31.00, mean_100:  19.25, episodes: 4\n",
      "109: reward:  32.00, mean_100:  21.80, episodes: 5\n",
      "126: reward:  17.00, mean_100:  21.00, episodes: 6\n",
      "152: reward:  26.00, mean_100:  21.71, episodes: 7\n",
      "178: reward:  26.00, mean_100:  22.25, episodes: 8\n",
      "227: reward:  49.00, mean_100:  25.22, episodes: 9\n",
      "241: reward:  14.00, mean_100:  24.10, episodes: 10\n",
      "274: reward:  33.00, mean_100:  24.91, episodes: 11\n",
      "292: reward:  18.00, mean_100:  24.33, episodes: 12\n",
      "327: reward:  35.00, mean_100:  25.15, episodes: 13\n",
      "355: reward:  28.00, mean_100:  25.36, episodes: 14\n",
      "368: reward:  13.00, mean_100:  24.53, episodes: 15\n",
      "380: reward:  12.00, mean_100:  23.75, episodes: 16\n",
      "423: reward:  43.00, mean_100:  24.88, episodes: 17\n",
      "480: reward:  57.00, mean_100:  26.67, episodes: 18\n",
      "501: reward:  21.00, mean_100:  26.37, episodes: 19\n",
      "534: reward:  33.00, mean_100:  26.70, episodes: 20\n",
      "575: reward:  41.00, mean_100:  27.38, episodes: 21\n",
      "594: reward:  19.00, mean_100:  27.00, episodes: 22\n",
      "620: reward:  26.00, mean_100:  26.96, episodes: 23\n",
      "677: reward:  57.00, mean_100:  28.21, episodes: 24\n",
      "753: reward:  76.00, mean_100:  30.12, episodes: 25\n",
      "768: reward:  15.00, mean_100:  29.54, episodes: 26\n",
      "914: reward: 146.00, mean_100:  33.85, episodes: 27\n",
      "947: reward:  33.00, mean_100:  33.82, episodes: 28\n",
      "1032: reward:  85.00, mean_100:  35.59, episodes: 29\n",
      "1068: reward:  36.00, mean_100:  35.60, episodes: 30\n",
      "1085: reward:  17.00, mean_100:  35.00, episodes: 31\n",
      "1105: reward:  20.00, mean_100:  34.53, episodes: 32\n",
      "1157: reward:  52.00, mean_100:  35.06, episodes: 33\n",
      "1194: reward:  37.00, mean_100:  35.12, episodes: 34\n",
      "1218: reward:  24.00, mean_100:  34.80, episodes: 35\n",
      "1250: reward:  32.00, mean_100:  34.72, episodes: 36\n",
      "1266: reward:  16.00, mean_100:  34.22, episodes: 37\n",
      "1314: reward:  48.00, mean_100:  34.58, episodes: 38\n",
      "1345: reward:  31.00, mean_100:  34.49, episodes: 39\n",
      "1361: reward:  16.00, mean_100:  34.02, episodes: 40\n",
      "1403: reward:  42.00, mean_100:  34.22, episodes: 41\n",
      "1439: reward:  36.00, mean_100:  34.26, episodes: 42\n",
      "1480: reward:  41.00, mean_100:  34.42, episodes: 43\n",
      "1513: reward:  33.00, mean_100:  34.39, episodes: 44\n",
      "1580: reward:  67.00, mean_100:  35.11, episodes: 45\n",
      "1625: reward:  45.00, mean_100:  35.33, episodes: 46\n",
      "1659: reward:  34.00, mean_100:  35.30, episodes: 47\n",
      "1697: reward:  38.00, mean_100:  35.35, episodes: 48\n",
      "1748: reward:  51.00, mean_100:  35.67, episodes: 49\n",
      "1811: reward:  63.00, mean_100:  36.22, episodes: 50\n",
      "1839: reward:  28.00, mean_100:  36.06, episodes: 51\n",
      "1880: reward:  41.00, mean_100:  36.15, episodes: 52\n",
      "1943: reward:  63.00, mean_100:  36.66, episodes: 53\n",
      "1984: reward:  41.00, mean_100:  36.74, episodes: 54\n",
      "2013: reward:  29.00, mean_100:  36.60, episodes: 55\n",
      "2057: reward:  44.00, mean_100:  36.73, episodes: 56\n",
      "2115: reward:  58.00, mean_100:  37.11, episodes: 57\n",
      "2159: reward:  44.00, mean_100:  37.22, episodes: 58\n",
      "2208: reward:  49.00, mean_100:  37.42, episodes: 59\n",
      "2323: reward: 115.00, mean_100:  38.72, episodes: 60\n",
      "2358: reward:  35.00, mean_100:  38.66, episodes: 61\n",
      "2402: reward:  44.00, mean_100:  38.74, episodes: 62\n",
      "2473: reward:  71.00, mean_100:  39.25, episodes: 63\n",
      "2536: reward:  63.00, mean_100:  39.62, episodes: 64\n",
      "2584: reward:  48.00, mean_100:  39.75, episodes: 65\n",
      "2634: reward:  50.00, mean_100:  39.91, episodes: 66\n",
      "2701: reward:  67.00, mean_100:  40.31, episodes: 67\n",
      "2774: reward:  73.00, mean_100:  40.79, episodes: 68\n",
      "2837: reward:  63.00, mean_100:  41.12, episodes: 69\n",
      "2938: reward: 101.00, mean_100:  41.97, episodes: 70\n",
      "2997: reward:  59.00, mean_100:  42.21, episodes: 71\n",
      "3190: reward: 193.00, mean_100:  44.31, episodes: 72\n",
      "3260: reward:  70.00, mean_100:  44.66, episodes: 73\n",
      "3460: reward: 200.00, mean_100:  46.76, episodes: 74\n",
      "3541: reward:  81.00, mean_100:  47.21, episodes: 75\n",
      "3615: reward:  74.00, mean_100:  47.57, episodes: 76\n",
      "3710: reward:  95.00, mean_100:  48.18, episodes: 77\n",
      "3739: reward:  29.00, mean_100:  47.94, episodes: 78\n",
      "3800: reward:  61.00, mean_100:  48.10, episodes: 79\n",
      "3880: reward:  80.00, mean_100:  48.50, episodes: 80\n",
      "3957: reward:  77.00, mean_100:  48.85, episodes: 81\n",
      "3994: reward:  37.00, mean_100:  48.71, episodes: 82\n",
      "4025: reward:  31.00, mean_100:  48.49, episodes: 83\n",
      "4058: reward:  33.00, mean_100:  48.31, episodes: 84\n",
      "4133: reward:  75.00, mean_100:  48.62, episodes: 85\n",
      "4207: reward:  74.00, mean_100:  48.92, episodes: 86\n",
      "4256: reward:  49.00, mean_100:  48.92, episodes: 87\n",
      "4377: reward: 121.00, mean_100:  49.74, episodes: 88\n",
      "4430: reward:  53.00, mean_100:  49.78, episodes: 89\n",
      "4497: reward:  67.00, mean_100:  49.97, episodes: 90\n",
      "4581: reward:  84.00, mean_100:  50.34, episodes: 91\n",
      "4688: reward: 107.00, mean_100:  50.96, episodes: 92\n",
      "4750: reward:  62.00, mean_100:  51.08, episodes: 93\n",
      "4797: reward:  47.00, mean_100:  51.03, episodes: 94\n",
      "4864: reward:  67.00, mean_100:  51.20, episodes: 95\n",
      "4986: reward: 122.00, mean_100:  51.94, episodes: 96\n",
      "5059: reward:  73.00, mean_100:  52.15, episodes: 97\n",
      "5132: reward:  73.00, mean_100:  52.37, episodes: 98\n",
      "5201: reward:  69.00, mean_100:  52.54, episodes: 99\n",
      "5250: reward:  49.00, mean_100:  52.50, episodes: 100\n",
      "5327: reward:  77.00, mean_100:  53.07, episodes: 101\n",
      "5430: reward: 103.00, mean_100:  53.96, episodes: 102\n",
      "5535: reward: 105.00, mean_100:  54.89, episodes: 103\n",
      "5605: reward:  70.00, mean_100:  55.28, episodes: 104\n",
      "5678: reward:  73.00, mean_100:  55.69, episodes: 105\n",
      "5752: reward:  74.00, mean_100:  56.26, episodes: 106\n",
      "5862: reward: 110.00, mean_100:  57.10, episodes: 107\n",
      "5967: reward: 105.00, mean_100:  57.89, episodes: 108\n",
      "6008: reward:  41.00, mean_100:  57.81, episodes: 109\n",
      "6161: reward: 153.00, mean_100:  59.20, episodes: 110\n",
      "6340: reward: 179.00, mean_100:  60.66, episodes: 111\n",
      "6490: reward: 150.00, mean_100:  61.98, episodes: 112\n",
      "6654: reward: 164.00, mean_100:  63.27, episodes: 113\n",
      "6751: reward:  97.00, mean_100:  63.96, episodes: 114\n",
      "6837: reward:  86.00, mean_100:  64.69, episodes: 115\n",
      "6985: reward: 148.00, mean_100:  66.05, episodes: 116\n",
      "7072: reward:  87.00, mean_100:  66.49, episodes: 117\n",
      "7233: reward: 161.00, mean_100:  67.53, episodes: 118\n",
      "7345: reward: 112.00, mean_100:  68.44, episodes: 119\n",
      "7545: reward: 200.00, mean_100:  70.11, episodes: 120\n",
      "7686: reward: 141.00, mean_100:  71.11, episodes: 121\n",
      "7739: reward:  53.00, mean_100:  71.45, episodes: 122\n",
      "7842: reward: 103.00, mean_100:  72.22, episodes: 123\n",
      "8042: reward: 200.00, mean_100:  73.65, episodes: 124\n",
      "8199: reward: 157.00, mean_100:  74.46, episodes: 125\n",
      "8296: reward:  97.00, mean_100:  75.28, episodes: 126\n",
      "8457: reward: 161.00, mean_100:  75.43, episodes: 127\n",
      "8534: reward:  77.00, mean_100:  75.87, episodes: 128\n",
      "8608: reward:  74.00, mean_100:  75.76, episodes: 129\n",
      "8729: reward: 121.00, mean_100:  76.61, episodes: 130\n",
      "8788: reward:  59.00, mean_100:  77.03, episodes: 131\n",
      "8874: reward:  86.00, mean_100:  77.69, episodes: 132\n",
      "8936: reward:  62.00, mean_100:  77.79, episodes: 133\n",
      "9060: reward: 124.00, mean_100:  78.66, episodes: 134\n",
      "9145: reward:  85.00, mean_100:  79.27, episodes: 135\n",
      "9264: reward: 119.00, mean_100:  80.14, episodes: 136\n",
      "9324: reward:  60.00, mean_100:  80.58, episodes: 137\n",
      "9427: reward: 103.00, mean_100:  81.13, episodes: 138\n",
      "9532: reward: 105.00, mean_100:  81.87, episodes: 139\n",
      "9732: reward: 200.00, mean_100:  83.71, episodes: 140\n",
      "9873: reward: 141.00, mean_100:  84.70, episodes: 141\n",
      "9965: reward:  92.00, mean_100:  85.26, episodes: 142\n",
      "10105: reward: 140.00, mean_100:  86.25, episodes: 143\n",
      "10165: reward:  60.00, mean_100:  86.52, episodes: 144\n",
      "10287: reward: 122.00, mean_100:  87.07, episodes: 145\n",
      "10443: reward: 156.00, mean_100:  88.18, episodes: 146\n",
      "10558: reward: 115.00, mean_100:  88.99, episodes: 147\n",
      "10685: reward: 127.00, mean_100:  89.88, episodes: 148\n",
      "10815: reward: 130.00, mean_100:  90.67, episodes: 149\n",
      "10907: reward:  92.00, mean_100:  90.96, episodes: 150\n",
      "11011: reward: 104.00, mean_100:  91.72, episodes: 151\n",
      "11171: reward: 160.00, mean_100:  92.91, episodes: 152\n",
      "11367: reward: 196.00, mean_100:  94.24, episodes: 153\n",
      "11513: reward: 146.00, mean_100:  95.29, episodes: 154\n",
      "11615: reward: 102.00, mean_100:  96.02, episodes: 155\n",
      "11774: reward: 159.00, mean_100:  97.17, episodes: 156\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11974: reward: 200.00, mean_100:  98.59, episodes: 157\n",
      "12105: reward: 131.00, mean_100:  99.46, episodes: 158\n",
      "12253: reward: 148.00, mean_100: 100.45, episodes: 159\n",
      "12423: reward: 170.00, mean_100: 101.00, episodes: 160\n",
      "12551: reward: 128.00, mean_100: 101.93, episodes: 161\n",
      "12654: reward: 103.00, mean_100: 102.52, episodes: 162\n",
      "12808: reward: 154.00, mean_100: 103.35, episodes: 163\n",
      "13008: reward: 200.00, mean_100: 104.72, episodes: 164\n",
      "13208: reward: 200.00, mean_100: 106.24, episodes: 165\n",
      "13336: reward: 128.00, mean_100: 107.02, episodes: 166\n",
      "13508: reward: 172.00, mean_100: 108.07, episodes: 167\n",
      "13655: reward: 147.00, mean_100: 108.81, episodes: 168\n",
      "13832: reward: 177.00, mean_100: 109.95, episodes: 169\n",
      "14032: reward: 200.00, mean_100: 110.94, episodes: 170\n",
      "14198: reward: 166.00, mean_100: 112.01, episodes: 171\n",
      "14323: reward: 125.00, mean_100: 111.33, episodes: 172\n",
      "14523: reward: 200.00, mean_100: 112.63, episodes: 173\n",
      "14700: reward: 177.00, mean_100: 112.40, episodes: 174\n",
      "14898: reward: 198.00, mean_100: 113.57, episodes: 175\n",
      "15054: reward: 156.00, mean_100: 114.39, episodes: 176\n",
      "15254: reward: 200.00, mean_100: 115.44, episodes: 177\n",
      "15405: reward: 151.00, mean_100: 116.66, episodes: 178\n",
      "15543: reward: 138.00, mean_100: 117.43, episodes: 179\n",
      "15700: reward: 157.00, mean_100: 118.20, episodes: 180\n",
      "15900: reward: 200.00, mean_100: 119.43, episodes: 181\n",
      "16100: reward: 200.00, mean_100: 121.06, episodes: 182\n",
      "16258: reward: 158.00, mean_100: 122.33, episodes: 183\n",
      "16406: reward: 148.00, mean_100: 123.48, episodes: 184\n",
      "16606: reward: 200.00, mean_100: 124.73, episodes: 185\n",
      "16803: reward: 197.00, mean_100: 125.96, episodes: 186\n",
      "16966: reward: 163.00, mean_100: 127.10, episodes: 187\n",
      "17106: reward: 140.00, mean_100: 127.29, episodes: 188\n",
      "17294: reward: 188.00, mean_100: 128.64, episodes: 189\n",
      "17494: reward: 200.00, mean_100: 129.97, episodes: 190\n",
      "17682: reward: 188.00, mean_100: 131.01, episodes: 191\n",
      "17812: reward: 130.00, mean_100: 131.24, episodes: 192\n",
      "18012: reward: 200.00, mean_100: 132.62, episodes: 193\n",
      "18205: reward: 193.00, mean_100: 134.08, episodes: 194\n",
      "18319: reward: 114.00, mean_100: 134.55, episodes: 195\n",
      "18508: reward: 189.00, mean_100: 135.22, episodes: 196\n",
      "18671: reward: 163.00, mean_100: 136.12, episodes: 197\n",
      "18871: reward: 200.00, mean_100: 137.39, episodes: 198\n",
      "18995: reward: 124.00, mean_100: 137.94, episodes: 199\n",
      "19195: reward: 200.00, mean_100: 139.45, episodes: 200\n",
      "19379: reward: 184.00, mean_100: 140.52, episodes: 201\n",
      "19579: reward: 200.00, mean_100: 141.49, episodes: 202\n",
      "19744: reward: 165.00, mean_100: 142.09, episodes: 203\n",
      "19902: reward: 158.00, mean_100: 142.97, episodes: 204\n",
      "20050: reward: 148.00, mean_100: 143.72, episodes: 205\n",
      "20204: reward: 154.00, mean_100: 144.52, episodes: 206\n",
      "20389: reward: 185.00, mean_100: 145.27, episodes: 207\n",
      "20589: reward: 200.00, mean_100: 146.22, episodes: 208\n",
      "20789: reward: 200.00, mean_100: 147.81, episodes: 209\n",
      "20989: reward: 200.00, mean_100: 148.28, episodes: 210\n",
      "21189: reward: 200.00, mean_100: 148.49, episodes: 211\n",
      "21338: reward: 149.00, mean_100: 148.48, episodes: 212\n",
      "21538: reward: 200.00, mean_100: 148.84, episodes: 213\n",
      "21738: reward: 200.00, mean_100: 149.87, episodes: 214\n",
      "21938: reward: 200.00, mean_100: 151.01, episodes: 215\n",
      "22138: reward: 200.00, mean_100: 151.53, episodes: 216\n",
      "22338: reward: 200.00, mean_100: 152.66, episodes: 217\n",
      "22538: reward: 200.00, mean_100: 153.05, episodes: 218\n",
      "22738: reward: 200.00, mean_100: 153.93, episodes: 219\n",
      "22938: reward: 200.00, mean_100: 153.93, episodes: 220\n",
      "23138: reward: 200.00, mean_100: 154.52, episodes: 221\n",
      "23326: reward: 188.00, mean_100: 155.87, episodes: 222\n",
      "23526: reward: 200.00, mean_100: 156.84, episodes: 223\n",
      "23726: reward: 200.00, mean_100: 156.84, episodes: 224\n",
      "23926: reward: 200.00, mean_100: 157.27, episodes: 225\n",
      "24126: reward: 200.00, mean_100: 158.30, episodes: 226\n",
      "24326: reward: 200.00, mean_100: 158.69, episodes: 227\n",
      "24526: reward: 200.00, mean_100: 159.92, episodes: 228\n",
      "24726: reward: 200.00, mean_100: 161.18, episodes: 229\n",
      "24926: reward: 200.00, mean_100: 161.97, episodes: 230\n",
      "25126: reward: 200.00, mean_100: 163.38, episodes: 231\n",
      "25326: reward: 200.00, mean_100: 164.52, episodes: 232\n",
      "25526: reward: 200.00, mean_100: 165.90, episodes: 233\n",
      "25726: reward: 200.00, mean_100: 166.66, episodes: 234\n",
      "25926: reward: 200.00, mean_100: 167.81, episodes: 235\n",
      "26126: reward: 200.00, mean_100: 168.62, episodes: 236\n",
      "26300: reward: 174.00, mean_100: 169.76, episodes: 237\n",
      "26500: reward: 200.00, mean_100: 170.73, episodes: 238\n",
      "26700: reward: 200.00, mean_100: 171.68, episodes: 239\n",
      "26900: reward: 200.00, mean_100: 171.68, episodes: 240\n",
      "27100: reward: 200.00, mean_100: 172.27, episodes: 241\n",
      "27300: reward: 200.00, mean_100: 173.35, episodes: 242\n",
      "27500: reward: 200.00, mean_100: 173.95, episodes: 243\n",
      "27669: reward: 169.00, mean_100: 175.04, episodes: 244\n",
      "27869: reward: 200.00, mean_100: 175.82, episodes: 245\n",
      "28001: reward: 132.00, mean_100: 175.58, episodes: 246\n",
      "28201: reward: 200.00, mean_100: 176.43, episodes: 247\n",
      "28401: reward: 200.00, mean_100: 177.16, episodes: 248\n",
      "28581: reward: 180.00, mean_100: 177.66, episodes: 249\n",
      "28781: reward: 200.00, mean_100: 178.74, episodes: 250\n",
      "28981: reward: 200.00, mean_100: 179.70, episodes: 251\n",
      "29181: reward: 200.00, mean_100: 180.10, episodes: 252\n",
      "29381: reward: 200.00, mean_100: 180.14, episodes: 253\n",
      "29581: reward: 200.00, mean_100: 180.68, episodes: 254\n",
      "29781: reward: 200.00, mean_100: 181.66, episodes: 255\n",
      "29981: reward: 200.00, mean_100: 182.07, episodes: 256\n",
      "30181: reward: 200.00, mean_100: 182.07, episodes: 257\n",
      "30372: reward: 191.00, mean_100: 182.67, episodes: 258\n",
      "30572: reward: 200.00, mean_100: 183.19, episodes: 259\n",
      "30772: reward: 200.00, mean_100: 183.49, episodes: 260\n",
      "30972: reward: 200.00, mean_100: 184.21, episodes: 261\n",
      "31147: reward: 175.00, mean_100: 184.93, episodes: 262\n",
      "31347: reward: 200.00, mean_100: 185.39, episodes: 263\n",
      "31547: reward: 200.00, mean_100: 185.39, episodes: 264\n",
      "31731: reward: 184.00, mean_100: 185.23, episodes: 265\n",
      "31931: reward: 200.00, mean_100: 185.95, episodes: 266\n",
      "32042: reward: 111.00, mean_100: 185.34, episodes: 267\n",
      "32211: reward: 169.00, mean_100: 185.56, episodes: 268\n",
      "32411: reward: 200.00, mean_100: 185.79, episodes: 269\n",
      "32537: reward: 126.00, mean_100: 185.05, episodes: 270\n",
      "32737: reward: 200.00, mean_100: 185.39, episodes: 271\n",
      "32937: reward: 200.00, mean_100: 186.14, episodes: 272\n",
      "33137: reward: 200.00, mean_100: 186.14, episodes: 273\n",
      "33337: reward: 200.00, mean_100: 186.37, episodes: 274\n",
      "33476: reward: 139.00, mean_100: 185.78, episodes: 275\n",
      "33676: reward: 200.00, mean_100: 186.22, episodes: 276\n",
      "33876: reward: 200.00, mean_100: 186.22, episodes: 277\n",
      "34076: reward: 200.00, mean_100: 186.71, episodes: 278\n",
      "34276: reward: 200.00, mean_100: 187.33, episodes: 279\n",
      "34401: reward: 125.00, mean_100: 187.01, episodes: 280\n",
      "34601: reward: 200.00, mean_100: 187.01, episodes: 281\n",
      "34801: reward: 200.00, mean_100: 187.01, episodes: 282\n",
      "35001: reward: 200.00, mean_100: 187.43, episodes: 283\n",
      "35201: reward: 200.00, mean_100: 187.95, episodes: 284\n",
      "35401: reward: 200.00, mean_100: 187.95, episodes: 285\n",
      "35601: reward: 200.00, mean_100: 187.98, episodes: 286\n",
      "35781: reward: 180.00, mean_100: 188.15, episodes: 287\n",
      "35981: reward: 200.00, mean_100: 188.75, episodes: 288\n",
      "36181: reward: 200.00, mean_100: 188.87, episodes: 289\n",
      "36381: reward: 200.00, mean_100: 188.87, episodes: 290\n",
      "36562: reward: 181.00, mean_100: 188.80, episodes: 291\n",
      "36666: reward: 104.00, mean_100: 188.54, episodes: 292\n",
      "36802: reward: 136.00, mean_100: 187.90, episodes: 293\n",
      "36924: reward: 122.00, mean_100: 187.19, episodes: 294\n",
      "37124: reward: 200.00, mean_100: 188.05, episodes: 295\n",
      "37324: reward: 200.00, mean_100: 188.16, episodes: 296\n",
      "37524: reward: 200.00, mean_100: 188.53, episodes: 297\n",
      "37724: reward: 200.00, mean_100: 188.53, episodes: 298\n",
      "37871: reward: 147.00, mean_100: 188.76, episodes: 299\n",
      "38071: reward: 200.00, mean_100: 188.76, episodes: 300\n",
      "38271: reward: 200.00, mean_100: 188.92, episodes: 301\n",
      "38471: reward: 200.00, mean_100: 188.92, episodes: 302\n",
      "38671: reward: 200.00, mean_100: 189.27, episodes: 303\n",
      "38871: reward: 200.00, mean_100: 189.69, episodes: 304\n",
      "39052: reward: 181.00, mean_100: 190.02, episodes: 305\n",
      "39252: reward: 200.00, mean_100: 190.48, episodes: 306\n",
      "39452: reward: 200.00, mean_100: 190.63, episodes: 307\n",
      "39652: reward: 200.00, mean_100: 190.63, episodes: 308\n",
      "39852: reward: 200.00, mean_100: 190.63, episodes: 309\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40052: reward: 200.00, mean_100: 190.63, episodes: 310\n",
      "40252: reward: 200.00, mean_100: 190.63, episodes: 311\n",
      "40452: reward: 200.00, mean_100: 191.14, episodes: 312\n",
      "40652: reward: 200.00, mean_100: 191.14, episodes: 313\n",
      "40852: reward: 200.00, mean_100: 191.14, episodes: 314\n",
      "41052: reward: 200.00, mean_100: 191.14, episodes: 315\n",
      "41252: reward: 200.00, mean_100: 191.14, episodes: 316\n",
      "41452: reward: 200.00, mean_100: 191.14, episodes: 317\n",
      "41652: reward: 200.00, mean_100: 191.14, episodes: 318\n",
      "41852: reward: 200.00, mean_100: 191.14, episodes: 319\n",
      "42028: reward: 176.00, mean_100: 190.90, episodes: 320\n",
      "42204: reward: 176.00, mean_100: 190.66, episodes: 321\n",
      "42404: reward: 200.00, mean_100: 190.78, episodes: 322\n",
      "42604: reward: 200.00, mean_100: 190.78, episodes: 323\n",
      "42804: reward: 200.00, mean_100: 190.78, episodes: 324\n",
      "43004: reward: 200.00, mean_100: 190.78, episodes: 325\n",
      "43204: reward: 200.00, mean_100: 190.78, episodes: 326\n",
      "43404: reward: 200.00, mean_100: 190.78, episodes: 327\n",
      "43604: reward: 200.00, mean_100: 190.78, episodes: 328\n",
      "43804: reward: 200.00, mean_100: 190.78, episodes: 329\n",
      "44004: reward: 200.00, mean_100: 190.78, episodes: 330\n",
      "44204: reward: 200.00, mean_100: 190.78, episodes: 331\n",
      "44404: reward: 200.00, mean_100: 190.78, episodes: 332\n",
      "44604: reward: 200.00, mean_100: 190.78, episodes: 333\n",
      "44804: reward: 200.00, mean_100: 190.78, episodes: 334\n",
      "45004: reward: 200.00, mean_100: 190.78, episodes: 335\n",
      "45204: reward: 200.00, mean_100: 190.78, episodes: 336\n",
      "45404: reward: 200.00, mean_100: 191.04, episodes: 337\n",
      "45604: reward: 200.00, mean_100: 191.04, episodes: 338\n",
      "45804: reward: 200.00, mean_100: 191.04, episodes: 339\n",
      "46004: reward: 200.00, mean_100: 191.04, episodes: 340\n",
      "46204: reward: 200.00, mean_100: 191.04, episodes: 341\n",
      "46360: reward: 156.00, mean_100: 190.60, episodes: 342\n",
      "46560: reward: 200.00, mean_100: 190.60, episodes: 343\n",
      "46688: reward: 128.00, mean_100: 190.19, episodes: 344\n",
      "46888: reward: 200.00, mean_100: 190.19, episodes: 345\n",
      "47088: reward: 200.00, mean_100: 190.87, episodes: 346\n",
      "47288: reward: 200.00, mean_100: 190.87, episodes: 347\n",
      "47488: reward: 200.00, mean_100: 190.87, episodes: 348\n",
      "47600: reward: 112.00, mean_100: 190.19, episodes: 349\n",
      "47683: reward:  83.00, mean_100: 189.02, episodes: 350\n",
      "47883: reward: 200.00, mean_100: 189.02, episodes: 351\n",
      "48083: reward: 200.00, mean_100: 189.02, episodes: 352\n",
      "48283: reward: 200.00, mean_100: 189.02, episodes: 353\n",
      "48483: reward: 200.00, mean_100: 189.02, episodes: 354\n",
      "48683: reward: 200.00, mean_100: 189.02, episodes: 355\n",
      "48883: reward: 200.00, mean_100: 189.02, episodes: 356\n",
      "49083: reward: 200.00, mean_100: 189.02, episodes: 357\n",
      "49283: reward: 200.00, mean_100: 189.11, episodes: 358\n",
      "49483: reward: 200.00, mean_100: 189.11, episodes: 359\n",
      "49683: reward: 200.00, mean_100: 189.11, episodes: 360\n",
      "49883: reward: 200.00, mean_100: 189.11, episodes: 361\n",
      "50083: reward: 200.00, mean_100: 189.36, episodes: 362\n",
      "50283: reward: 200.00, mean_100: 189.36, episodes: 363\n",
      "50483: reward: 200.00, mean_100: 189.36, episodes: 364\n",
      "50683: reward: 200.00, mean_100: 189.52, episodes: 365\n",
      "50866: reward: 183.00, mean_100: 189.35, episodes: 366\n",
      "51066: reward: 200.00, mean_100: 190.24, episodes: 367\n",
      "51266: reward: 200.00, mean_100: 190.55, episodes: 368\n",
      "51466: reward: 200.00, mean_100: 190.55, episodes: 369\n",
      "51666: reward: 200.00, mean_100: 191.29, episodes: 370\n",
      "51866: reward: 200.00, mean_100: 191.29, episodes: 371\n",
      "52066: reward: 200.00, mean_100: 191.29, episodes: 372\n",
      "52266: reward: 200.00, mean_100: 191.29, episodes: 373\n",
      "52466: reward: 200.00, mean_100: 191.29, episodes: 374\n",
      "52666: reward: 200.00, mean_100: 191.90, episodes: 375\n",
      "52866: reward: 200.00, mean_100: 191.90, episodes: 376\n",
      "53066: reward: 200.00, mean_100: 191.90, episodes: 377\n",
      "53266: reward: 200.00, mean_100: 191.90, episodes: 378\n",
      "53466: reward: 200.00, mean_100: 191.90, episodes: 379\n",
      "53666: reward: 200.00, mean_100: 192.65, episodes: 380\n",
      "53866: reward: 200.00, mean_100: 192.65, episodes: 381\n",
      "54066: reward: 200.00, mean_100: 192.65, episodes: 382\n",
      "54266: reward: 200.00, mean_100: 192.65, episodes: 383\n",
      "54466: reward: 200.00, mean_100: 192.65, episodes: 384\n",
      "54666: reward: 200.00, mean_100: 192.65, episodes: 385\n",
      "54866: reward: 200.00, mean_100: 192.65, episodes: 386\n",
      "55004: reward: 138.00, mean_100: 192.23, episodes: 387\n",
      "55204: reward: 200.00, mean_100: 192.23, episodes: 388\n",
      "55404: reward: 200.00, mean_100: 192.23, episodes: 389\n",
      "55604: reward: 200.00, mean_100: 192.23, episodes: 390\n",
      "55804: reward: 200.00, mean_100: 192.42, episodes: 391\n",
      "56004: reward: 200.00, mean_100: 193.38, episodes: 392\n",
      "56204: reward: 200.00, mean_100: 194.02, episodes: 393\n",
      "56404: reward: 200.00, mean_100: 194.80, episodes: 394\n",
      "56604: reward: 200.00, mean_100: 194.80, episodes: 395\n",
      "56804: reward: 200.00, mean_100: 194.80, episodes: 396\n",
      "57004: reward: 200.00, mean_100: 194.80, episodes: 397\n",
      "57204: reward: 200.00, mean_100: 194.80, episodes: 398\n",
      "57404: reward: 200.00, mean_100: 195.33, episodes: 399\n",
      "Solved in 57404 steps and 399 episodes!\n"
     ]
    }
   ],
   "source": [
    "for step_idx, exp in enumerate(exp_source):\n",
    "    batch_states.append(exp.state)\n",
    "    batch_actions.append(int(exp.action))\n",
    "    cur_rewards.append(exp.reward)\n",
    "\n",
    "    if exp.last_state is None:\n",
    "        batch_qvals.extend(calc_qvals(cur_rewards))\n",
    "        cur_rewards.clear()\n",
    "        batch_episodes += 1\n",
    "\n",
    "    # handle new rewards\n",
    "    new_rewards = exp_source.pop_total_rewards()\n",
    "    if new_rewards:\n",
    "        done_episodes += 1\n",
    "        reward = new_rewards[0]\n",
    "        total_rewards.append(reward)\n",
    "        mean_rewards = float(np.mean(total_rewards[-100:]))\n",
    "        print(\"%d: reward: %6.2f, mean_100: %6.2f, episodes: %d\" % (\n",
    "            step_idx, reward, mean_rewards, done_episodes))\n",
    "        writer.add_scalar(\"reward\", reward, step_idx)\n",
    "        writer.add_scalar(\"reward_100\", mean_rewards, step_idx)\n",
    "        writer.add_scalar(\"episodes\", done_episodes, step_idx)\n",
    "        if mean_rewards > 195:\n",
    "            print(\"Solved in %d steps and %d episodes!\" % (step_idx, done_episodes))\n",
    "            break\n",
    "\n",
    "    if batch_episodes < EPISODES_TO_TRAIN:\n",
    "        continue\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    states_v = torch.FloatTensor(batch_states)\n",
    "    batch_actions_t = torch.LongTensor(batch_actions)\n",
    "    batch_qvals_v = torch.FloatTensor(batch_qvals)\n",
    "\n",
    "    logits_v = net(states_v)\n",
    "    log_prob_v = F.log_softmax(logits_v, dim=1)\n",
    "    log_prob_actions_v = batch_qvals_v * log_prob_v[range(len(batch_states)), batch_actions_t]\n",
    "    loss_v = -log_prob_actions_v.mean()\n",
    "\n",
    "    loss_v.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    batch_episodes = 0\n",
    "    batch_states.clear()\n",
    "    batch_actions.clear()\n",
    "    batch_qvals.clear()\n",
    "\n",
    "writer.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
