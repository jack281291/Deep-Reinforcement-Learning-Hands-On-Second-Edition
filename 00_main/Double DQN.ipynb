{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Double DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../Chapter08/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import ptan\n",
    "import argparse\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "\n",
    "from ignite.engine import Engine\n",
    "\n",
    "from lib import dqn_model, common\n",
    "\n",
    "NAME = \"03_double\"\n",
    "STATES_TO_EVALUATE = 1000\n",
    "EVAL_EVERY_FRAME = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The core implementation is very simple. What we need to do is slightly modify our\n",
    "loss function. Let's go a step further and compare action values produced by the\n",
    "basic DQN and double DQN. To do this, we store a random held-out set of states\n",
    "and periodically calculate the mean value of the best action for every state in the\n",
    "evaluation set.\n",
    "The complete example is in Chapter08/03_dqn_double.py. Let's first take a look\n",
    "at the loss function:\n",
    "def calc_loss_double_dqn(batch, net, tgt_net, gamma,\n",
    "device=\"cpu\", double=True):\n",
    "states, actions, rewards, dones, next_states = \\\n",
    "common.unpack_batch(batch)\n",
    "The double extra argument turns on and off the double DQN way of calculating\n",
    "actions to take.\n",
    "\n",
    "``` python\n",
    "states_v = torch.tensor(states).to(device)\n",
    "actions_v = torch.tensor(actions).to(device)\n",
    "rewards_v = torch.tensor(rewards).to(device)\n",
    "done_mask = torch.BoolTensor(dones).to(device)\n",
    "```\n",
    "\n",
    "The preceding section is the same as before.\n",
    "\n",
    "``` python\n",
    "actions_v = actions_v.unsqueeze(-1)\n",
    "state_action_vals = net(states_v).gather(1, actions_v)\n",
    "state_action_vals = state_action_vals.squeeze(-1)\n",
    "with torch.no_grad():\n",
    "    next_states_v = torch.tensor(next_states).to(device)\n",
    "    if double:\n",
    "        next_state_acts = net(next_states_v).max(1)[1]\n",
    "        next_state_acts = next_state_acts.unsqueeze(-1)\n",
    "        next_state_vals = tgt_net(next_states_v).gather(1, next_state_acts).squeeze(-1)\n",
    "    else:\n",
    "        next_state_vals = tgt_net(next_states_v).max(1)[0]\n",
    "    next_state_vals[done_mask] = 0.0\n",
    "    exp_sa_vals = next_state_vals.detach()*gamma+rewards_v\n",
    "    return nn.MSELoss()(state_action_vals, exp_sa_vals)\n",
    "```\n",
    "\n",
    "Here is the difference compared to the basic DQN loss function. If double DQN is\n",
    "enabled, we calculate the best action to take in the next state using our main trained\n",
    "network, but values corresponding to this action come from the target network.\n",
    "Of course, this part could be implemented in a faster way, by combining next_\n",
    "states_v with states_v and calling our main network only once, but it will make\n",
    "the code less clear.\n",
    "The rest of the function is the same: we mask completed episodes and compute the\n",
    "mean squared error (MSE) loss between Q-values predicted by the network and\n",
    "approximated Q-values. The last function that we consider calculates the values\n",
    "of our held-out state:\n",
    "\n",
    "``` python\n",
    "def calc_values_of_states(states, net, device=\"cpu\"):\n",
    "    mean_vals = []\n",
    "    for batch in np.array_split(states, 64):\n",
    "        states_v = torch.tensor(batch).to(device)\n",
    "        action_values_v = net(states_v)\n",
    "        best_action_values_v = action_values_v.max(1)[0]\n",
    "        mean_vals.append(best_action_values_v.mean().item())\n",
    "    return np.mean(mean_vals)\n",
    "```\n",
    "\n",
    "There is nothing too complicated here: we just split our held-out states array into\n",
    "equal chunks and pass every chunk to the network to obtain action values. From\n",
    "those values, we choose the action with the largest value (for every state) and\n",
    "calculate the mean of such values. As our array with states is fixed for the whole\n",
    "training process, and this array is large enough (in the code we store 1,000 states),\n",
    "we can compare the dynamics of this mean value in both DQN variants.\n",
    "The rest of the 03_dqn_double.py file is almost the same; the two differences are\n",
    "usage of our tweaked loss function and keep randomly sampled 1,000 states for\n",
    "periodical evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_loss_double_dqn(batch, net, tgt_net, gamma,\n",
    "                         device=\"cpu\", double=True):\n",
    "    states, actions, rewards, dones, next_states = \\\n",
    "        common.unpack_batch(batch)\n",
    "\n",
    "    states_v = torch.tensor(states).to(device)\n",
    "    actions_v = torch.tensor(actions).to(device)\n",
    "    rewards_v = torch.tensor(rewards).to(device)\n",
    "    done_mask = torch.BoolTensor(dones).to(device)\n",
    "\n",
    "    actions_v = actions_v.unsqueeze(-1)\n",
    "    state_action_vals = net(states_v).gather(1, actions_v)\n",
    "    state_action_vals = state_action_vals.squeeze(-1)\n",
    "    with torch.no_grad():\n",
    "        next_states_v = torch.tensor(next_states).to(device)\n",
    "        if double:\n",
    "            next_state_acts = net(next_states_v).max(1)[1]\n",
    "            next_state_acts = next_state_acts.unsqueeze(-1)\n",
    "            next_state_vals = tgt_net(next_states_v).gather(\n",
    "                1, next_state_acts).squeeze(-1)\n",
    "        else:\n",
    "            next_state_vals = tgt_net(next_states_v).max(1)[0]\n",
    "        next_state_vals[done_mask] = 0.0\n",
    "        exp_sa_vals = next_state_vals.detach() * gamma + rewards_v\n",
    "    return nn.MSELoss()(state_action_vals, exp_sa_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1: reward=-20, steps=1063, speed=0.0 f/s, elapsed=0:00:26\n",
      "Episode 2: reward=-21, steps=848, speed=0.0 f/s, elapsed=0:00:26\n",
      "Episode 3: reward=-21, steps=819, speed=0.0 f/s, elapsed=0:00:26\n",
      "Episode 4: reward=-20, steps=928, speed=0.0 f/s, elapsed=0:00:26\n",
      "Episode 5: reward=-21, steps=839, speed=0.0 f/s, elapsed=0:00:26\n",
      "Episode 6: reward=-20, steps=917, speed=0.0 f/s, elapsed=0:00:26\n",
      "Episode 7: reward=-20, steps=1014, speed=0.0 f/s, elapsed=0:00:26\n",
      "Episode 8: reward=-19, steps=939, speed=0.0 f/s, elapsed=0:00:26\n",
      "Episode 9: reward=-20, steps=1016, speed=0.0 f/s, elapsed=0:00:26\n",
      "Episode 10: reward=-19, steps=932, speed=0.0 f/s, elapsed=0:00:26\n",
      "Episode 11: reward=-20, steps=916, speed=53.3 f/s, elapsed=0:00:31\n",
      "Episode 12: reward=-21, steps=812, speed=53.4 f/s, elapsed=0:00:44\n",
      "Episode 13: reward=-20, steps=861, speed=53.5 f/s, elapsed=0:00:59\n",
      "Episode 14: reward=-21, steps=820, speed=53.7 f/s, elapsed=0:01:12\n",
      "Episode 15: reward=-19, steps=1053, speed=53.9 f/s, elapsed=0:01:29\n",
      "Episode 16: reward=-21, steps=880, speed=54.0 f/s, elapsed=0:01:43\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n",
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "random.seed(common.SEED)\n",
    "torch.manual_seed(common.SEED)\n",
    "params = common.HYPERPARAMS['pong']\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "env = gym.make(params.env_name)\n",
    "env = ptan.common.wrappers.wrap_dqn(env)\n",
    "env.seed(common.SEED)\n",
    "\n",
    "net = dqn_model.DQN(env.observation_space.shape, env.action_space.n).to(device)\n",
    "\n",
    "tgt_net = ptan.agent.TargetNet(net)\n",
    "selector = ptan.actions.EpsilonGreedyActionSelector(epsilon=params.epsilon_start)\n",
    "epsilon_tracker = common.EpsilonTracker(selector, params)\n",
    "agent = ptan.agent.DQNAgent(net, selector, device=device)\n",
    "\n",
    "exp_source = ptan.experience.ExperienceSourceFirstLast(\n",
    "    env, agent, gamma=params.gamma)\n",
    "buffer = ptan.experience.ExperienceReplayBuffer(\n",
    "    exp_source, buffer_size=params.replay_size)\n",
    "optimizer = optim.Adam(net.parameters(), lr=params.learning_rate)\n",
    "\n",
    "def process_batch(engine, batch):\n",
    "    optimizer.zero_grad()\n",
    "    loss_v = calc_loss_double_dqn(batch, net, tgt_net.target_model,\n",
    "                                  gamma=params.gamma, device=device,\n",
    "                                  double=True)\n",
    "    loss_v.backward()\n",
    "    optimizer.step()\n",
    "    epsilon_tracker.frame(engine.state.iteration)\n",
    "    if engine.state.iteration % params.target_net_sync == 0:\n",
    "        tgt_net.sync()\n",
    "    if engine.state.iteration % EVAL_EVERY_FRAME == 0:\n",
    "        eval_states = getattr(engine.state, \"eval_states\", None)\n",
    "        if eval_states is None:\n",
    "            eval_states = buffer.sample(STATES_TO_EVALUATE)\n",
    "            eval_states = [np.array(transition.state, copy=False) for transition in eval_states]\n",
    "            eval_states = np.array(eval_states, copy=False)\n",
    "            engine.state.eval_states = eval_states\n",
    "        engine.state.metrics[\"values\"] = \\\n",
    "            common.calc_values_of_states(eval_states, net, device)\n",
    "    return {\n",
    "        \"loss\": loss_v.item(),\n",
    "        \"epsilon\": selector.epsilon,\n",
    "    }\n",
    "\n",
    "engine = Engine(process_batch)\n",
    "common.setup_ignite(engine, params, exp_source, f\"{NAME}={True}\", extra_metrics=('values',))\n",
    "engine.run(common.batch_generator(buffer, params.replay_initial, params.batch_size))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
