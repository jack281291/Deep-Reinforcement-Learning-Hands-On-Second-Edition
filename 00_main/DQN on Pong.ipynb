{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DQN on Pong\n",
    "\n",
    "Before we jump into the code, some introduction is needed. Our examples are\n",
    "becoming increasingly challenging and complex, which is not surprising, as\n",
    "the complexity of the problems that we are trying to tackle is also growing. The\n",
    "examples are as simple and concise as possible, but some of the code may be\n",
    "difficult to understand at first.\n",
    "Another thing to note is performance. Our previous examples for FrozenLake, or\n",
    "CartPole, were not demanding from a performance perspective, as observations\n",
    "were small, NN parameters were tiny, and shaving off extra milliseconds in the\n",
    "training loop wasn't important. However, from now on, that's not the case. One\n",
    "single observation from the Atari environment is 100k values, which have to be\n",
    "rescaled, converted to floats, and stored in the replay buffer. One extra copy of\n",
    "this data array can cost you training speed, which will not be seconds and minutes\n",
    "anymore, but could be hours on even the fastest graphics processing unit (GPU)\n",
    "available.\n",
    "The NN training loop could also be a bottleneck. Of course, RL models are not\n",
    "as huge monsters as state-of-the-art ImageNet models, but even the DQN model\n",
    "from 2015 has more than 1.5M parameters, which is a lot for a GPU to crunch.\n",
    "So, to make a long story short, performance matters, especially when you are\n",
    "experimenting with hyperparameters and need to wait not for a single model to\n",
    "train, but for dozens of them.\n",
    "PyTorch is quite expressive, so more-or-less efficient processing code could look\n",
    "much less cryptic than optimized TensorFlow graphs, but there is still a significant\n",
    "opportunity for doing things slowly and making mistakes. For example, a naïve\n",
    "version of DQN loss computation, which loops over every batch sample, is about two\n",
    "times slower than a parallel version. However, a single extra copy of the data batch\n",
    "could make the speed of the same code 13 times slower, which is quite significant.\n",
    "This example has been split into three modules due to its length, logical structure,\n",
    "and reusability. The modules are as follows:\n",
    "\n",
    "• Chapter06/lib/wrappers.py: These are Atari environment wrappers,\n",
    "mostly taken from the OpenAI Baselines project.\n",
    "\n",
    "• Chapter06/lib/dqn_model.py: This is the DQN NN layer, with the same\n",
    "architecture as the DeepMind DQN from the Nature paper.\n",
    "\n",
    "• Chapter06/02_dqn_pong.py: This is the main module, with the training\n",
    "loop, loss function calculation, and experience replay buffer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wrappers\n",
    "\n",
    "To make things faster, several transformations are applied to the Atari platform\n",
    "interaction, which are described in DeepMind's paper. Some of these transformations\n",
    "influence only performance, but some address Atari platform features that make\n",
    "learning long and unstable. Transformations are usually implemented as OpenAI\n",
    "Gym wrappers of various kinds. The full list is quite lengthy and there are several\n",
    "implementations of the same wrappers in various sources. My personal favorite is\n",
    "in the OpenAI Baselines repository, which is a set of RL methods and algorithms\n",
    "implemented in TensorFlow and applied to popular benchmarks to establish the\n",
    "common ground for comparing methods. The repository is available from https://\n",
    "github.com/openai/baselines, and wrappers are available in this file: https://\n",
    "github.com/openai/baselines/blob/master/baselines/common/atari_\n",
    "wrappers.py.\n",
    "\n",
    "Sometimes, when the DQN is not converging, the\n",
    "problem is not in the code but in the wrongly wrapped environment. I've spent\n",
    "several days debugging convergence issues caused by missing the FIRE button press\n",
    "at the beginning of a game!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import gym\n",
    "import gym.spaces\n",
    "import numpy as np\n",
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FireResetEnv(gym.Wrapper):\n",
    "    def __init__(self, env=None):\n",
    "        \"\"\"For environments where the user need to press FIRE for the game to start.\"\"\"\n",
    "        super(FireResetEnv, self).__init__(env)\n",
    "        assert env.unwrapped.get_action_meanings()[1] == 'FIRE'\n",
    "        assert len(env.unwrapped.get_action_meanings()) >= 3\n",
    "\n",
    "    def step(self, action):\n",
    "        return self.env.step(action)\n",
    "\n",
    "    def reset(self):\n",
    "        self.env.reset()\n",
    "        obs, _, done, _ = self.env.step(1)\n",
    "        if done:\n",
    "            self.env.reset()\n",
    "        obs, _, done, _ = self.env.step(2)\n",
    "        if done:\n",
    "            self.env.reset()\n",
    "        return obs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The preceding wrapper presses the FIRE button in environments that require that\n",
    "for the game to start. In addition to pressing FIRE, this wrapper checks for several\n",
    "corner cases that are present in some games."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaxAndSkipEnv(gym.Wrapper):\n",
    "    def __init__(self, env=None, skip=4):\n",
    "        \"\"\"Return only every `skip`-th frame\"\"\"\n",
    "        super(MaxAndSkipEnv, self).__init__(env)\n",
    "        # most recent raw observations (for max pooling across time steps)\n",
    "        self._obs_buffer = collections.deque(maxlen=2)\n",
    "        self._skip = skip\n",
    "\n",
    "    def step(self, action):\n",
    "        total_reward = 0.0\n",
    "        done = None\n",
    "        for _ in range(self._skip):\n",
    "            obs, reward, done, info = self.env.step(action)\n",
    "            self._obs_buffer.append(obs)\n",
    "            total_reward += reward\n",
    "            if done:\n",
    "                break\n",
    "        max_frame = np.max(np.stack(self._obs_buffer), axis=0)\n",
    "        return max_frame, total_reward, done, info\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Clear past frame buffer and init. to first obs. from inner env.\"\"\"\n",
    "        self._obs_buffer.clear()\n",
    "        obs = self.env.reset()\n",
    "        self._obs_buffer.append(obs)\n",
    "        return obs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This wrapper combines the repetition of actions during K frames and pixels from\n",
    "two consecutive frames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProcessFrame84(gym.ObservationWrapper):\n",
    "    def __init__(self, env=None):\n",
    "        super(ProcessFrame84, self).__init__(env)\n",
    "        self.observation_space = gym.spaces.Box(\n",
    "            low=0, high=255, shape=(84, 84, 1), dtype=np.uint8)\n",
    "\n",
    "    def observation(self, obs):\n",
    "        return ProcessFrame84.process(obs)\n",
    "\n",
    "    @staticmethod\n",
    "    def process(frame):\n",
    "        if frame.size == 210 * 160 * 3:\n",
    "            img = np.reshape(frame, [210, 160, 3]).astype(\n",
    "                np.float32)\n",
    "        elif frame.size == 250 * 160 * 3:\n",
    "            img = np.reshape(frame, [250, 160, 3]).astype(\n",
    "                np.float32)\n",
    "        else:\n",
    "            assert False, \"Unknown resolution.\"\n",
    "        img = img[:, :, 0] * 0.299 + img[:, :, 1] * 0.587 + \\\n",
    "              img[:, :, 2] * 0.114\n",
    "        resized_screen = cv2.resize(\n",
    "            img, (84, 110), interpolation=cv2.INTER_AREA)\n",
    "        x_t = resized_screen[18:102, :]\n",
    "        x_t = np.reshape(x_t, [84, 84, 1])\n",
    "        return x_t.astype(np.uint8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this wrapper is to convert input observations from the emulator, which\n",
    "normally has a resolution of 210×160 pixels with RGB color channels, to a grayscale\n",
    "84×84 image. It does this using a colorimetric grayscale conversion (which is closer\n",
    "to human color perception than a simple averaging of color channels), resizing the\n",
    "image, and cropping the top and bottom parts of the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BufferWrapper(gym.ObservationWrapper):\n",
    "    def __init__(self, env, n_steps, dtype=np.float32):\n",
    "        super(BufferWrapper, self).__init__(env)\n",
    "        self.dtype = dtype\n",
    "        old_space = env.observation_space\n",
    "        self.observation_space = gym.spaces.Box(\n",
    "            old_space.low.repeat(n_steps, axis=0),\n",
    "            old_space.high.repeat(n_steps, axis=0), dtype=dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This class creates a stack of subsequent frames along the first dimension and returns\n",
    "them as an observation. The purpose is to give the network an idea about the\n",
    "dynamics of the objects, such as the speed and direction of the ball in Pong or how\n",
    "enemies are moving. This is very important information, which it is not possible to\n",
    "obtain from a single image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageToPyTorch(gym.ObservationWrapper):\n",
    "    def __init__(self, env):\n",
    "        super(ImageToPyTorch, self).__init__(env)\n",
    "        old_shape = self.observation_space.shape\n",
    "        new_shape = (old_shape[-1], old_shape[0], old_shape[1])\n",
    "        self.observation_space = gym.spaces.Box(\n",
    "            low=0.0, high=1.0, shape=new_shape, dtype=np.float32)\n",
    "\n",
    "    def observation(self, observation):\n",
    "        return np.moveaxis(observation, 2, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**This simple wrapper changes the shape of the observation from HWC (height, width,\n",
    "channel) to the CHW (channel, height, width) format required by PyTorch. The\n",
    "input shape of the tensor has a color channel as the last dimension, but PyTorch's\n",
    "convolution layers assume the color channel to be the first dimension.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScaledFloatFrame(gym.ObservationWrapper):\n",
    "    def observation(self, obs):\n",
    "        return np.array(obs).astype(np.float32) / 255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final wrapper we have in the library converts observation data from bytes to\n",
    "floats, and scales every pixel's value to the range [0.0...1.0]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_env(env_name):\n",
    "    env = gym.make(env_name)\n",
    "    env = MaxAndSkipEnv(env)\n",
    "    env = FireResetEnv(env)\n",
    "    env = ProcessFrame84(env)\n",
    "    env = ImageToPyTorch(env)\n",
    "    env = BufferWrapper(env, 4)\n",
    "    return ScaledFloatFrame(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the end of the file is a simple function that creates an environment by its name\n",
    "and applies all the required wrappers to it. That's it for wrappers, so let's look at our\n",
    "model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DQN Model\n",
    "\n",
    "The model published in Nature has three convolution layers followed by two\n",
    "fully connected layers. All layers are separated by rectified linear unit (ReLU)\n",
    "nonlinearities. The output of the model is Q-values for every action available in\n",
    "the environment, without nonlinearity applied (as Q-values can have any value).\n",
    "The approach of having all Q-values calculated with one pass through the network\n",
    "helps us to increase speed significantly in comparison to treating Q(s, a) literally and\n",
    "feeding observations and actions to the network to obtain the value of the action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, input_shape, n_actions):\n",
    "        super(DQN, self).__init__()\n",
    "\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(input_shape[0], 32, kernel_size=8, stride=4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        conv_out_size = self._get_conv_out(input_shape)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(conv_out_size, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, n_actions)\n",
    "        )\n",
    "\n",
    "    def _get_conv_out(self, shape):\n",
    "        o = self.conv(torch.zeros(1, *shape))\n",
    "        return int(np.prod(o.size()))\n",
    "\n",
    "    def forward(self, x):\n",
    "        conv_out = self.conv(x).view(x.size()[0], -1)\n",
    "        return self.fc(conv_out)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**To be able to write our network in the generic way, it was implemented in two\n",
    "parts: convolution and sequential. PyTorch doesn't have a \"flatter\" layer that could\n",
    "transform a 3D tensor into a 1D vector of numbers, with the requirement of feeding\n",
    "convolution output to the fully connected layer. This problem is solved in the\n",
    "forward() function, where we can reshape our batch of 3D tensors into a batch of\n",
    "1D vectors.**\n",
    "\n",
    "**Another small problem is that we don't know the exact number of values in the\n",
    "output from the convolution layer produced with the input of the given shape.\n",
    "However, we need to pass this number to the first fully connected layer constructor.\n",
    "One possible solution would be to hard-code this number, which is a function of\n",
    "input shape (for 84×84 input, the output from the convolution layer will have 3136\n",
    "values); however, it's not the best way, as our code will become less robust to input\n",
    "shape change. The better solution would be to have a simple function, _get_conv_\n",
    "out(), that accepts the input shape and applies the convolution layer to a fake\n",
    "tensor of such a shape. The result of the function would be equal to the number of\n",
    "parameters returned by this application. It would be fast, as this call would be done\n",
    "once on model creation, but also, it would allow us to have generic code.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The application of transformations is done in two steps: \n",
    "- first we apply the convolution layer to the input, and then we obtain a 4D tensor on output. This result is flattened to have two dimensions: a batch size and all the parameters returned by the convolution for this batch entry as one long vector of numbers. This is done by the view() function of the tensors, which lets one single dimension be a -1 argument as a wildcard for the rest of the parameters. For example, if we have a tensor, T, of shape (2, 3, 4), which is a 3D tensor of 24 elements, we can reshape it into a 2D tensor with six rows and four columns using T.view(6, 4). This operation doesn't create a new memory object or move the data in memory; it just changes the higher-level shape of the tensor. The same result could be obtained by T.view(-1, 4) or T.view(6, -1), which is very convenient when your tensor has a batch size in the first dimension. \n",
    "- Finally, we pass this flattened 2D tensor to our fully connected layers to obtain Q-values for every batch input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "The third module contains the experience replay buffer, the agent, the loss function\n",
    "calculation, and the training loop itself. \n",
    "\n",
    "Before going into the code, something needs\n",
    "to be said about the training hyperparameters. DeepMind's Nature paper contained\n",
    "a table with all the details about hyperparameters used to train its model on all 49\n",
    "Atari games used for evaluation. DeepMind kept all those parameters the same for\n",
    "all games (but trained individual models for every game), and it was the team's\n",
    "intention to show that the method is robust enough to solve lots of games with\n",
    "varying complexity, action space, reward structure, and other details using one\n",
    "single model architecture and hyperparameters. However, our goal here is much\n",
    "more modest: we want to solve just the Pong game.\n",
    "Pong is quite simple and straightforward in comparison to other games in the Atari\n",
    "test set, so the hyperparameters in the paper are overkill for our task. For example,\n",
    "to get the best result on all 49 games, DeepMind used a million-observations replay\n",
    "buffer, which requires approximately 20 GB of RAM to keep and lots of samples\n",
    "from the environment to populate.\n",
    "\n",
    "The epsilon decay schedule that was used is also not the best for a single Pong game.\n",
    "In the training, DeepMind linearly decayed epsilon from 1.0 to 0.1 during the first\n",
    "million frames obtained from the environment. However, my own experiments\n",
    "have shown that for Pong, it's enough to decay epsilon over the first 150k frames\n",
    "and then keep it stable. The replay buffer also can be much smaller: 10k transitions\n",
    "will be enough.\n",
    "\n",
    "In the following example, I've used my parameters. These differ from the parameters\n",
    "in the paper but will allow us to solve Pong about 10 times faster. On a GeForce GTX\n",
    "1080 Ti, the following version converges to a mean score of 19.0 in one to two hours,\n",
    "but with DeepMind's hyperparameters, it will require at least a day.\n",
    "This speedup, of course, is fine-tuning for one particular environment and can break\n",
    "convergence on other games. You are free to play with the options and other games\n",
    "from the Atari set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import time\n",
    "import numpy as np\n",
    "import collections\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "\n",
    "DEFAULT_ENV_NAME = \"PongNoFrameskip-v4\"\n",
    "MEAN_REWARD_BOUND = 19\n",
    "\n",
    "GAMMA = 0.99\n",
    "BATCH_SIZE = 32\n",
    "REPLAY_SIZE = 10000\n",
    "LEARNING_RATE = 1e-4\n",
    "SYNC_TARGET_FRAMES = 1000\n",
    "REPLAY_START_SIZE = 10000\n",
    "\n",
    "EPSILON_DECAY_LAST_FRAME = 150000\n",
    "EPSILON_START = 1.0\n",
    "EPSILON_FINAL = 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These parameters define the following:\n",
    "- Our gamma value used for Bellman approximation (GAMMA)\n",
    "- The batch size sampled from the replay buffer (BATCH_SIZE)\n",
    "- The maximum capacity of the buffer (REPLAY_SIZE)\n",
    "- The count of frames we wait for before starting training to populate the\n",
    "replay buffer (REPLAY_START_SIZE)\n",
    "- The learning rate used in the Adam optimizer, which is used in this example\n",
    "(LEARNING_RATE)\n",
    "- How frequently we sync model weights from the training model to the\n",
    "target model, which is used to get the value of the next state in the Bellman\n",
    "approximation (SYNC_TARGET_FRAMES)\n",
    "\n",
    "The last batch of hyperparameters is related to the epsilon decay schedule. To\n",
    "achieve proper exploration, we start with epsilon = 1.0 at the early stages of training,\n",
    "which causes all actions to be selected randomly. Then, during the first 150,000\n",
    "frames, epsilon is linearly decayed to 0.01, which corresponds to the random action\n",
    "taken in 1% of steps. A similar scheme was used in the original DeepMind paper,\n",
    "but the duration of decay was almost 10 times longer (so, epsilon = 0.01 was reached\n",
    "after a million frames)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next chunk of code defines our experience replay buffer, the purpose of which\n",
    "is to keep the transitions obtained from the environment (tuples of the observation,\n",
    "action, reward, done flag, and the next state). Each time we do a step in the\n",
    "environment, we push the transition into the buffer, keeping only a fixed number of\n",
    "steps (in our case, 10k transitions). For training, we randomly sample the batch of\n",
    "transitions from the replay buffer, which allows us to break the correlation between\n",
    "subsequent steps in the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "Experience = collections.namedtuple(\n",
    "    'Experience', field_names=['state', 'action', 'reward',\n",
    "                               'done', 'new_state'])\n",
    "\n",
    "class ExperienceBuffer:\n",
    "    def __init__(self, capacity):\n",
    "        self.buffer = collections.deque(maxlen=capacity)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "    def append(self, experience):\n",
    "        self.buffer.append(experience)\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        indices = np.random.choice(len(self.buffer), batch_size,\n",
    "                                   replace=False)\n",
    "        states, actions, rewards, dones, next_states = \\\n",
    "            zip(*[self.buffer[idx] for idx in indices])\n",
    "        return np.array(states), np.array(actions), \\\n",
    "               np.array(rewards, dtype=np.float32), \\\n",
    "               np.array(dones, dtype=np.uint8), \\\n",
    "               np.array(next_states)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of the experience replay buffer code is quite straightforward: it basically\n",
    "exploits the capability of the deque class to maintain the given number of entries\n",
    "in the buffer. In the sample() method, we create a list of random indices and then\n",
    "repack the sampled entries into NumPy arrays for more convenient loss calculation.\n",
    "\n",
    "The next class we need to have is an Agent, which interacts with the environment\n",
    "and saves the result of the interaction into the experience replay buffer that you\n",
    "have just seen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, env, exp_buffer):\n",
    "        self.env = env\n",
    "        self.exp_buffer = exp_buffer\n",
    "        self._reset()\n",
    "\n",
    "    def _reset(self):\n",
    "        self.state = env.reset()\n",
    "        self.total_reward = 0.0\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def play_step(self, net, epsilon=0.0, device=\"cpu\"):\n",
    "        done_reward = None\n",
    "\n",
    "        if np.random.random() < epsilon:\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            state_a = np.array([self.state], copy=False)\n",
    "            state_v = torch.tensor(state_a).to(device)\n",
    "            q_vals_v = net(state_v)\n",
    "            _, act_v = torch.max(q_vals_v, dim=1)\n",
    "            action = int(act_v.item())\n",
    "\n",
    "        # do step in the environment\n",
    "        new_state, reward, is_done, _ = self.env.step(action)\n",
    "        self.total_reward += reward\n",
    "\n",
    "        exp = Experience(self.state, action, reward,\n",
    "                         is_done, new_state)\n",
    "        self.exp_buffer.append(exp)\n",
    "        self.state = new_state\n",
    "        if is_done:\n",
    "            done_reward = self.total_reward\n",
    "            self._reset()\n",
    "        return done_reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During the agent's initialization, we need to store references to the environment\n",
    "and experience replay buffer, tracking the current observation and the total reward\n",
    "accumulated so far (_reset module).\n",
    "\n",
    "The main method of the agent is to perform a step in the environment and store its\n",
    "result in the buffer. To do this, we need to select the action first. With the probability\n",
    "epsilon (passed as an argument), we take the random action; otherwise, we use the\n",
    "past model to obtain the Q-values for all possible actions and choose the best.\n",
    "\n",
    "As the action has been chosen, we pass it to the environment to get the next\n",
    "observation and reward, store the data in the experience buffer, and then handle\n",
    "the end-of-episode situation. The result of the function is the total accumulated\n",
    "reward if we have reached the end of the episode with this step, or None if not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it is time for the last function in the training module, which calculates the loss\n",
    "for the sampled batch. This function is written in a form to maximally exploit GPU\n",
    "parallelism by processing all batch samples with vector operations, which makes\n",
    "it harder to understand when compared with a naïve loop over the batch. Yet this\n",
    "optimization pays off: the parallel version is more than two times faster than an\n",
    "explicit loop over the batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_loss(batch, net, tgt_net, device=\"cpu\"):\n",
    "    states, actions, rewards, dones, next_states = batch\n",
    "\n",
    "    states_v = torch.tensor(np.array(\n",
    "        states, copy=False)).to(device)\n",
    "    next_states_v = torch.tensor(np.array(\n",
    "        next_states, copy=False)).to(device)\n",
    "    actions_v = torch.tensor(actions).to(device)\n",
    "    rewards_v = torch.tensor(rewards).to(device)\n",
    "    done_mask = torch.BoolTensor(dones).to(device)\n",
    "\n",
    "    state_action_values = net(states_v).gather(\n",
    "        1, actions_v.unsqueeze(-1)).squeeze(-1)\n",
    "    with torch.no_grad():\n",
    "        next_state_values = tgt_net(next_states_v).max(1)[0]\n",
    "        next_state_values[done_mask] = 0.0\n",
    "        next_state_values = next_state_values.detach()\n",
    "\n",
    "    expected_state_action_values = next_state_values * GAMMA + \\\n",
    "                                   rewards_v\n",
    "    return nn.MSELoss()(state_action_values,\n",
    "                        expected_state_action_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**state_action_values = net(states_v).gather(1, actions_v.unsqueeze(-1)).squeeze(-1)**\n",
    "\n",
    "In the preceding line, we pass observations to the first model and extract the\n",
    "specific Q-values for the taken actions using the gather() tensor operation. The\n",
    "first argument to the gather() call is a dimension index that we want to perform\n",
    "gathering on (in our case, it is equal to 1, which corresponds to actions). \n",
    "\n",
    "The second argument is a tensor of indices of elements to be chosen. Extra\n",
    "unsqueeze() and squeeze() calls are required to compute the index argument\n",
    "for the gather functions, and to get rid of the extra dimensions that we created,\n",
    "respectively. (The index should have the same number of dimensions as the data we\n",
    "are processing.)\n",
    "\n",
    "Keep in mind that the result of gather() applied to tensors is a differentiable\n",
    "operation that will keep all gradients with respect to the final loss value.\n",
    "\n",
    "**next_state_values = tgt_net(next_states_v).max(1)**\n",
    "\n",
    "In the preceding line, we apply the target network to our next state observations\n",
    "and calculate the maximum Q-value along the same action dimension, 1. Function\n",
    "max() returns both maximum values and indices of those values (so it calculates\n",
    "both max and argmax), which is very convenient. However, in this case, we are\n",
    "interested only in values, so we take the first entry of the result.\n",
    "\n",
    "**next_state_values[done_mask] = 0.0**\n",
    "\n",
    "Here we make one simple, but very important, point: if transition in the batch\n",
    "is from the last step in the episode, then our value of the action doesn't have a\n",
    "discounted reward of the next state, as there is no next state from which to gather\n",
    "the reward. This may look minor, but it is very important in practice: without this,\n",
    "training will not converge.\n",
    "\n",
    "**next_state_values = next_state_values.detach()**\n",
    "\n",
    "In this line, we detach the value from its computation graph to prevent gradients\n",
    "from flowing into the NN used to calculate Q approximation for the next states. This is important, as without this, our backpropagation of the loss will start to affect\n",
    "both predictions for the current state and the next state. However, we don't want\n",
    "to touch predictions for the next state, as they are used in the Bellman equation\n",
    "to calculate reference Q-values. To block gradients from flowing into this branch\n",
    "of the graph, we are using the detach() method of the tensor, which returns the\n",
    "tensor without connection to its calculation history.\n",
    "\n",
    "**expected_state_action_values = next_state_values * GAMMA + rewards_v\n",
    "return nn.MSELoss()(state_action_values, expected_state_action_values)**\n",
    "\n",
    "Finally, we calculate the Bellman approximation value and the mean squared error\n",
    "loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In arguments, we pass our batch as a tuple of arrays (repacked by the sample()\n",
    "method in the experience buffer), our network that we are training, and the target\n",
    "network, which is periodically synced with the trained one.\n",
    "The first model (passed as the argument network) is used to calculate gradients;\n",
    "the second model in the tgt_net argument is used to calculate values for the next\n",
    "states, and this calculation shouldn't affect gradients. To achieve this, we use the\n",
    "detach() function of the PyTorch tensor to prevent gradients from flowing into the\n",
    "target network's graph.\n",
    "\n",
    "We wrap individual NumPy\n",
    "arrays with batch data in PyTorch tensors and copy them to GPU if the CUDA device\n",
    "was specified in arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'PongNoFrameskip-v4'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DEFAULT_ENV_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DQN(\n",
      "  (conv): Sequential(\n",
      "    (0): Conv2d(4, 32, kernel_size=(8, 8), stride=(4, 4))\n",
      "    (1): ReLU()\n",
      "    (2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2))\n",
      "    (3): ReLU()\n",
      "    (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (5): ReLU()\n",
      "  )\n",
      "  (fc): Sequential(\n",
      "    (0): Linear(in_features=3136, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=6, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "ename": "NotImplementedError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-5659fc47f166>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mbuffer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mExperienceBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mREPLAY_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0magent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAgent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0mepsilon\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEPSILON_START\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-12-bd19259b1ac7>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, env, exp_buffer)\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp_buffer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexp_buffer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-12-bd19259b1ac7>\u001b[0m in \u001b[0;36m_reset\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtotal_reward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/envs/reinforcement_learning/lib/python3.6/site-packages/gym/core.py\u001b[0m in \u001b[0;36mreset\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    255\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mObservationWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mWrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 257\u001b[0;31m         \u001b[0mobservation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    258\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/envs/reinforcement_learning/lib/python3.6/site-packages/gym/core.py\u001b[0m in \u001b[0;36mreset\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    256\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m         \u001b[0mobservation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 258\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    259\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/envs/reinforcement_learning/lib/python3.6/site-packages/gym/core.py\u001b[0m in \u001b[0;36mobservation\u001b[0;34m(self, observation)\u001b[0m\n\u001b[1;32m    263\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mobservation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobservation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 265\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    266\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\")\n",
    "\n",
    "env = make_env(DEFAULT_ENV_NAME)\n",
    "\n",
    "net = DQN(env.observation_space.shape,\n",
    "                    env.action_space.n).to(device)\n",
    "tgt_net = DQN(env.observation_space.shape,\n",
    "                        env.action_space.n).to(device)\n",
    "writer = SummaryWriter(comment=\"-\" + DEFAULT_ENV_NAME)\n",
    "print(net)\n",
    "\n",
    "buffer = ExperienceBuffer(REPLAY_SIZE)\n",
    "agent = Agent(env, buffer)\n",
    "epsilon = EPSILON_START\n",
    "\n",
    "optimizer = optim.Adam(net.parameters(), lr=LEARNING_RATE)\n",
    "total_rewards = []\n",
    "frame_idx = 0\n",
    "ts_frame = 0\n",
    "ts = time.time()\n",
    "best_m_reward = None\n",
    "\n",
    "while True:\n",
    "    frame_idx += 1\n",
    "    epsilon = max(EPSILON_FINAL, EPSILON_START -\n",
    "                  frame_idx / EPSILON_DECAY_LAST_FRAME)\n",
    "\n",
    "    reward = agent.play_step(net, epsilon, device=device)\n",
    "    if reward is not None:\n",
    "        total_rewards.append(reward)\n",
    "        speed = (frame_idx - ts_frame) / (time.time() - ts)\n",
    "        ts_frame = frame_idx\n",
    "        ts = time.time()\n",
    "        m_reward = np.mean(total_rewards[-100:])\n",
    "        print(\"%d: done %d games, reward %.3f, \"\n",
    "              \"eps %.2f, speed %.2f f/s\" % (\n",
    "            frame_idx, len(total_rewards), m_reward, epsilon,\n",
    "            speed\n",
    "        ))\n",
    "        writer.add_scalar(\"epsilon\", epsilon, frame_idx)\n",
    "        writer.add_scalar(\"speed\", speed, frame_idx)\n",
    "        writer.add_scalar(\"reward_100\", m_reward, frame_idx)\n",
    "        writer.add_scalar(\"reward\", reward, frame_idx)\n",
    "        if best_m_reward is None or best_m_reward < m_reward:\n",
    "            torch.save(net.state_dict(), args.env +\n",
    "                       \"-best_%.0f.dat\" % m_reward)\n",
    "            if best_m_reward is not None:\n",
    "                print(\"Best reward updated %.3f -> %.3f\" % (\n",
    "                    best_m_reward, m_reward))\n",
    "            best_m_reward = m_reward\n",
    "        if m_reward > MEAN_REWARD_BOUND:\n",
    "            print(\"Solved in %d frames!\" % frame_idx)\n",
    "            break\n",
    "\n",
    "    if len(buffer) < REPLAY_START_SIZE:\n",
    "        continue\n",
    "\n",
    "    if frame_idx % SYNC_TARGET_FRAMES == 0:\n",
    "        tgt_net.load_state_dict(net.state_dict())\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    batch = buffer.sample(BATCH_SIZE)\n",
    "    loss_t = calc_loss(batch, net, tgt_net, device=device)\n",
    "    loss_t.backward()\n",
    "    optimizer.step()\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
