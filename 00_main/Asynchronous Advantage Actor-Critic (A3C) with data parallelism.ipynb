{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Asynchronous Advantage Actor-Critic (A3C) with data parallelism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../Chapter13/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import os\n",
    "import ptan\n",
    "import numpy as np\n",
    "import argparse\n",
    "import collections\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "import torch\n",
    "import torch.nn.utils as nn_utils\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.multiprocessing as mp\n",
    "\n",
    "from lib import common"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In hyperparameters, we have three new values:\n",
    "- PROCESSES_COUNT specifies the number of child processes that will gather\n",
    "training data for us. This activity is mostly CPU-bound, as the heaviest\n",
    "operation here is the preprocessing of Atari frames, so this value is set\n",
    "equal to the number of CPU cores on my machine.\n",
    "- MICRO_BATCH_SIZE sets the number of training samples that every child\n",
    "process needs to obtain before transferring those samples to the main\n",
    "process.\n",
    "- NUM_ENVS is the number of environments every child process will use to\n",
    "gather data. This number multiplied by the number of processes is the total\n",
    "amount of parallel environments that we will get our training data from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "GAMMA = 0.99\n",
    "LEARNING_RATE = 0.001\n",
    "ENTROPY_BETA = 0.01\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "REWARD_STEPS = 4\n",
    "CLIP_GRAD = 0.1\n",
    "\n",
    "PROCESSES_COUNT = 4\n",
    "NUM_ENVS = 8\n",
    "MICRO_BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if True:\n",
    "    ENV_NAME = \"PongNoFrameskip-v4\"\n",
    "    REWARD_BOUND = 18\n",
    "else:\n",
    "    ENV_NAME = \"BreakoutNoFrameskip-v4\"\n",
    "    REWARD_BOUND = 400"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we get to the child process function, we need the environment construction\n",
    "function and a tiny wrapper that we will use to send the total episode reward into\n",
    "the main training process.\n",
    "\n",
    "The data_func function is very simple, but it is special, as it will be executed in the\n",
    "child process. (We will use the mp.Process class to launch those processes in the\n",
    "main code block.) \n",
    "\n",
    "We pass it three arguments: our NN, the device to be used to\n",
    "perform computation (cpu or cuda string), and the queue we will use to send data\n",
    "from the child process to our master process, which will perform training. The\n",
    "queue is used in the many-producers and one-consumer mode, and can contain\n",
    "two different types of objects:\n",
    "- TotalReward: This is a preceding object that we've defined, which has only\n",
    "one field reward, which is a float value of the total undiscounted reward for\n",
    "the completed episode.\n",
    "- A tuple with tensors returned by the function common.unpack_batch().\n",
    "\n",
    "Due to torch.multiprocessing magic, those tensors will be transferred\n",
    "to the main process without copying physical memory, which might be a\n",
    "costly operation (as an Atari observation is large).\n",
    "As we get the required number of experience samples for our microbatch, we\n",
    "convert them into training data using the unpack_batch function and clear the\n",
    "batch. One thing to note is that as our experience samples represent four-step\n",
    "subsequences (as REWARD_STEPS is 4), we need to use a proper discount factor of\n",
    "ùõæ^4 for the last V(s) reward term."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_env():\n",
    "    return ptan.common.wrappers.wrap_dqn(gym.make(ENV_NAME))\n",
    "\n",
    "\n",
    "TotalReward = collections.namedtuple('TotalReward', field_names='reward')\n",
    "\n",
    "\n",
    "def data_func(net, device, train_queue):\n",
    "    envs = [make_env() for _ in range(NUM_ENVS)]\n",
    "    agent = ptan.agent.PolicyAgent(\n",
    "        lambda x: net(x)[0], device=device, apply_softmax=True)\n",
    "    exp_source = ptan.experience.ExperienceSourceFirstLast(\n",
    "        envs, agent, gamma=GAMMA, steps_count=REWARD_STEPS)\n",
    "    micro_batch = []\n",
    "\n",
    "    for exp in exp_source:\n",
    "        new_rewards = exp_source.pop_total_rewards()\n",
    "        if new_rewards:\n",
    "            data = TotalReward(reward=np.mean(new_rewards))\n",
    "            train_queue.put(data)\n",
    "\n",
    "        micro_batch.append(exp)\n",
    "        if len(micro_batch) < MICRO_BATCH_SIZE:\n",
    "            continue\n",
    "\n",
    "        data = common.unpack_batch(\n",
    "            micro_batch, net, device=device,\n",
    "            last_val_gamma=GAMMA ** REWARD_STEPS)\n",
    "        train_queue.put(data)\n",
    "        micro_batch.clear()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the beginning, we take familiar steps, except for a single call to the mp.set_start_method, which instructs the multiprocessing module about the kind of\n",
    "parallelism we want to use. The native multiprocessing library in Python supports\n",
    "several ways to start subprocesses, but due to PyTorch multiprocessing limitations,\n",
    "spawn is the only option if you want to use GPU.\n",
    "\n",
    "Another new line is assignment to the OMP_NUM_THREADS, which is an environment\n",
    "variable instructing the OpenMP library about the number of threads it can start.\n",
    "\n",
    "OpenMP (https://www.openmp.org/) is heavily used by the Gym and OpenCV\n",
    "libraries to provide a speed-up on multicore systems, which is a good thing most\n",
    "of the time. By default, the process that uses OpenMP starts a thread for every core\n",
    "in the system. But in our case, the effect from OpenMP is the opposite: as we're\n",
    "implementing our own parallelism, by launching several processes, extra threads\n",
    "overload the cores with frequent context switches, which negatively impacts\n",
    "performance. \n",
    "\n",
    "To avoid this, we explicitly set the maximum number of threads\n",
    "OpenMP can start with a single thread. If you want, you can experiment yourself\n",
    "with this parameter. On my system, I experienced a 3-4x performance drop without\n",
    "this code line.\n",
    "\n",
    "After that, we create our NN, move it to the CUDA device, and ask it to share its\n",
    "weights. CUDA tensors are shared by default, but for CPU mode, a call to share_memory() is required for multiprocessing to work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AtariA2C(\n",
       "  (conv): Sequential(\n",
       "    (0): Conv2d(4, 32, kernel_size=(8, 8), stride=(4, 4))\n",
       "    (1): ReLU()\n",
       "    (2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2))\n",
       "    (3): ReLU()\n",
       "    (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (5): ReLU()\n",
       "  )\n",
       "  (policy): Sequential(\n",
       "    (0): Linear(in_features=3136, out_features=512, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=512, out_features=6, bias=True)\n",
       "  )\n",
       "  (value): Sequential(\n",
       "    (0): Linear(in_features=3136, out_features=512, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=512, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mp.set_start_method('spawn')\n",
    "os.environ['OMP_NUM_THREADS'] = \"1\"\n",
    "device = \"cuda\"\n",
    "\n",
    "writer = SummaryWriter(comment=f\"-a3c-data_pong_test_a3c\")\n",
    "\n",
    "env = make_env()\n",
    "net = common.AtariA2C(env.observation_space.shape,\n",
    "                      env.action_space.n).to(device)\n",
    "net.share_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can have only two types of objects in the queue (TotalReward and a tuple\n",
    "with a microbatch), we need to check an entry obtained from the queue only once.\n",
    "After the TotalReward entries are handled, we process the tuple with tensors. We\n",
    "accumulate them in lists, and once the required batch size has been reached, we\n",
    "concatenate the tensors using a torch.cat() call, which appends tensors along the\n",
    "first dimension.\n",
    "The rest of the training loop is standard actor-critic loss calculation, which is\n",
    "performed in exactly the same way as in the previous chapter: we calculate the logits\n",
    "of the policy and value estimation using our current network, and calculate the\n",
    "policy, value, and entropy losses.\n",
    "\n",
    "As the last step, we pass the calculated tensors to the TensorBoard tracker class,\n",
    "which will perform the averaging and store the data that we want to monitor.\n",
    "\n",
    "In the last finally block, which can be executed due to an exception (Ctrl + C, for\n",
    "example) or the game solved condition, we terminate the child processes and wait for\n",
    "them. This is required to make sure that there are no leftover processes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-cde7142aa0a5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m                 writer, 100) as tb_tracker:\n\u001b[1;32m     22\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m                 \u001b[0mtrain_entry\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_entry\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTotalReward\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m                     if tracker.reward(train_entry.reward,\n",
      "\u001b[0;32m/usr/lib/python3.6/multiprocessing/queues.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m     92\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mblock\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_rlock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m                 \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_recv_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/multiprocessing/connection.py\u001b[0m in \u001b[0;36mrecv_bytes\u001b[0;34m(self, maxlength)\u001b[0m\n\u001b[1;32m    214\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmaxlength\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mmaxlength\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"negative maxlength\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m         \u001b[0mbuf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_recv_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaxlength\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbuf\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_bad_message_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/multiprocessing/connection.py\u001b[0m in \u001b[0;36m_recv_bytes\u001b[0;34m(self, maxsize)\u001b[0m\n\u001b[1;32m    405\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    406\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_recv_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 407\u001b[0;31m         \u001b[0mbuf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_recv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    408\u001b[0m         \u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstruct\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munpack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"!i\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    409\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmaxsize\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0msize\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mmaxsize\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/multiprocessing/connection.py\u001b[0m in \u001b[0;36m_recv\u001b[0;34m(self, size, read)\u001b[0m\n\u001b[1;32m    377\u001b[0m         \u001b[0mremaining\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    378\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0mremaining\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 379\u001b[0;31m             \u001b[0mchunk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mremaining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    380\u001b[0m             \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "optimizer = optim.Adam(net.parameters(), lr=LEARNING_RATE,\n",
    "                       eps=1e-3)\n",
    "\n",
    "train_queue = mp.Queue(maxsize=PROCESSES_COUNT)\n",
    "data_proc_list = []\n",
    "for _ in range(PROCESSES_COUNT):\n",
    "    data_proc = mp.Process(target=data_func,\n",
    "                           args=(net, device, train_queue))\n",
    "    data_proc.start()\n",
    "    data_proc_list.append(data_proc)\n",
    "\n",
    "batch_states = []\n",
    "batch_actions = []\n",
    "batch_vals_ref = []\n",
    "step_idx = 0\n",
    "batch_size = 0\n",
    "\n",
    "try:\n",
    "    with common.RewardTracker(writer, REWARD_BOUND) as tracker:\n",
    "        with ptan.common.utils.TBMeanTracker(\n",
    "                writer, 100) as tb_tracker:\n",
    "            while True:\n",
    "                train_entry = train_queue.get()\n",
    "                if isinstance(train_entry, TotalReward):\n",
    "                    if tracker.reward(train_entry.reward,\n",
    "                                      step_idx):\n",
    "                        break\n",
    "                    continue\n",
    "\n",
    "                states_t, actions_t, vals_ref_t = train_entry\n",
    "                batch_states.append(states_t)\n",
    "                batch_actions.append(actions_t)\n",
    "                batch_vals_ref.append(vals_ref_t)\n",
    "                step_idx += states_t.size()[0]\n",
    "                batch_size += states_t.size()[0]\n",
    "                if batch_size < BATCH_SIZE:\n",
    "                    continue\n",
    "\n",
    "                states_v = torch.cat(batch_states)\n",
    "                actions_t = torch.cat(batch_actions)\n",
    "                vals_ref_v = torch.cat(batch_vals_ref)\n",
    "                batch_states.clear()\n",
    "                batch_actions.clear()\n",
    "                batch_vals_ref.clear()\n",
    "                batch_size = 0\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                logits_v, value_v = net(states_v)\n",
    "\n",
    "                loss_value_v = F.mse_loss(\n",
    "                    value_v.squeeze(-1), vals_ref_v)\n",
    "\n",
    "                log_prob_v = F.log_softmax(logits_v, dim=1)\n",
    "                adv_v = vals_ref_v - value_v.detach()\n",
    "                size = states_v.size()[0]\n",
    "                log_p_a = log_prob_v[range(size), actions_t]\n",
    "                log_prob_actions_v = adv_v * log_p_a\n",
    "                loss_policy_v = -log_prob_actions_v.mean()\n",
    "\n",
    "                prob_v = F.softmax(logits_v, dim=1)\n",
    "                ent = (prob_v * log_prob_v).sum(dim=1).mean()\n",
    "                entropy_loss_v = ENTROPY_BETA * ent\n",
    "\n",
    "                loss_v = entropy_loss_v + loss_value_v + \\\n",
    "                         loss_policy_v\n",
    "                loss_v.backward()\n",
    "                nn_utils.clip_grad_norm_(\n",
    "                    net.parameters(), CLIP_GRAD)\n",
    "                optimizer.step()\n",
    "\n",
    "                tb_tracker.track(\"advantage\", adv_v, step_idx)\n",
    "                tb_tracker.track(\"values\", value_v, step_idx)\n",
    "                tb_tracker.track(\"batch_rewards\", vals_ref_v,\n",
    "                                 step_idx)\n",
    "                tb_tracker.track(\"loss_entropy\",\n",
    "                                 entropy_loss_v, step_idx)\n",
    "                tb_tracker.track(\"loss_policy\",\n",
    "                                 loss_policy_v, step_idx)\n",
    "                tb_tracker.track(\"loss_value\",\n",
    "                                 loss_value_v, step_idx)\n",
    "                tb_tracker.track(\"loss_total\",\n",
    "                                 loss_v, step_idx)\n",
    "finally:\n",
    "    for p in data_proc_list:\n",
    "        p.terminate()\n",
    "        p.join()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
